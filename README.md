# LLM-in_Vision
Recent LLM (Large Language Models)-based CV and multi-modal works. Welcome to comment/contribute!

### 2023.3

<!-- - (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.3) Can an **Embodied** Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation, [[Paper]](https://arxiv.org/pdf/2303.03480.pdf), [[Project]](https://gamma.umd.edu/LGX/)

- (arXiv 2023.3) Visual **ChatGPT**: Talking, Drawing and Editing with Visual Foundation Models, [[Paper]](https://arxiv.org/abs/2303.04671), [[Code]](https://github.com/microsoft/visual-chatgpt)

- (arXiv 2023.3) PaLM-E: An **Embodied** Multimodal Language Model, [[Paper]](https://palm-e.github.io/assets/palm-e.pdf), [[Project]](https://palm-e.github.io/)

- (arXiv 2023.3) Language Is Not All You Need: **Aligning** Perception with Language Models, [[Paper]](https://arxiv.org/pdf/2302.14045.pdf), [[Code]](https://github.com/microsoft/unilm)

### 2022.7

- (arXiv 2022.7) Language Models are General-Purpose **Interfaces**, [[Paper]](https://arxiv.org/pdf/2206.06336.pdf), [[Code]](https://github.com/microsoft/unilm)
