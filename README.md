# LLM-in-Vision
Recent LLM (Large Language Models)-based CV and multi-modal works. Welcome to comment/contribute!

### 2023.7

<!-- - (arXiv 2023.7) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.7) TOWARDS A **UNIFIED AGENT** WITH FOUNDATION MODELS, [[Paper]](https://arxiv.org/pdf/2307.09668.pdf)

- (arXiv 2023.7) Robots That Ask For Help: **Uncertainty** Alignment for Large Language Model **Planners**, [[Paper]](https://arxiv.org/pdf/2307.01928.pdf), [[Project]](https://robot-help.github.io/)

- (arXiv 2023.7) Building Cooperative **Embodied Agents** Modularly with Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.02485.pdf), [[Project]](https://vis-www.cs.umass.edu/Co-LLM-Agents/)

- (arXiv 2023.7) **Embodied Task Planning** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.01848.pdf), [[Project]](https://gary3410.github.io/TaPA/)

- (arXiv 2023.7) Motion-X: A Large-scale 3D Expressive Whole-body Human **Motion Dataset**, [[Paper]](https://arxiv.org/pdf/2307.00818.pdf), [[Code]](https://github.com/IDEA-Research/Motion-X)

### 2023.6

<!-- - (arXiv 2023.6) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.6) OpenShape: Scaling Up **3D Shape Representation** Towards **Open-World** Understanding, [[Paper]](https://arxiv.org/pdf/2305.10764.pdf), [[Project]](https://colin97.github.io/OpenShape/)

- (arXiv 2023.6) Statler: State-Maintaining Language Models for **Embodied Reasoning**, [[Paper]](https://arxiv.org/pdf/2306.17840.pdf), [[Project]](https://statler-lm.github.io/)

- (arXiv 2023.6) CLARA: Classifying and Disambiguating User Commands for Reliable **Interactive Robotic** Agents, [[Paper]](https://arxiv.org/pdf/2306.10376.pdf)

- (arXiv 2023.6) **Mass-Producing Failures** of **Multimodal** Systems with Language Models, [[Paper]](https://arxiv.org/pdf/2306.12105.pdf), [[Code]](https://github.com/tsb0601/MultiMon)

- (arXiv 2023.6) SoftGPT: Learn Goal-oriented **Soft Object Manipulation** Skills by Generative Pre-trained Heterogeneous Graph Transformer, [[Paper]](https://arxiv.org/pdf/2306.12677.pdf)

- (arXiv 2023.6) SPRINT: SCALABLE **POLICY PRE-TRAINING** VIA LANGUAGE INSTRUCTION RELABELING, [[Paper]](https://arxiv.org/pdf/2306.11886.pdf), [[Project]](https://clvrai.github.io/sprint/)

- (arXiv 2023.6) MotionGPT: Finetuned LLMs are General-Purpose **Motion Generators**, [[Paper]](https://arxiv.org/pdf/2306.10900.pdf), [[Project]](https://qiqiapink.github.io/MotionGPT/)

- (arXiv 2023.6) MIMIC-IT: **Multi-Modal** **In-Context** Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2306.05425.pdf), [[Code]](https://github.com/Luodian/Otter)

- (arXiv 2023.6) Dense and Aligned **Captions** (DAC) Promote **Compositional Reasoning** in VL Models, [[Paper]](https://arxiv.org/pdf/2305.19595.pdf)

### 2023.5

<!-- - (arXiv 2023.5) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.5) VIMA: General **Robot Manipulation** with Multimodal Prompts, [[Paper]](https://arxiv.org/pdf/2210.03094.pdf), [[Project]](https://vimalabs.github.io/)

- (arXiv 2023.5) TidyBot: Personalized **Robot Assistance** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2305.05658.pdf), [[Project]](https://tidybot.cs.princeton.edu/)

- (arXiv 2023.5) Training **Diffusion** Models with **Reinforcement Learning**, [[Paper]](https://arxiv.org/pdf/2305.13301.pdf), [[Project]](https://rl-diffusion.github.io/)

- (arXiv 2023.5) EmbodiedGPT: Vision-Language Pre-Training via **Embodied** Chain of Thought, [[Paper]](https://arxiv.org/pdf/2305.15021.pdf), [[Project]](https://embodiedgpt.github.io/)

- (arXiv 2023.5) ArtGPT-4: **Artistic Vision-Language** Understanding with Adapter-enhanced MiniGPT-4, [[Paper]](https://arxiv.org/pdf/2305.07490.pdf), [[Code]](https://huggingface.co/Tyrannosaurus/ArtGPT-4)

- (arXiv 2023.5) Evaluating **Object Hallucination** in Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2305.10355.pdf), [[Code]](https://github.com/RUCAIBox/POPE)

- (arXiv 2023.5) LLMScore: Unveiling the Power of Large Language Models in **Text-to-Image Synthesis Evaluation**, [[Paper]](https://arxiv.org/pdf/2305.11116.pdf), [[Code]](https://github.com/YujieLu10/LLMScore)

- (arXiv 2023.5) VisionLLM: Large Language Model is also an Open-Ended **Decoder** for **Vision**-Centric Tasks, [[Paper]](https://arxiv.org/pdf/2305.11175.pdf), [[Code]](https://github.com/OpenGVLab/VisionLLM)

- (arXiv 2023.5) OpenShape: Scaling Up **3D Shape Representation** Towards **Open-World** Understanding, [[Paper]](https://arxiv.org/pdf/2305.10764.pdf), [[Project]](https://colin97.github.io/OpenShape/)

- (arXiv 2023.5) Towards A Foundation Model for Generalist **Robots**: Diverse **Skill Learning** at Scale via Automated Task and Scene Generation, [[Paper]](https://arxiv.org/pdf/2305.10455.pdf)

- (arXiv 2023.5) An Android **Robot Head** as Embodied **Conversational** Agent, [[Paper]](https://arxiv.org/pdf/2305.10945.pdf)

- (arXiv 2023.5) Instruct2Act: Mapping Multi-modality Instructions to **Robotic Actions** with Large Language Model, [[Paper]](https://arxiv.org/pdf/2305.11176.pdf), [[Code]](https://github.com/OpenGVLab/Instruct2Act)

- (arXiv 2023.5) **Principle**-Driven Self-Alignment of Language Models from Scratch with **Minimal Human Supervision**, [[Paper]](https://arxiv.org/pdf/2305.03047.pdf), [[Project]](https://mitibmdemos.draco.res.ibm.com/dromedary)

- (arXiv 2023.5) Multimodal **Procedural Planning** via Dual Text-Image Prompting, [[Paper]](https://arxiv.org/pdf/2305.01795.pdf), [[Code]](https://github.com/YujieLu10/TIP)

- (arXiv 2023.5) ArK: **Augmented Reality** with **Knowledge** Interactive Emergent Ability, [[Paper]](https://arxiv.org/pdf/2305.00970.pdf)

### 2023.4

<!-- - (arXiv 2023.4) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.4) LLaMA-Adapter V2: Parameter-**Efficient** **Visual Instruction** Model, [[Paper]](https://arxiv.org/pdf/2304.15010.pdf), [[Code]](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- (arXiv 2023.4) Multimodal Grounding for **Embodied AI** via **Augmented Reality** Headsets for Natural Language Driven Task Planning, [[Paper]](https://arxiv.org/pdf/2304.13676.pdf)

- (arXiv 2023.4) mPLUG-Owl : **Modularization** Empowers Large Language Models with **Multimodality**, [[Paper]](https://arxiv.org/pdf/2304.14178.pdf), [[Code]](https://github.com/X-PLUG/mPLUG-Owl)

- (arXiv 2023.4) ChatVideo: A Tracklet-centric Multimodal and Versatile **Video Understanding** System, [[Paper]](https://arxiv.org/pdf/2304.14407.pdf), [[Project]](https://www.wangjunke.info/ChatVideo/)

- (arXiv 2023.4) ChatABL: **Abductive Learning** via Natural Language Interaction with ChatGPT, [[Paper]](https://arxiv.org/pdf/2304.11107.pdf)

- (arXiv 2023.4) **Robot**-Enabled Construction **Assembly** with Automated Sequence Planning based on ChatGPT: RoboGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.11018.pdf)

- (arXiv 2023.4) Graph-ToolFormer: To Empower LLMs with **Graph Reasoning** Ability via Prompt Augmented by ChatGPT, [[Paper]](https://arxiv.org/pdf/2304.11116.pdf), [[Code]](https://github.com/jwzhanggy/Graph_Toolformer)

- (arXiv 2023.4) Can GPT-4 Perform **Neural Architecture Search**?, [[Paper]](https://arxiv.org/pdf/2304.10970.pdf), [[Code]](https://github.com/mingkai-zheng/GENIUS)

- (arXiv 2023.4) MiniGPT-4: Enhancing **Vision-Language** Understanding with Advanced Large Language Models, [[Paper]](https://arxiv.org/pdf/2304.10592.pdf), [[Project]](https://minigpt-4.github.io/)

- (arXiv 2023.4) SINC: Spatial Composition of **3D** Human **Motions** for Simultaneous Action Generation, [[Paper]](https://arxiv.org/pdf/2304.10417.pdf), [[Project]](https://sinc.is.tue.mpg.de/)

- (arXiv 2023.4) LLM as A **Robotic Brain**: Unifying **Egocentric** Memory and Control, [[Paper]](https://arxiv.org/pdf/2304.09349.pdf)

- (arXiv 2023.4) Chameleon: **Plug-and-Play** Compositional **Reasoning** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2304.09842.pdf), [[Project]](https://chameleon-llm.github.io/)

- (arXiv 2023.4) Visual **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2304.08485.pdf), [[Project]](https://llava-vl.github.io/)

- (arXiv 2023.4) MiniGPT-4: Enhancing **Vision-language** Understanding with Advanced Large Language Models, [[Paper]](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf), [[Project]](https://minigpt-4.github.io/)

- (arXiv 2023.4) RAFT: **Reward** rAnked FineTuning for **Generative** Foundation Model Alignment, [[Paper]](https://arxiv.org/pdf/2304.06767.pdf), [[Code]]()

- (arXiv 2023.4) **Multimodal** C4: An Open, Billion-scale Corpus of Images Interleaved With Text, [[Paper]](https://arxiv.org/pdf/2304.06939.pdf), [[Code]](https://github.com/allenai/mmc4)

- (arXiv 2023.4) ViewRefer: Grasp the Multi-view Knowledge for **3D Visual Grounding** with GPT and Prototype Guidance, [[Paper]](https://arxiv.org/pdf/2303.16894.pdf), [[Code]](https://github.com/ZiyuGuo99/ViewRefer3D)

- (arXiv 2023.4) **HuggingGPT**: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, [[Paper]](https://arxiv.org/pdf/2303.17580.pdf), [[Code]](https://github.com/microsoft/JARVIS)

- (arXiv 2023.4) ERRA: An **Embodied** Representation and Reasoning Architecture for **Long-horizon** Language-conditioned **Manipulation** Tasks, [[Paper]](https://arxiv.org/pdf/2304.02251.pdf), [[Code]](https://robotll.github.io/ERRA/)

- (arXiv 2023.4) Advancing **Medical Imaging** with Language Models: A Journey from N-grams to ChatGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.04920.pdf)

- (arXiv 2023.4) ChatGPT Empowered Long-Step **Robot Control** in Various Environments: A Case Application, [[Paper]](https://arxiv.org/pdf/2304.03893.pdf), [[Code]](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts)

- (arXiv 2023.4) OpenAGI: When LLM Meets Domain **Experts**, [[Paper]](https://arxiv.org/pdf/2304.04370.pdf), [[Code]](https://github.com/agiresearch/OpenAGI)

- (arXiv 2023.4) **Video** ChatCaptioner: Towards the Enriched Spatiotemporal **Descriptions**, [[Paper]](https://arxiv.org/pdf/2304.04227.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

### 2023.3

<!-- - (arXiv 2023.3) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.3) Open-World **Object Manipulation** using Pre-Trained Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2303.00905.pdf), [[Project]](https://robot-moo.github.io/)

- (arXiv 2023.3) Grounded Decoding: Guiding Text Generation with Grounded Models for **Robot Control**, [[Paper]](https://arxiv.org/pdf/2303.00855.pdf), [[Project]](https://grounded-decoding.github.io/) 

- (arXiv 2023.3) Task and Motion **Planning** with Large Language Models for **Object Rearrangement**, [[Paper]](https://arxiv.org/pdf/2303.06247.pdf), [[Project]](https://sites.google.com/view/llm-grop)

- (arXiv 2023.3) RE-MOVE: An Adaptive **Policy Design** Approach for Dynamic Environments via Language-Based Feedback, [[Paper]](https://arxiv.org/pdf/2303.07622.pdf), [[Project]](http://gamma.umd.edu/remove/)

- (arXiv 2023.3) Chat with the Environment: **Interactive Multimodal Perception** using Large Language Models, [[Paper]](https://arxiv.org/pdf/2303.08268.pdf)

- (arXiv 2023.3) MAtch, eXpand and Improve: Unsupervised Finetuning for **Zero-Shot Action Recognition** with Language Knowledge, [[Paper]](https://arxiv.org/pdf/2303.08914.pdf), [[Code]](https://github.com/wlin-at/MAXI)

- (arXiv 2023.3) DialogPaint: A Dialog-based **Image Editing** Model, [[Paper]](https://arxiv.org/pdf/2303.10073.pdf)

- (arXiv 2023.3) MM-REACT : Prompting ChatGPT for **Multimodal Reasoning** and **Action**, [[Paper]](https://arxiv.org/pdf/2303.11381.pdf), [[Project]](https://multimodal-react.github.io/)

- (arXiv 2023.3) eP-ALM: Efficient Perceptual **Augmentation** of Language Models, [[Paper]](https://arxiv.org/pdf/2303.11403.pdf), [[Code]](https://github.com/mshukor/eP-ALM)

- (arXiv 2023.3) Errors are Useful Prompts: Instruction Guided **Task Programming** with Verifier-Assisted Iterative Prompting, [[Paper]](https://arxiv.org/pdf/2303.14100.pdf), [[Project]](https://ac-rad.github.io/clairify/)

- (arXiv 2023.3) **LLaMA**-**Adapter**: Efficient Fine-tuning of Language Models with Zero-init Attention, [[Paper]](https://arxiv.org/pdf/2303.16199.pdf), [[Code]](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- (arXiv 2023.3) **MULTIMODAL** ANALOGICAL **REASONING** OVER KNOWLEDGE GRAPHS, [[Paper]](https://arxiv.org/pdf/2210.00312.pdf), [[Code]](https://github.com/zjunlp/MKG_Analogy)

- (arXiv 2023.3) CAN LARGE LANGUAGE MODELS **DESIGN** A **ROBOT**? [[Paper]](https://arxiv.org/pdf/2303.15324.pdf)

- (arXiv 2023.3) Learning **video** embedding space with Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2303.14584.pdf)

- (arXiv 2023.3) Audio Visual Language Maps for Robot **Navigation**, [[Paper]](https://arxiv.org/pdf/2303.07522.pdf), [[Project]](https://avlmaps.github.io/)

- (arXiv 2023.3) ViperGPT: Visual Inference via **Python** Execution for **Reasoning**, [[Paper]](https://arxiv.org/pdf/2303.08128.pdf)

- (arXiv 2023.3) **ChatGPT** Asks, **BLIP-2** Answers: Automatic Questioning Towards Enriched **Visual Descriptions**, [[Paper]](https://arxiv.org/pdf/2303.06594.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

- (arXiv 2023.3) Can an **Embodied** Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation, [[Paper]](https://arxiv.org/pdf/2303.03480.pdf), [[Project]](https://gamma.umd.edu/LGX/)

- (arXiv 2023.3) Visual **ChatGPT**: Talking, Drawing and Editing with Visual Foundation Models, [[Paper]](https://arxiv.org/abs/2303.04671), [[Code]](https://github.com/microsoft/visual-chatgpt)

- (arXiv 2023.3) PaLM-E: An **Embodied** Multimodal Language Model, [[Paper]](https://palm-e.github.io/assets/palm-e.pdf), [[Project]](https://palm-e.github.io/)

- (arXiv 2023.3) Language Is Not All You Need: **Aligning** Perception with Language Models, [[Paper]](https://arxiv.org/pdf/2302.14045.pdf), [[Code]](https://github.com/microsoft/unilm)

### 2023.2

- (arXiv 2023.2) ChatGPT for **Robotics**: Design Principles and Model Abilities, , [[Paper]](https://arxiv.org/pdf/2306.17582.pdf), [[Code]](https://github.com/microsoft/PromptCraft-Robotics)

- (arXiv 2023.2) Internet Explorer: Targeted Representation Learning on the Open Web, [[Paper]](https://arxiv.org/pdf/2302.14051.pdf), [[Project]](https://internet-explorer-ssl.github.io/)

### 2022.11

- (arXiv 2022.11) Visual Programming: Compositional **visual reasoning** without training, [[Paper]](https://arxiv.org/pdf/2211.11559.pdf), [[Project]](https://prior.allenai.org/projects/visprog)

### 2022.7

- (arXiv 2022.7) Language Models are General-Purpose **Interfaces**, [[Paper]](https://arxiv.org/pdf/2206.06336.pdf), [[Code]](https://github.com/microsoft/unilm)

- (arXiv 2022.7) LM-Nav: Robotic **Navigation** with Large Pre-Trained Models of Language, Vision, and Action, [[Paper]](https://arxiv.org/pdf/2207.04429.pdf), [[Project]](https://sites.google.com/view/lmnav)
