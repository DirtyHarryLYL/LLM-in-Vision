# LLM-in_Vision
Recent LLM (Large Language Models)-based CV and multi-modal works. Welcome to comment/contribute!

### 2023.4

<!-- - (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.4) ViewRefer: Grasp the Multi-view Knowledge for **3D Visual Grounding** with GPT and Prototype Guidance, [[Paper]](https://arxiv.org/pdf/2303.16894.pdf), [[Code]](https://github.com/ZiyuGuo99/ViewRefer3D)

- (arXiv 2023.4) **HuggingGPT**: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, [[Paper]](https://arxiv.org/pdf/2303.17580.pdf), [[Code]](https://github.com/microsoft/JARVIS)

- (arXiv 2023.4) ERRA: An **Embodied** Representation and Reasoning Architecture for **Long-horizon** Language-conditioned **Manipulation** Tasks, [[Paper]](https://arxiv.org/pdf/2304.02251.pdf), [[Code]](https://robotll.github.io/ERRA/)

- (arXiv 2023.4) Advancing **Medical Imaging** with Language Models: A Journey from N-grams to ChatGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.04920.pdf)

- (arXiv 2023.4) ChatGPT Empowered Long-Step **Robot Control** in Various Environments: A Case Application, [[Paper]](https://arxiv.org/pdf/2304.03893.pdf), [[Code]](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts)

- (arXiv 2023.4) OpenAGI: When LLM Meets Domain **Experts**, [[Paper]](https://arxiv.org/pdf/2304.04370.pdf), [[Code]](https://github.com/agiresearch/OpenAGI)

- (arXiv 2023.4) **Video** ChatCaptioner: Towards the Enriched Spatiotemporal **Descriptions**, [[Paper]](https://arxiv.org/pdf/2304.04227.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

### 2023.3

<!-- - (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.3) **MULTIMODAL** ANALOGICAL **REASONING** OVER KNOWLEDGE GRAPHS, [[Paper]](https://arxiv.org/pdf/2210.00312.pdf), [[Code]](https://github.com/zjunlp/MKG_Analogy)

- (arXiv 2023.3) CAN LARGE LANGUAGE MODELS **DESIGN** A **ROBOT**? [[Paper]](https://arxiv.org/pdf/2303.15324.pdf)

- (arXiv 2023.3) Learning **video** embedding space with Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2303.14584.pdf)

- (arXiv 2023.3) Audio Visual Language Maps for Robot **Navigation**, [[Paper]](https://arxiv.org/pdf/2303.07522.pdf), [[Project]](https://avlmaps.github.io/)

- (arXiv 2023.3) ViperGPT: Visual Inference via **Python** Execution for **Reasoning**, [[Paper]](https://arxiv.org/pdf/2303.08128.pdf)

- (arXiv 2023.3) **ChatGPT** Asks, **BLIP-2** Answers: Automatic Questioning Towards Enriched **Visual Descriptions**, [[Paper]](https://arxiv.org/pdf/2303.06594.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

- (arXiv 2023.3) Can an **Embodied** Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation, [[Paper]](https://arxiv.org/pdf/2303.03480.pdf), [[Project]](https://gamma.umd.edu/LGX/)

- (arXiv 2023.3) Visual **ChatGPT**: Talking, Drawing and Editing with Visual Foundation Models, [[Paper]](https://arxiv.org/abs/2303.04671), [[Code]](https://github.com/microsoft/visual-chatgpt)

- (arXiv 2023.3) PaLM-E: An **Embodied** Multimodal Language Model, [[Paper]](https://palm-e.github.io/assets/palm-e.pdf), [[Project]](https://palm-e.github.io/)

- (arXiv 2023.3) Language Is Not All You Need: **Aligning** Perception with Language Models, [[Paper]](https://arxiv.org/pdf/2302.14045.pdf), [[Code]](https://github.com/microsoft/unilm)

### 2023.2

- (arXiv 2023.2) ChatGPT for **Robotics**: Design Principles and Model Abilities, , [[Paper]](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf), [[Code]](https://github.com/microsoft/PromptCraft-Robotics)

- (arXiv 2023.2) Internet Explorer: Targeted Representation Learning on the Open Web, [[Paper]](https://arxiv.org/pdf/2302.14051.pdf), [[Project]](https://internet-explorer-ssl.github.io/)

### 2022.7

- (arXiv 2022.7) Language Models are General-Purpose **Interfaces**, [[Paper]](https://arxiv.org/pdf/2206.06336.pdf), [[Code]](https://github.com/microsoft/unilm)

- (arXiv 2022.7) LM-Nav: Robotic **Navigation** with Large Pre-Trained Models of Language, Vision, and Action, [[Paper]](https://arxiv.org/pdf/2207.04429.pdf), [[Project]](https://sites.google.com/view/lmnav)
