# LLM-in_Vision
Recent LLM (Large Language Models)-based CV and multi-modal works. Welcome to comment/contribute!

### 2023.4

<!-- - (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]()

- (arXiv 2023.4) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.4) SINC: Spatial Composition of **3D** Human **Motions** for Simultaneous Action Generation, [[Paper]](https://arxiv.org/pdf/2304.10417.pdf), [[Project]](https://sinc.is.tue.mpg.de/)

- (arXiv 2023.4) LLM as A **Robotic Brain**: Unifying **Egocentric** Memory and Control, [[Paper]](https://arxiv.org/pdf/2304.09349.pdf)

- (arXiv 2023.4) Chameleon: **Plug-and-Play** Compositional **Reasoning** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2304.09842.pdf), [[Project]](https://chameleon-llm.github.io/)

- (arXiv 2023.4) Visual **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2304.08485.pdf), [[Project]](https://llava-vl.github.io/)

- (arXiv 2023.4) MiniGPT-4: Enhancing **Vision-language** Understanding with Advanced Large Language Models, [[Paper]](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf), [[Project]](https://minigpt-4.github.io/)

- (arXiv 2023.4) RAFT: **Reward** rAnked FineTuning for **Generative** Foundation Model Alignment, [[Paper]](https://arxiv.org/pdf/2304.06767.pdf), [[Code]]()

- (arXiv 2023.4) **Multimodal** C4: An Open, Billion-scale Corpus of Images Interleaved With Text, [[Paper]](https://arxiv.org/pdf/2304.06939.pdf), [[Code]](https://github.com/allenai/mmc4)

- (arXiv 2023.4) ViewRefer: Grasp the Multi-view Knowledge for **3D Visual Grounding** with GPT and Prototype Guidance, [[Paper]](https://arxiv.org/pdf/2303.16894.pdf), [[Code]](https://github.com/ZiyuGuo99/ViewRefer3D)

- (arXiv 2023.4) **HuggingGPT**: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, [[Paper]](https://arxiv.org/pdf/2303.17580.pdf), [[Code]](https://github.com/microsoft/JARVIS)

- (arXiv 2023.4) ERRA: An **Embodied** Representation and Reasoning Architecture for **Long-horizon** Language-conditioned **Manipulation** Tasks, [[Paper]](https://arxiv.org/pdf/2304.02251.pdf), [[Code]](https://robotll.github.io/ERRA/)

- (arXiv 2023.4) Advancing **Medical Imaging** with Language Models: A Journey from N-grams to ChatGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.04920.pdf)

- (arXiv 2023.4) ChatGPT Empowered Long-Step **Robot Control** in Various Environments: A Case Application, [[Paper]](https://arxiv.org/pdf/2304.03893.pdf), [[Code]](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts)

- (arXiv 2023.4) OpenAGI: When LLM Meets Domain **Experts**, [[Paper]](https://arxiv.org/pdf/2304.04370.pdf), [[Code]](https://github.com/agiresearch/OpenAGI)

- (arXiv 2023.4) **Video** ChatCaptioner: Towards the Enriched Spatiotemporal **Descriptions**, [[Paper]](https://arxiv.org/pdf/2304.04227.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

### 2023.3

<!-- - (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]()

- (arXiv 2023.3) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.3) Task and Motion **Planning** with Large Language Models for **Object Rearrangement**, [[Paper]](https://arxiv.org/pdf/2303.06247.pdf), [[Project]](https://sites.google.com/view/llm-grop)

- (arXiv 2023.3) RE-MOVE: An Adaptive **Policy Design** Approach for Dynamic Environments via Language-Based Feedback, [[Paper]](https://arxiv.org/pdf/2303.07622.pdf), [[Project]](http://gamma.umd.edu/remove/)

- (arXiv 2023.3) Chat with the Environment: **Interactive Multimodal Perception** using Large Language Models, [[Paper]](https://arxiv.org/pdf/2303.08268.pdf)

- (arXiv 2023.3) MAtch, eXpand and Improve: Unsupervised Finetuning for **Zero-Shot Action Recognition** with Language Knowledge, [[Paper]](https://arxiv.org/pdf/2303.08914.pdf), [[Code]](https://github.com/wlin-at/MAXI)

- (arXiv 2023.3) DialogPaint: A Dialog-based **Image Editing** Model, [[Paper]](https://arxiv.org/pdf/2303.10073.pdf)

- (arXiv 2023.3) MM-REACT : Prompting ChatGPT for **Multimodal Reasoning** and **Action**, [[Paper]](https://arxiv.org/pdf/2303.11381.pdf), [[Project]](https://multimodal-react.github.io/)

- (arXiv 2023.3) eP-ALM: Efficient Perceptual **Augmentation** of Language Models, [[Paper]](https://arxiv.org/pdf/2303.11403.pdf), [[Code]](https://github.com/mshukor/eP-ALM)

- (arXiv 2023.3) Errors are Useful Prompts: Instruction Guided **Task Programming** with Verifier-Assisted Iterative Prompting, [[Paper]](https://arxiv.org/pdf/2303.14100.pdf), [[Project]](https://ac-rad.github.io/clairify/)

- (arXiv 2023.3) **LLaMA**-**Adapter**: Efficient Fine-tuning of Language Models with Zero-init Attention, [[Paper]](https://arxiv.org/pdf/2303.16199.pdf), [[Code]](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- (arXiv 2023.3) **MULTIMODAL** ANALOGICAL **REASONING** OVER KNOWLEDGE GRAPHS, [[Paper]](https://arxiv.org/pdf/2210.00312.pdf), [[Code]](https://github.com/zjunlp/MKG_Analogy)

- (arXiv 2023.3) CAN LARGE LANGUAGE MODELS **DESIGN** A **ROBOT**? [[Paper]](https://arxiv.org/pdf/2303.15324.pdf)

- (arXiv 2023.3) Learning **video** embedding space with Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2303.14584.pdf)

- (arXiv 2023.3) Audio Visual Language Maps for Robot **Navigation**, [[Paper]](https://arxiv.org/pdf/2303.07522.pdf), [[Project]](https://avlmaps.github.io/)

- (arXiv 2023.3) ViperGPT: Visual Inference via **Python** Execution for **Reasoning**, [[Paper]](https://arxiv.org/pdf/2303.08128.pdf)

- (arXiv 2023.3) **ChatGPT** Asks, **BLIP-2** Answers: Automatic Questioning Towards Enriched **Visual Descriptions**, [[Paper]](https://arxiv.org/pdf/2303.06594.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

- (arXiv 2023.3) Can an **Embodied** Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation, [[Paper]](https://arxiv.org/pdf/2303.03480.pdf), [[Project]](https://gamma.umd.edu/LGX/)

- (arXiv 2023.3) Visual **ChatGPT**: Talking, Drawing and Editing with Visual Foundation Models, [[Paper]](https://arxiv.org/abs/2303.04671), [[Code]](https://github.com/microsoft/visual-chatgpt)

- (arXiv 2023.3) PaLM-E: An **Embodied** Multimodal Language Model, [[Paper]](https://palm-e.github.io/assets/palm-e.pdf), [[Project]](https://palm-e.github.io/)

- (arXiv 2023.3) Language Is Not All You Need: **Aligning** Perception with Language Models, [[Paper]](https://arxiv.org/pdf/2302.14045.pdf), [[Code]](https://github.com/microsoft/unilm)

### 2023.2

- (arXiv 2023.2) ChatGPT for **Robotics**: Design Principles and Model Abilities, , [[Paper]](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf), [[Code]](https://github.com/microsoft/PromptCraft-Robotics)

- (arXiv 2023.2) Internet Explorer: Targeted Representation Learning on the Open Web, [[Paper]](https://arxiv.org/pdf/2302.14051.pdf), [[Project]](https://internet-explorer-ssl.github.io/)

### 2022.7

- (arXiv 2022.7) Language Models are General-Purpose **Interfaces**, [[Paper]](https://arxiv.org/pdf/2206.06336.pdf), [[Code]](https://github.com/microsoft/unilm)

- (arXiv 2022.7) LM-Nav: Robotic **Navigation** with Large Pre-Trained Models of Language, Vision, and Action, [[Paper]](https://arxiv.org/pdf/2207.04429.pdf), [[Project]](https://sites.google.com/view/lmnav)
