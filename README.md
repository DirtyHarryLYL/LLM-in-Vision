# LLM-in-Vision
Recent LLM (Large Language Models)-based CV and multi-modal works. Welcome to comment/contribute!

### 2023.10

<!-- - (arXiv 2023.10) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.10) Evaluating **Spatial Understanding** of Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.14540.pdf)

- (arXiv 2023.10) Learning **Reward** for **Physical Skills** using Large Language Model, [[Paper]](https://arxiv.org/pdf/2310.14092.pdf)

- (arXiv 2023.10) CREATIVE **ROBOT TOOL USE** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.13065.pdf), [[Project]](https://creative-robotool.github.io/)

- (arXiv 2023.10) Open-Ended Instructable **Embodied Agents** with Memory-Augmented Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.15127.pdf), [[Project]](https://helper-agent-llm.github.io/)

- (arXiv 2023.10) **Robot** Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World **Reinforcement Learning**, [[Paper]](https://arxiv.org/pdf/2310.15145.pdf), [[Project]](https://robofume.github.io/)

- (arXiv 2023.10) LARGE LANGUAGE MODELS CAN **Share IMAGES**, TOO! [[Paper]](https://arxiv.org/pdf/2310.14804.pdf), [[Code]](https://github.com/passing2961/LLM-Share-Image)

- (arXiv 2023.10) Dataset **Bias** Mitigation in Multiple-Choice **Visual Question Answering** and Beyond, [[Paper]](https://arxiv.org/pdf/2310.14670.pdf)

- (arXiv 2023.10) HALLUSIONBENCH: You See What You Think? Or You Think What You See? An **Image-Context Reasoning Benchmark** Challenging for GPT-4V(vision), LLaVA-1.5, and Other Multi-modality Models, [[Paper]](https://arxiv.org/pdf/2310.14566.pdf), [[Code]](https://github.com/tianyi-lab/HallusionBench)

- (arXiv 2023.10) Can Language Models Laugh at YouTube **Short-form Videos**? [[Paper]](https://arxiv.org/pdf/2310.14159.pdf), [[Code]](https://github.com/dayoon-ko/ExFunTube)

- (arXiv 2023.10) Large Language Models are **Visual Reasoning** Coordinators, [[Paper]](https://arxiv.org/pdf/2310.15166.pdf), [[Code]](https://github.com/cliangyu/Cola)

- (arXiv 2023.10) Language Models as Zero-Shot **Trajectory Generators**, [[Paper]](https://arxiv.org/pdf/2310.11604.pdf), [[Project]](https://www.robot-learning.uk/language-models-trajectory-generators)

- (arXiv 2023.10) **Localizing** Active **Objects** from **Egocentric** Vision with Symbolic World Knowledge, [[Paper]](https://arxiv.org/pdf/2310.15066.pdf), [[Code]](https://github.com/PlusLabNLP/ENVISION)

- (arXiv 2023.10) Multimodal Large Language Model for Visual **Navigation**, [[Paper]](https://arxiv.org/pdf/2310.08669.pdf)

- (arXiv 2023.10) MAKING **MULTIMODAL GENERATION** EASIER: WHEN DIFFUSION MODELS MEET LLMS, [[Paper]](https://arxiv.org/pdf/2310.08949.pdf), [[Code]](https://github.com/zxy556677/EasyGen)

- (arXiv 2023.10) Open X-Embodiment: **Robotic Learning** Datasets and RT-X Models, [[Paper]](https://arxiv.org/pdf/2310.08864.pdf), [[Project]](https://robotics-transformer-x.github.io/)

- (arXiv 2023.10) Large Language Models Meet **Open-World Intent** Discovery and Recognition: An Evaluation of ChatGPT, [[Paper]](https://arxiv.org/pdf/2310.10176.pdf), [[Code]](https://github.com/songxiaoshuai/OOD-Evaluation)

- (arXiv 2023.10) Lost in Translation: When GPT-4V(ision) Can’t See Eye to Eye with Text A **Vision-Language-Consistency** Analysis of VLLMs and Beyond, [[Paper]](https://arxiv.org/pdf/2310.12520.pdf)

- (arXiv 2023.10) Interactive **Navigation** in Environments with Traversable Obstacles Using Large Language and Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2310.08873.pdf)

- (arXiv 2023.10) VLIS: Unimodal Language Models Guide **Multimodal** **Language Generation**, [[Paper]](https://arxiv.org/pdf/2310.09767.pdf), [[Code]](https://github.com/JiwanChung/vlis)

- (arXiv 2023.10) CLIN: A CONTINUALLY LEARNING LANGUAGE **AGENT** FOR RAPID TASK ADAPTATION AND GENERALIZATION, [[Paper]](https://arxiv.org/pdf/2310.10134.pdf), [[Project]](https://allenai.github.io/clin/)

- (arXiv 2023.10) **Navigation** with Large Language Models: Semantic Guesswork as a Heuristic for Planning, [[Paper]](https://arxiv.org/pdf/2310.10103.pdf)

- (arXiv 2023.10) Lost in Translation: When GPT-4V(ision) Can’t See Eye to Eye with Text A **Vision-Language-Consistency** Analysis of VLLMs and Beyond, [[Paper]](https://arxiv.org/pdf/2310.12520.pdf)

- (arXiv 2023.10) FROZEN TRANSFORMERS IN **LANGUAGE** MODELS ARE EFFECTIVE **VISUAL** ENCODER LAYERS, [[Paper]](https://arxiv.org/pdf/2310.12973.pdf), [[Code]](https://github.com/ziqipang/LM4VisualEncoding)

- (arXiv 2023.10) CLAIR: Evaluating **Image Captions** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.12971.pdf), [[Project]](https://davidmchan.github.io/clair/)

- (arXiv 2023.10) 3D-GPT: PROCEDURAL **3D MODELING** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.12945.pdf), [[Project]](https://chuny1.github.io/3DGPT/3dgpt.html)

- (arXiv 2023.10) Automated Natural Language **Explanation** of Deep Visual Neurons with Large Models, [[Paper]](https://arxiv.org/pdf/2310.10708.pdf)

- (arXiv 2023.10) Set-of-Mark Prompting Unleashes Extraordinary **Visual Grounding** in GPT-4V, [[Paper]](https://arxiv.org/pdf/2310.11441.pdf), [[Project]](https://som-gpt4v.github.io/)

- (arXiv 2023.10) EvalCrafter: **Benchmarking** and Evaluating Large **Video Generation** Models, [[Paper]](https://arxiv.org/pdf/2310.11440.pdf), [[Project]](https://evalcrafter.github.io/)

- (arXiv 2023.10) MISAR: A **MULTIMODAL** INSTRUCTIONAL SYSTEM WITH **AUGMENTED REALITY**, [[Paper]](https://arxiv.org/pdf/2310.11699.pdf), [[Code]](https://github.com/nguyennm1024/misar)

- (arXiv 2023.10) NON-INTRUSIVE ADAPTATION: INPUT-CENTRIC PARAMETER-**EFFICIENT** **FINE-TUNING** FOR VERSATILE **MULTIMODAL** MODELING, [[Paper]](https://arxiv.org/pdf/2310.12100.pdf)

- (arXiv 2023.10) LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for **Robotic** Tabletop **Manipulation**, [[Paper]](https://arxiv.org/pdf/2310.12020.pdf), [[Project]](https://shengqiang-zhang.github.io/lohoravens-webpage/)

- (arXiv 2023.10) ChatGPT-guided Semantics for **Zero-shot** Learning, [[Paper]](https://arxiv.org/pdf/2310.11657.pdf)

- (arXiv 2023.10) On the Benefit of Generative Foundation Models for Human **Activity Recognition**, [[Paper]](https://arxiv.org/pdf/2310.12085.pdf)

- (arXiv 2023.10) DiagrammerGPT: **Generating** Open-Domain, Open-Platform **Diagrams** via LLM Planning, [[Paper]](https://arxiv.org/pdf/2310.12128.pdf), [[Project]](https://diagrammergpt.github.io/)

- (arXiv 2023.10) Interactive Task **Planning** with Language Models, [[Paper]](https://arxiv.org/pdf/2310.10645.pdf), [[Project]](https://wuphilipp.github.io/itp_site/)

- (arXiv 2023.10) Bootstrap Your Own **Skills**: Learning to Solve New Tasks with Large Language Model Guidance, [[Paper]](https://arxiv.org/pdf/2310.10021.pdf), [[Project]](https://clvrai.github.io/boss/)

- (arXiv 2023.10) Penetrative AI: Making LLMs Comprehend the **Physical** World, [[Paper]](https://arxiv.org/pdf/2310.09605.pdf)

- (arXiv 2023.10) **BONGARD**-OPENWORLD: FEW-SHOT REASONING FOR FREE-FORM VISUAL CONCEPTS IN THE REAL WORLD, [[Paper]](https://arxiv.org/pdf/2310.10207.pdf), [[Project]](https://joyjayng.github.io/Bongard-OpenWorld.github.io/)

- (arXiv 2023.10) ViPE: **Visualise** Pretty-much Everything, [[Paper]](https://arxiv.org/pdf/2310.10543.pdf)

- (arXiv 2023.10) MINIGPT-V2: LARGE LANGUAGE MODEL AS A UNIFIED INTERFACE FOR **VISION-LANGUAGE** MULTITASK LEARNING, [[Paper]](https://arxiv.org/pdf/2310.09478.pdf), [[Project]](https://minigpt-v2.github.io/)

- (arXiv 2023.10) MoConVQ: Unified Physics-Based **Motion Control** via Scalable Discrete Representations, [[Paper]](https://arxiv.org/pdf/2310.10198.pdf)

- (arXiv 2023.10) LLM BLUEPRINT: ENABLING **TEXT-TO-IMAGE** GENERATION WITH COMPLEX AND DETAILED PROMPTS, [[Paper]](https://arxiv.org/pdf/2310.10640.pdf)

- (arXiv 2023.10) VIDEO LANGUAGE **PLANNING**, [[Paper]](https://arxiv.org/pdf/2310.10625.pdf), [[Project]](https://video-language-planning.github.io/)

- (arXiv 2023.10) Dobby: A Conversational Service **Robot** Driven by GPT-4, [[Paper]](https://arxiv.org/pdf/2310.06303.pdf)

- (arXiv 2023.10) CoPAL: Corrective **Planning** of **Robot** Actions with Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.07263.pdf)

- (arXiv 2023.10) Forgetful Large Language Models: Lessons Learned from Using LLMs in **Robot Programming**, [[Paper]](https://arxiv.org/pdf/2310.06646.pdf)

- (arXiv 2023.10) TREE-PLANNER: EFFICIENT CLOSE-LOOP TASK **PLANNING** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.08582.pdf), [[Project]](https://tree-planner.github.io/)

- (arXiv 2023.10) TOWARDS ROBUST **MULTI-MODAL** REASONING VIA MODEL SELECTION, [[Paper]](https://arxiv.org/pdf/2310.08446.pdf), [[Code]](https://github.com/LINs-lab/M3)

- (arXiv 2023.10) FERRET: REFER AND **GROUND** ANYTHING ANYWHERE AT ANY GRANULARITY, [[Paper]](https://arxiv.org/pdf/2310.07704.pdf), [[Code]](https://github.com/apple/ml-ferret)

- (arXiv 2023.10) FROM SCARCITY TO EFFICIENCY: IMPROVING **CLIP TRAINING** VIA VISUAL-ENRICHED **CAPTIONS**, [[Paper]](https://arxiv.org/pdf/2310.07699.pdf)

- (arXiv 2023.10) OPENLEAF: OPEN-DOMAIN INTERLEAVED **IMAGE-TEXT** GENERATION AND EVALUATION, [[Paper]](https://arxiv.org/pdf/2310.07749.pdf)

- (arXiv 2023.10) Can We **Edit** **Multimodal** Large Language Models? [[Paper]](https://arxiv.org/pdf/2310.08475.pdf), [[Code]](https://github.com/zjunlp/EasyEdit)

- (arXiv 2023.10) VISUAL **DATA-TYPE** UNDERSTANDING DOES NOT EMERGE FROM SCALING VISION-LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.08577.pdf), [[Code]](https://github.com/bethgelab/DataTypeIdentification)

- (arXiv 2023.10) Idea2Img: Iterative Self-Refinement with GPT-4V(vision) for Automatic **Image Design and Generation**, [[Paper]](https://arxiv.org/pdf/2310.08541.pdf), [[Project]](https://idea2img.github.io/)

- (arXiv 2023.10) OCTOPUS: **EMBODIED** VISION-LANGUAGE PROGRAMMER FROM ENVIRONMENTAL FEEDBACK, [[Paper]](https://arxiv.org/pdf/2310.08588.pdf), [[Project]](https://choiszt.github.io/Octopus/)

### 2023.9

<!-- - (arXiv 2023.9) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.9) DynaCon: Dynamic **Robot Planner** with Contextual Awareness via LLMs, [[Paper]](https://arxiv.org/pdf/2309.16031.pdf), [[Project]](https://sites.google.com/view/dynacon)

- (arXiv 2023.9) AnyMAL: An Efficient and Scalable **Any-Modality** Augmented Language Model, [[Paper]](https://arxiv.org/pdf/2309.16058.pdf)

- (arXiv 2023.9) ConceptGraphs: Open-Vocabulary **3D Scene Graphs** for Perception and **Planning**, [[Paper]](https://arxiv.org/pdf/2309.16650.pdf), [[Project]](https://concept-graphs.github.io/)

- (arXiv 2023.9) LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic **Object Rearrangement**, [[Paper]](https://arxiv.org/pdf/2309.15821.pdf), [[Code]](https://github.com/changhaonan/LG-MCTS)

- (arXiv 2023.9) ONE FOR ALL: **VIDEO CONVERSATION** IS FEASIBLE WITHOUT VIDEO INSTRUCTION TUNING, [[Paper]](https://arxiv.org/pdf/2309.15785.pdf)

- (arXiv 2023.9) Verifiable Learned Behaviors via **Motion Primitive Composition**: Applications to Scooping of Granular Media, [[Paper]](https://arxiv.org/pdf/2309.14894.pdf)

- (arXiv 2023.9) Human-Assisted Continual **Robot** Learning with Foundation Models, [[Paper]](https://arxiv.org/pdf/2309.14321.pdf), [[Project]](https://sites.google.com/mit.edu/halp-robot-learning)

- (arXiv 2023.9) InternLM-XComposer: A **Vision-Language** Large Model for Advanced Text-image Comprehension and Composition, [[Paper]](https://arxiv.org/pdf/2309.15112.pdf), [[Code]](https://github.com/InternLM/InternLM-XComposer)

- (arXiv 2023.9) VIDEODIRECTORGPT: CONSISTENT MULTI-SCENE **VIDEO GENERATION** VIA LLM-GUIDED PLANNING, [[Paper]](https://arxiv.org/pdf/2309.15091.pdf), [[Project]](https://videodirectorgpt.github.io/)

- (arXiv 2023.9) **Text-to-Image** Generation for Abstract Concepts, [[Paper]](https://arxiv.org/pdf/2309.14623.pdf)

- (arXiv 2023.9) Free-Bloom: Zero-Shot **Text-to-Video** Generator with LLM Director and LDM Animator, [[Paper]](https://arxiv.org/pdf/2309.14494.pdf), [[Code]](https://github.com/SooLab/Free-Bloom)

- (arXiv 2023.9) ALIGNING LARGE **MULTIMODAL** MODELS WITH FACTUALLY AUGMENTED **RLHF**, [[Paper]](https://arxiv.org/pdf/2309.14525.pdf), [[Project]](https://llava-rlhf.github.io/)

- (arXiv 2023.9) Self-Recovery **Prompting**: Promptable General Purpose Service **Robot** System with Foundation Models and Self-Recovery, [[Paper]](https://arxiv.org/pdf/2309.14425.pdf), [[Project]](https://sites.google.com/view/srgpsr)

- (arXiv 2023.9) Q-BENCH: A BENCHMARK FOR GENERAL-PURPOSE FOUNDATION MODELS ON **LOW-LEVEL VISION**, [[Paper]](https://arxiv.org/pdf/2309.14181.pdf)

- (arXiv 2023.9) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave **Chat** via **Multi-Modal** Causal Attention, [[Paper]](https://arxiv.org/pdf/2309.14327.pdf), [[Code]](https://github.com/microsoft/DeepSpeedExamples)

- (arXiv 2023.9) LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set **Object Recognition**, [[Paper]](https://arxiv.org/pdf/2309.12780.pdf), [[Code]](https://github.com/Harryqu123/LMC)

- (arXiv 2023.9) LLM-Grounder: Open-Vocabulary **3D** Visual **Grounding** with Large Language Model as an Agent, [[Paper]](https://arxiv.org/pdf/2309.12311.pdf), [[Project]](https://chat-with-nerf.github.io/)

- (arXiv 2023.9) Video-ChatGPT: Towards Detailed **Video Understanding** via Large Vision and Language Models, [[Paper]](https://arxiv.org/pdf/2306.05424.pdf), [[Code]](https://github.com/mbzuai-oryx/Video-ChatGPT)

- (arXiv 2023.9) STRUCTCHART: PERCEPTION, STRUCTURING, REASONING FOR **VISUAL CHART** UNDERSTANDING, [[Paper]](https://arxiv.org/pdf/2309.11268.pdf)

- (arXiv 2023.9) DREAMLLM: SYNERGISTIC **MULTIMODAL** COMPREHENSION AND CREATION, [[Paper]](https://arxiv.org/pdf/2309.11499.pdf), [[Project]](https://dreamllm.github.io/)

- (arXiv 2023.9) A LARGE-SCALE DATASET FOR **AUDIO-LANGUAGE** REPRESENTATION LEARNING, [[Paper]](https://arxiv.org/pdf/2309.11500.pdf), [[Project]](https://auto-acd.github.io/)

- (arXiv 2023.9) YOU ONLY LOOK AT SCREENS: **MULTIMODAL** CHAIN-OF-ACTION **AGENTS**, [[Paper]](https://arxiv.org/pdf/2309.11436.pdf), [[Code]](https://github.com/cooelf/Auto-UI)

- (arXiv 2023.9) SMART-LLM: Smart Multi-Agent Robot Task **Planning** using Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.10062.pdf), [[Project]](https://sites.google.com/view/smart-llm/)

- (arXiv 2023.9) Conformal Temporal Logic **Planning** using Large Language Models: Knowing When to Do What and When to Ask for Help, [[Paper]](https://arxiv.org/pdf/2309.10092.pdf), [[Project]](https://ltl-llm.github.io/)

- (arXiv 2023.9) Investigating the **Catastrophic Forgetting** in **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.10313.pdf)

- (arXiv 2023.9) Specification-Driven **Video Search** via Foundation Models and Formal Verification, [[Paper]](https://arxiv.org/pdf/2309.10171.pdf)

- (arXiv 2023.9) Language as the Medium: Multimodal **Video Classification** through text only, [[Paper]](https://arxiv.org/pdf/2309.10783.pdf)

- (arXiv 2023.9) **Multimodal** Foundation Models: From Specialists to General-Purpose Assistants, [[Paper]](https://arxiv.org/pdf/2309.10020.pdf)

- (arXiv 2023.9) TEXTBIND: Multi-turn Interleaved **Multimodal** **Instruction**-following, [[Paper]](https://arxiv.org/pdf/2309.08637.pdf), [[Project]](https://textbind.github.io/)

- (arXiv 2023.9) Prompt a **Robot** to **Walk** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.09969.pdf), [[Project]](https://prompt2walk.github.io/)

- (arXiv 2023.9) Grasp-Anything: Large-scale **Grasp** Dataset from Foundation Models, [[Paper]](https://arxiv.org/pdf/2309.09818.pdf), [[Project]](https://grasp-anything-2023.github.io/)

- (arXiv 2023.9) MMICL: EMPOWERING **VISION-LANGUAGE** MODEL WITH MULTI-MODAL IN-CONTEXT LEARNING, [[Paper]](https://arxiv.org/pdf/2309.07915.pdf), [[Code]](https://github.com/HaozheZhao/MIC)

- (arXiv 2023.9) SwitchGPT: Adapting Large Language Models for **Non-Text Outputs**, [[Paper]](https://arxiv.org/pdf/2309.07623.pdf), [[Code]](https://github.com/xinke-wang/SwitchGPT)

- (arXiv 2023.9) UNIFIED **HUMAN-SCENE INTERACTION** VIA PROMPTED CHAIN-OF-CONTACTS, [[Paper]](https://arxiv.org/pdf/2309.07918.pdf), [[Code]](https://github.com/OpenRobotLab/UniHSI)

- (arXiv 2023.9) Incremental Learning of **Humanoid Robot Behavior** from Natural Interaction and Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.04316.pdf)

- (arXiv 2023.9) NExT-GPT: Any-to-Any **Multimodal** LLM, [[Paper]](https://arxiv.org/pdf/2309.05519.pdf), [[Project]](https://next-gpt.github.io/)

- (arXiv 2023.9) Multi3DRefer: **Grounding** Text Description to Multiple **3D** Objects, [[Paper]](https://arxiv.org/pdf/2309.05251.pdf), [[Project]](https://3dlg-hcvc.github.io/multi3drefer/#/)

- (arXiv 2023.9) Language Models as Black-Box **Optimizers** for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2309.05950.pdf)

- (arXiv 2023.9) Evaluation and Mitigation of Agnosia in **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.04041.pdf)

- (arXiv 2023.9) Measuring and Improving **Chain-of-Thought** Reasoning in Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2309.04461.pdf), [[Code]](https://github.com/Yangyi-Chen/CoTConsistency)

- (arXiv 2023.9) Context-Aware **Prompt Tuning** for Vision-Language Model with Dual-Alignment, [[Paper]](https://arxiv.org/pdf/2309.04158.pdf)

- (arXiv 2023.9) ImageBind-LLM: **Multi-modality Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2309.03905.pdf), [[Code]](https://github.com/OpenGVLab/LLaMA-Adapter)

- (arXiv 2023.9) Developmental **Scaffolding** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.00904.pdf)

- (arXiv 2023.9) Gesture-Informed **Robot Assistance** via Foundation Models, [[Paper]](https://arxiv.org/pdf/2309.02721.pdf), [[Project]](https://sites.google.com/view/giraf23)

- (arXiv 2023.9) Zero-Shot **Recommendations** with Pre-Trained Large Language Models for Multimodal Nudging, [[Paper]](https://arxiv.org/pdf/2309.01026.pdf)

- (arXiv 2023.9) Large AI Model Empowered Multimodal Semantic Communications, [[Paper]](https://arxiv.org/pdf/2309.01249.pdf)

- (arXiv 2023.9) CoTDet: **Affordance** Knowledge Prompting for Task Driven Object **Detection**, [[Paper]](https://arxiv.org/pdf/2309.01093.pdf), [[Project]](https://toneyaya.github.io/cotdet/)

- (arXiv 2023.9) Scaling Autoregressive **Multi-Modal** Models: Pretraining and Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2309.02591.pdf)

- (arXiv 2023.9) CIEM: Contrastive Instruction Evaluation Method for Better **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2309.02301.pdf)

- (arXiv 2023.9) Point-Bind & Point-LLM: Aligning **Point Cloud** with **Multi-modality** for 3D Understanding, Generation, and Instruction Following, [[Paper]](https://arxiv.org/pdf/2309.00615.pdf), [[Code]](https://github.com/ZiyuGuo99/Point-Bind_Point-LLM)

### 2023.8

<!-- - (arXiv 2023.8) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.8) Improving Knowledge Extraction from LLMs for Task Learning through **Agent** Analysis, [[Paper]](https://arxiv.org/pdf/2306.06770.pdf)

- (arXiv 2023.8) Sparkles: Unlocking Chats Across Multiple Images for **Multimodal Instruction**-Following Models, [[Paper]](https://arxiv.org/pdf/2308.16463.pdf), [[Code]](https://github.com/HYPJUDY/Sparkles)

- (arXiv 2023.8) PointLLM: Empowering Large Language Models to Understand **Point Clouds**, [[Paper]](https://arxiv.org/pdf/2308.16911.pdf), [[Project]](https://runsenxu.com/projects/PointLLM/)

- (arXiv 2023.8) TouchStone: **Evaluating Vision-Language** Models by Language Models, [[Paper]](https://arxiv.org/pdf/2308.16890.pdf), [[Code]](https://github.com/OFA-Sys/TouchStone)

- (arXiv 2023.8) WALL-E: Embodied **Robotic** WAiter Load Lifting with Large Language Model, [[Paper]](https://arxiv.org/pdf/2308.15962.pdf)

- (arXiv 2023.8) ISR-LLM: Iterative Self-Refined Large Language Model for **Long-Horizon Sequential Task Planning**, [[Paper]](https://arxiv.org/pdf/2308.13724.pdf), [[Code]](https://github.com/zhehuazhou/ISR-LLM)

- (arXiv 2023.8) LLM-Based **Human-Robot Collaboration** Framework for Manipulation Tasks, [[Paper]](https://arxiv.org/pdf/2308.14972.pdf)

- (arXiv 2023.8) Evaluation and Analysis of **Hallucination** in Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2308.15126.pdf)

- (arXiv 2023.8) MLLM-**DataEngine**: An Iterative Refinement Approach for MLLM, [[Paper]](https://arxiv.org/pdf/2308.13566.pdf)

- (arXiv 2023.8) Position-Enhanced **Visual Instruction** Tuning for Multimodal Large Language Models, [[Paper]](https://arxiv.org/pdf/2308.13437.pdf)

- (arXiv 2023.8) Can Linguistic Knowledge Improve Multimodal Alignment in **Vision-Language** Pretraining? [[Paper]](https://arxiv.org/pdf/2308.12898.pdf), [[Code]](https://github.com/WangFei-2019/SNARE/)

- (arXiv 2023.8) VIGC: **Visual Instruction** Generation and Correction, [[Paper]](https://arxiv.org/pdf/2308.12714.pdf)

- (arXiv 2023.8) Towards Realistic **Zero-Shot** Classification via Self Structural Semantic Alignment, [[Paper]](https://arxiv.org/pdf/2308.12960.pdf)

- (arXiv 2023.8) Qwen-VL: A Frontier Large **Vision-Language** Model with Versatile Abilities, [[Paper]](https://arxiv.org/pdf/2308.12966.pdf), [[Code]](https://github.com/QwenLM/Qwen-VL)

- (arXiv 2023.8) **DIFFUSION** **LANGUAGE MODELS** CAN PERFORM MANY TASKS WITH SCALING AND INSTRUCTION-FINETUNING, [[Paper]](https://arxiv.org/pdf/2308.12219.pdf), [[Code]](https://github.com/yegcjs/DiffusionLLM)

- (arXiv 2023.8) CHORUS: Learning Canonicalized **3D Human-Object** Spatial **Relations** from Unbounded Synthesized Images, [[Paper]](https://arxiv.org/pdf/2308.12288.pdf), [[Project]](https://jellyheadandrew.github.io/projects/chorus)

- (arXiv 2023.8) Pro**Agent**: Building Proactive Cooperative AI with Large Language Models, [[Paper]](https://arxiv.org/pdf/2308.11339.pdf), [[Project]](https://pku-proagent.github.io/)

- (arXiv 2023.8) ROSGPT_Vision: Commanding **Robots** Using Only Language Models’ Prompts, [[Paper]](https://arxiv.org/pdf/2308.11236.pdf), [[Code]](https://github.com/bilel-bj/ROSGPT_Vision)

- (arXiv 2023.8) StoryBench: A Multifaceted Benchmark for Continuous **Story Visualization**, [[Paper]](https://arxiv.org/pdf/2308.11606.pdf), [[Code]](https://github.com/google/storybench)

- (arXiv 2023.8) Tackling Vision Language Tasks Through Learning **Inner Monologues**, [[Paper]](https://arxiv.org/pdf/2308.09970.pdf)

- (arXiv 2023.8) ExpeL: LLM Agents Are **Experiential Learners**, [[Paper]](https://arxiv.org/pdf/2308.10144.pdf)

- (arXiv 2023.8) On the Adversarial **Robustness** of **Multi-Modal** Foundation Models, [[Paper]](https://arxiv.org/pdf/2308.10741.pdf)

- (arXiv 2023.8) WanJuan: A Comprehensive **Multimodal** **Dataset** for Advancing English and Chinese Large Models, [[Paper]](https://arxiv.org/pdf/2308.10755.pdf), [[Project]](https://opendatalab.org.cn/WanJuan1.0)

- (arXiv 2023.8) March in Chat: Interactive **Prompting** for Remote **Embodied** Referring Expression, [[Paper]](https://arxiv.org/pdf/2308.10141.pdf), [[Code]](https://github.com/YanyuanQiao/MiC)

- (arXiv 2023.8) BLIVA: A Simple **Multimodal** LLM for Better Handling of Text-Rich Visual Questions, [[Paper]](https://arxiv.org/pdf/2308.09936.pdf), [[Code]](https://github.com/mlpc-ucsd/BLIVA.git)

- (arXiv 2023.8) VIT-LENS: Towards **Omni-modal** Representations, [[Paper]](https://arxiv.org/pdf/2308.10185.pdf), [[Code]](https://github.com/TencentARC/ViT-Lens)

- (arXiv 2023.8) StableLLaVA: Enhanced **Visual Instruction Tuning** with Synthesized Image-Dialogue Data, [[Paper]](https://arxiv.org/pdf/2308.10253.pdf), [[Project]](https://github.com/icoz69/StableLLAVA)

- (arXiv 2023.8) PUMGPT: A Large Vision-Language Model for **Product Understanding**, [[Paper]](https://arxiv.org/pdf/2308.09568.pdf)

- (arXiv 2023.8) Link-Context Learning for **Multimodal** LLMs, [[Paper]](https://arxiv.org/pdf/2308.07891.pdf), [[Code]](https://github.com/isekai-portal/Link-Context-Learning)

- (arXiv 2023.8) Detecting and Preventing **Hallucinations** in Large Vision Language Models, [[Paper]](https://arxiv.org/pdf/2308.06394.pdf)

- (arXiv 2023.8) VisIT-Bench: A **Benchmark** for **Vision-Language Instruction** Following Inspired by Real-World Use, [[Paper]](https://arxiv.org/pdf/2308.06595.pdf), [[Project]](https://visit-bench.github.io/)

- (arXiv 2023.8) Foundation Model based Open Vocabulary Task Planning and Executive System for General Purpose **Service Robots**, [[Paper]](https://arxiv.org/pdf/2308.03357.pdf)

- (arXiv 2023.8) LayoutLLM-T2I: Eliciting **Layout** Guidance from LLM for **Text-to-Image** Generation, [[Paper]](https://arxiv.org/pdf/2308.05095.pdf), [[Project]](https://layoutllm-t2i.github.io/)

- (arXiv 2023.8) OmniDataComposer: A Unified Data Structure for Multimodal **Data Fusion** and Infinite Data **Generation**, [[Paper]](https://arxiv.org/pdf/2308.04126.pdf)

- (arXiv 2023.8) EMPOWERING VISION-LANGUAGE MODELS TO FOLLOW INTERLEAVED **VISION-LANGUAGE** INSTRUCTIONS, [[Paper]](https://arxiv.org/pdf/2308.04152.pdf), [[Code]](https://github.com/DCDmllm/Cheetah)

- (arXiv 2023.8) 3D-VisTA: Pre-trained Transformer for **3D** Vision and **Text** Alignment, [[Paper]](https://arxiv.org/pdf/2308.04352.pdf), [[Project]](https://3d-vista.github.io/)

- (arXiv 2023.8) Gentopia.AI: A **Collaborative Platform** for **Tool**-Augmented LLMs, [[Paper]](https://arxiv.org/pdf/2308.04030.pdf), [[Project]](https://github.com/Gentopia-AI)

- (arXiv 2023.8) AgentBench: Evaluating LLMs as **Agents**, [[Paper]](https://arxiv.org/pdf/2308.03688.pdf), [[Project]](https://llmbench.ai/)

- (arXiv 2023.8) Learning Concise and Descriptive **Attributes** for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2308.03685.pdf)

- (arXiv 2023.8) Tiny LVLM-eHub: Early Multimodal Experiments with **Bard**, [[Paper]](https://arxiv.org/pdf/2308.03729.pdf), [[Project]](http://lvlm-ehub.opengvlab.com/)

- (arXiv 2023.8) MM-Vet: **Evaluating** Large **Multimodal** Models for Integrated Capabilities, [[Paper]](https://arxiv.org/pdf/2308.02490.pdf), [[Code]](https://github.com/yuweihao/MM-Vet)

- (arXiv 2023.8) RegionBLIP: A Unified **Multi-modal Pre-training** Framework for Holistic and Regional Comprehension, [[Paper]](https://arxiv.org/pdf/2308.02299.pdf), [[Code]](https://github.com/mightyzau/RegionBLIP)

- (arXiv 2023.8) Learning to **Model** the **World** with Language, [[Paper]](https://arxiv.org/pdf/2308.01399.pdf), [[Project]](https://dynalang.github.io/)

- (arXiv 2023.8) The All-Seeing Project: Towards Panoptic **Visual Recognition** and Understanding of the Open World, [[Paper]](https://arxiv.org/pdf/2308.01907.pdf), [[Code]](https://github.com/OpenGVLab/all-seeing)

- (arXiv 2023.8) Multimodal **Neurons** in Pretrained Text-Only Transformers, [[Paper]](https://arxiv.org/pdf/2308.01544.pdf), [[Project]](https://mmns.csail.mit.edu/)

- (arXiv 2023.8) LISA: REASONING **SEGMENTATION** VIA LARGE LANGUAGE MODEL, [[Paper]](https://arxiv.org/pdf/2308.00692.pdf), [[Code]](https://github.com/dvlab-research/LISA)

### 2023.7

<!-- - (arXiv 2023.7) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.7) DesCo: Learning Object **Recognition** with Rich Language Descriptions, [[Paper]](https://arxiv.org/pdf/2306.14060.pdf)

- (arXiv 2023.7) KOSMOS-2: **Grounding** **Multimodal** Large Language Models to the World, [[Paper]](https://arxiv.org/pdf/2306.14824.pdf), [[Project]](https://thegenerality.com/agi/)

- (arXiv 2023.7) MME: A Comprehensive Evaluation **Benchmark** for **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2306.13394.pdf), [[Code]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)

- (arXiv 2023.7) Evaluating ChatGPT and GPT-4 for **Visual Programming**, [[Paper]](https://arxiv.org/pdf/2308.02522.pdf)

- (arXiv 2023.7) SEED-Bench: **Benchmarking** **Multimodal** LLMs with Generative Comprehension, [[Paper]](https://arxiv.org/pdf/2307.16125.pdf), [[Code]](https://github.com/AILab-CVC/SEED-Bench)

- (arXiv 2023.7) AntGPT: Can Large Language Models Help **Long-term Action Anticipation** from Videos? [[Paper]](https://arxiv.org/pdf/2307.16368.pdf), [[Project]](https://brown-palm.github.io/AntGPT/)

- (arXiv 2023.7) Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex **Visual Reasoning** Tasks, [[Paper]](https://arxiv.org/pdf/2307.16395.pdf)

- (arXiv 2023.7) MovieChat: From Dense Token to Sparse Memory for **Long Video Understanding**, [[Paper]](https://arxiv.org/pdf/2307.16449.pdf), [[Project]](https://rese1f.github.io/MovieChat/)

- (arXiv 2023.7) Large Language Models as **General Pattern Machines**, [[Paper]](https://arxiv.org/pdf/2307.04721.pdf), [[Project]](https://general-pattern-machines.github.io/)

- (arXiv 2023.7) How Good is Google **Bard**’s **Visual Understanding**? An Empirical Study on Open Challenges, [[Paper]](https://arxiv.org/pdf/2307.15016.pdf), [[Project]](https://github.com/htqin/GoogleBard-VisUnderstand)

- (arXiv 2023.7) RT-2: **Vision-Language-Action** Models Transfer Web Knowledge to **Robotic Control**, [[Paper]](https://robotics-transformer2.github.io/assets/rt2.pdf), [[Project]](https://robotics-transformer2.github.io/)

- (arXiv 2023.7) Scaling Up and Distilling Down: Language-Guided **Robot Skill Acquisition**, [[Paper]](https://arxiv.org/pdf/2307.14535.pdf), [[Project]](https://www.cs.columbia.edu/~huy/scalingup/)

- (arXiv 2023.7) GraspGPT: Leveraging Semantic Knowledge from a Large Language Model for Task-Oriented **Grasping**, [[Paper]](https://arxiv.org/pdf/2307.13204.pdf), [[Project]](https://sites.google.com/view/graspgpt/)

- (arXiv 2023.7) CARTIER: Cartographic lAnguage Reasoning Targeted at **Instruction Execution** for **Robots**, [[Paper]](https://arxiv.org/pdf/2307.11865.pdf)

- (arXiv 2023.7) **3D-LLM**: Injecting the 3D World into Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.12981.pdf), [[Project]](https://vis-www.cs.umass.edu/3dllm/)

- (arXiv 2023.7) Generative **Pretraining** in **Multimodality**, [[Paper]](https://arxiv.org/pdf/2307.05222.pdf), [[Code]](https://github.com/baaivision/Emu)

- (arXiv 2023.7) VoxPoser: Composable 3D Value Maps for **Robotic Manipulation** with Language Models, [[Paper]](https://arxiv.org/pdf/2307.05973.pdf), [[Project]](https://voxposer.github.io/)

- (arXiv 2023.7) VELMA: Verbalization Embodiment of LLM Agents for Vision and Language **Navigation** in Street View, [[Paper]](https://arxiv.org/pdf/2307.06082.pdf)

- (arXiv 2023.7) SayPlan: Grounding Large Language Models using **3D Scene Graphs** for Scalable **Task Planning**, [[Paper]](https://arxiv.org/pdf/2307.06135.pdf), [[Project]](https://sayplan.github.io/)

- (arXiv 2023.7) Enhancing **CLIP** with GPT-4: Harnessing Visual Descriptions as **Prompts**, [[Paper]](https://arxiv.org/pdf/2307.11661.pdf)

- (arXiv 2023.7) InternVid: A Large-scale **Video-Text Dataset** for Multimodal Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2307.06942.pdf), [[Data]](https://arxiv.org/pdf/2307.06942.pdf)

- (arXiv 2023.7) MBLIP: EFFICIENT BOOTSTRAPPING OF **MULTILINGUAL VISION-LLMS**, [[Paper]](https://arxiv.org/pdf/2307.06930.pdf), [[Code]](https://github.com/gregor-ge/mBLIP)

- (arXiv 2023.7) Bootstrapping **Vision-Language** Learning with Decoupled Language Pre-training, [[Paper]](https://arxiv.org/pdf/2307.07063.pdf)

- (arXiv 2023.7) BuboGPT: Enabling **Visual Grounding** in Multi-Modal LLMs, [[Paper]](https://arxiv.org/pdf/2307.08581.pdf), [[Project]](https://bubo-gpt.github.io/)

- (arXiv 2023.7) ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2307.09474.pdf), [[Project]](https://chatspot.streamlit.app/)

- (arXiv 2023.7) TOWARDS A **UNIFIED AGENT** WITH FOUNDATION MODELS, [[Paper]](https://arxiv.org/pdf/2307.09668.pdf)

- (arXiv 2023.7) Robots That Ask For Help: **Uncertainty** Alignment for Large Language Model **Planners**, [[Paper]](https://arxiv.org/pdf/2307.01928.pdf), [[Project]](https://robot-help.github.io/)

- (arXiv 2023.7) Building Cooperative **Embodied Agents** Modularly with Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.02485.pdf), [[Project]](https://vis-www.cs.umass.edu/Co-LLM-Agents/)

- (arXiv 2023.7) **Embodied Task Planning** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.01848.pdf), [[Project]](https://gary3410.github.io/TaPA/)

- (arXiv 2023.7) What Matters in **Training** a GPT4-Style Language Model with **Multimodal** Inputs?, [[Paper]](https://arxiv.org/pdf/2307.02469.pdf), [[Project]](https://lynx-llm.github.io/)

- (arXiv 2023.7) GPT4RoI: **Instruction Tuning** Large Language Model on **Region-of-Interest**, [[Paper]](https://arxiv.org/pdf/2307.03601.pdf), [[Code]](https://github.com/jshilong/GPT4RoI)

- (arXiv 2023.7) JourneyDB: A Benchmark for **Generative Image Understanding**, [[Paper]](https://arxiv.org/pdf/2307.00716.pdf), [[Code]](https://journeydb.github.io/)

- (arXiv 2023.7) DoReMi: Grounding Language Model by Detecting and Recovering from **Plan-Execution Misalignment**, [[Paper]](https://arxiv.org/pdf/2307.00329.pdf), [[Project]](https://sites.google.com/view/doremi-paper)
  
- (arXiv 2023.7) Motion-X: A Large-scale 3D Expressive Whole-body Human **Motion Dataset**, [[Paper]](https://arxiv.org/pdf/2307.00818.pdf), [[Code]](https://github.com/IDEA-Research/Motion-X)

- (arXiv 2023.7) Visual **Instruction Tuning** with Polite Flamingo, [[Paper]](https://arxiv.org/pdf/2307.01003.pdf), [[Code]](https://github.com/ChenDelong1999/polite_flamingo)

- (arXiv 2023.7) Statler: State-Maintaining Language Models for **Embodied Reasoning**, [[Paper]](https://arxiv.org/pdf/2306.17840.pdf), [[Project]](https://statler-lm.github.io/)

- (arXiv 2023.7) SCITUNE: Aligning Large Language Models with **Scientific Multimodal Instructions**, [[Paper]](https://arxiv.org/pdf/2307.01139.pdf)

- (arXiv 2023.7) SPAE: Semantic **Pyramid** AutoEncoder for **Multimodal Generation** with Frozen LLMs, [[Paper]](https://arxiv.org/pdf/2306.17842.pdf), [[Code]](https://github.com/google-research/magvit/)

- (arXiv 2023.7) KITE: Keypoint-Conditioned **Policies** for **Semantic Manipulation**, [[Paper]](https://arxiv.org/pdf/2306.16605.pdf), [[Project]](https://sites.google.com/view/kite-website/home)

### 2023.6

<!-- - (arXiv 2023.6) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.6) LAMM: Language-Assisted **Multi-Modal Instruction-Tuning** Dataset, Framework, and Benchmark, [[Paper]](https://arxiv.org/pdf/2306.06687.pdf), [[Code]](https://github.com/OpenLAMM/LAMM)

- (arXiv 2023.6) Scalable **3D Captioning** with Pretrained Models, [[Paper]](https://arxiv.org/pdf/2306.07279.pdf), [[Code]](https://huggingface.co/datasets/tiange/Cap3D)

- (arXiv 2023.6) AutoTAMP: Autoregressive Task and Motion **Planning** with LLMs as Translators and Checkers, [[Paper]](https://arxiv.org/pdf/2306.06531.pdf), [[Code]](https://github.com/yongchao98/AutoTAMP)

- (arXiv 2023.6) VALLEY: **VIDEO ASSISTANT** WITH LARGE LANGUAGE MODEL ENHANCED ABILITY, [[Paper]](https://arxiv.org/pdf/2306.07207.pdf), [[Code]](https://github.com/RupertLuo/Valley)

- (arXiv 2023.6) Pave the Way to Grasp Anything: Transferring Foundation Models for Universal **Pick-Place Robots**, [[Paper]](https://arxiv.org/pdf/2306.05716.pdf)

- (arXiv 2023.6) LVLM-eHub: A Comprehensive Evaluation **Benchmark** for Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2306.09265.pdf)

- (arXiv 2023.6) AssistGPT: A General **Multi-modal Assistant** that can Plan, Execute, Inspect, and Learn, [[Paper]](https://arxiv.org/pdf/2306.08640.pdf), [[Project]](https://showlab.github.io/assistgpt/)

- (arXiv 2023.6) Towards AGI in Computer Vision: **Lessons** Learned from GPT and Large Language Models, [[Paper]](https://arxiv.org/pdf/2306.08641.pdf)

- (arXiv 2023.6) MACAW-LLM: **MULTI-MODAL** LANGUAGE MODELING WITH IMAGE, AUDIO, VIDEO, AND TEXT INTEGRATION, [[Paper]](https://arxiv.org/pdf/2306.09093.pdf), [[Code]](https://github.com/lyuchenyang/Macaw-LLM)

- (arXiv 2023.6) Investigating Prompting Techniques for Zero- and Few-Shot **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2306.09996.pdf)

- (arXiv 2023.6) Language to Rewards for **Robotic** Skill Synthesis, [[Paper]](https://arxiv.org/pdf/2306.08647.pdf), [[Project]](https://language-to-reward.github.io/)

- (arXiv 2023.6) Toward Grounded **Social** **Reasoning**, [[Paper]](https://arxiv.org/pdf/2306.08651.pdf), [[Code]](https://minaek.github.io/groundedsocialreasoning/)

- (arXiv 2023.6) Improving Image **Captioning** Descriptiveness by Ranking and LLM-based Fusion, [[Paper]](https://arxiv.org/pdf/2306.11593.pdf), [[Code]](https://drive.google.com/drive/folders/1tXbJyvwrEF-a-t-NZbt2xjoKaiGGeaz1?usp=sharing)

- (arXiv 2023.6) RM-PRT: Realistic **Robotic** Manipulation Simulator and **Benchmark** with Progressive Reasoning Tasks, [[Paper]](https://arxiv.org/pdf/2306.11335.pdf), [[Code]](https://necolizer.github.io/RM-PRT/)

- (arXiv 2023.6) Mitigating Hallucination in Large Multi-Modal Models via Robust **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2306.14565.pdf), [[Project]](https://fuxiaoliu.github.io/LRV/)

- (arXiv 2023.6) Towards Language Models That Can See: Computer **Vision** Through the LENS of Natural **Language**, [[Paper]](https://arxiv.org/pdf/2306.16410.pdf), [[Code]](https://arxiv.org/pdf/2306.16410.pdf)

- (arXiv 2023.6) LLaVAR: Enhanced **Visual Instruction Tuning** for Text-Rich Image Understanding, [[Paper]](https://arxiv.org/pdf/2306.17107.pdf), [[Project]](https://llavar.github.io/)

- (arXiv 2023.6) OpenShape: Scaling Up **3D Shape Representation** Towards **Open-World** Understanding, [[Paper]](https://arxiv.org/pdf/2305.10764.pdf), [[Project]](https://colin97.github.io/OpenShape/)

- (arXiv 2023.6) Statler: State-Maintaining Language Models for **Embodied Reasoning**, [[Paper]](https://arxiv.org/pdf/2306.17840.pdf), [[Project]](https://statler-lm.github.io/)

- (arXiv 2023.6) CLARA: Classifying and Disambiguating User Commands for Reliable **Interactive Robotic** Agents, [[Paper]](https://arxiv.org/pdf/2306.10376.pdf)

- (arXiv 2023.6) **Mass-Producing Failures** of **Multimodal** Systems with Language Models, [[Paper]](https://arxiv.org/pdf/2306.12105.pdf), [[Code]](https://github.com/tsb0601/MultiMon)

- (arXiv 2023.6) SoftGPT: Learn Goal-oriented **Soft Object Manipulation** Skills by Generative Pre-trained Heterogeneous Graph Transformer, [[Paper]](https://arxiv.org/pdf/2306.12677.pdf)

- (arXiv 2023.6) SPRINT: SCALABLE **POLICY PRE-TRAINING** VIA LANGUAGE INSTRUCTION RELABELING, [[Paper]](https://arxiv.org/pdf/2306.11886.pdf), [[Project]](https://clvrai.github.io/sprint/)

- (arXiv 2023.6) MotionGPT: Finetuned LLMs are General-Purpose **Motion Generators**, [[Paper]](https://arxiv.org/pdf/2306.10900.pdf), [[Project]](https://qiqiapink.github.io/MotionGPT/)

- (arXiv 2023.6) MIMIC-IT: **Multi-Modal** **In-Context** Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2306.05425.pdf), [[Code]](https://github.com/Luodian/Otter)

- (arXiv 2023.6) Dense and Aligned **Captions** (DAC) Promote **Compositional Reasoning** in VL Models, [[Paper]](https://arxiv.org/pdf/2305.19595.pdf)

### 2023.5

<!-- - (arXiv 2023.5) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.5) Prompting Large Language Models with Answer Heuristics for Knowledge-based **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2303.01903.pdf), [[Code]](https://github.com/MILVLG/prophet)

- (arXiv 2023.5) VIMA: General **Robot Manipulation** with Multimodal Prompts, [[Paper]](https://arxiv.org/pdf/2210.03094.pdf), [[Project]](https://vimalabs.github.io/)

- (arXiv 2023.5) TidyBot: Personalized **Robot Assistance** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2305.05658.pdf), [[Project]](https://tidybot.cs.princeton.edu/)

- (arXiv 2023.5) Training **Diffusion** Models with **Reinforcement Learning**, [[Paper]](https://arxiv.org/pdf/2305.13301.pdf), [[Project]](https://rl-diffusion.github.io/)

- (arXiv 2023.5) EmbodiedGPT: Vision-Language Pre-Training via **Embodied** Chain of Thought, [[Paper]](https://arxiv.org/pdf/2305.15021.pdf), [[Project]](https://embodiedgpt.github.io/)

- (arXiv 2023.5) ArtGPT-4: **Artistic Vision-Language** Understanding with Adapter-enhanced MiniGPT-4, [[Paper]](https://arxiv.org/pdf/2305.07490.pdf), [[Code]](https://huggingface.co/Tyrannosaurus/ArtGPT-4)

- (arXiv 2023.5) Evaluating **Object Hallucination** in Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2305.10355.pdf), [[Code]](https://github.com/RUCAIBox/POPE)

- (arXiv 2023.5) LLMScore: Unveiling the Power of Large Language Models in **Text-to-Image Synthesis Evaluation**, [[Paper]](https://arxiv.org/pdf/2305.11116.pdf), [[Code]](https://github.com/YujieLu10/LLMScore)

- (arXiv 2023.5) VisionLLM: Large Language Model is also an Open-Ended **Decoder** for **Vision**-Centric Tasks, [[Paper]](https://arxiv.org/pdf/2305.11175.pdf), [[Code]](https://github.com/OpenGVLab/VisionLLM)

- (arXiv 2023.5) OpenShape: Scaling Up **3D Shape Representation** Towards **Open-World** Understanding, [[Paper]](https://arxiv.org/pdf/2305.10764.pdf), [[Project]](https://colin97.github.io/OpenShape/)

- (arXiv 2023.5) Towards A Foundation Model for Generalist **Robots**: Diverse **Skill Learning** at Scale via Automated Task and Scene Generation, [[Paper]](https://arxiv.org/pdf/2305.10455.pdf)

- (arXiv 2023.5) An Android **Robot Head** as Embodied **Conversational** Agent, [[Paper]](https://arxiv.org/pdf/2305.10945.pdf)

- (arXiv 2023.5) Instruct2Act: Mapping Multi-modality Instructions to **Robotic Actions** with Large Language Model, [[Paper]](https://arxiv.org/pdf/2305.11176.pdf), [[Code]](https://github.com/OpenGVLab/Instruct2Act)

- (arXiv 2023.5) **Principle**-Driven Self-Alignment of Language Models from Scratch with **Minimal Human Supervision**, [[Paper]](https://arxiv.org/pdf/2305.03047.pdf), [[Project]](https://mitibmdemos.draco.res.ibm.com/dromedary)

- (arXiv 2023.5) Multimodal **Procedural Planning** via Dual Text-Image Prompting, [[Paper]](https://arxiv.org/pdf/2305.01795.pdf), [[Code]](https://github.com/YujieLu10/TIP)

- (arXiv 2023.5) ArK: **Augmented Reality** with **Knowledge** Interactive Emergent Ability, [[Paper]](https://arxiv.org/pdf/2305.00970.pdf)

### 2023.4

<!-- - (arXiv 2023.4) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.4) LLaMA-Adapter V2: Parameter-**Efficient** **Visual Instruction** Model, [[Paper]](https://arxiv.org/pdf/2304.15010.pdf), [[Code]](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- (arXiv 2023.4) Multimodal Grounding for **Embodied AI** via **Augmented Reality** Headsets for Natural Language Driven Task Planning, [[Paper]](https://arxiv.org/pdf/2304.13676.pdf)

- (arXiv 2023.4) mPLUG-Owl : **Modularization** Empowers Large Language Models with **Multimodality**, [[Paper]](https://arxiv.org/pdf/2304.14178.pdf), [[Code]](https://github.com/X-PLUG/mPLUG-Owl)

- (arXiv 2023.4) ChatVideo: A Tracklet-centric Multimodal and Versatile **Video Understanding** System, [[Paper]](https://arxiv.org/pdf/2304.14407.pdf), [[Project]](https://www.wangjunke.info/ChatVideo/)

- (arXiv 2023.4) ChatABL: **Abductive Learning** via Natural Language Interaction with ChatGPT, [[Paper]](https://arxiv.org/pdf/2304.11107.pdf)

- (arXiv 2023.4) **Robot**-Enabled Construction **Assembly** with Automated Sequence Planning based on ChatGPT: RoboGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.11018.pdf)

- (arXiv 2023.4) Graph-ToolFormer: To Empower LLMs with **Graph Reasoning** Ability via Prompt Augmented by ChatGPT, [[Paper]](https://arxiv.org/pdf/2304.11116.pdf), [[Code]](https://github.com/jwzhanggy/Graph_Toolformer)

- (arXiv 2023.4) Can GPT-4 Perform **Neural Architecture Search**?, [[Paper]](https://arxiv.org/pdf/2304.10970.pdf), [[Code]](https://github.com/mingkai-zheng/GENIUS)

- (arXiv 2023.4) MiniGPT-4: Enhancing **Vision-Language** Understanding with Advanced Large Language Models, [[Paper]](https://arxiv.org/pdf/2304.10592.pdf), [[Project]](https://minigpt-4.github.io/)

- (arXiv 2023.4) SINC: Spatial Composition of **3D** Human **Motions** for Simultaneous Action Generation, [[Paper]](https://arxiv.org/pdf/2304.10417.pdf), [[Project]](https://sinc.is.tue.mpg.de/)

- (arXiv 2023.4) LLM as A **Robotic Brain**: Unifying **Egocentric** Memory and Control, [[Paper]](https://arxiv.org/pdf/2304.09349.pdf)

- (arXiv 2023.4) Chameleon: **Plug-and-Play** Compositional **Reasoning** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2304.09842.pdf), [[Project]](https://chameleon-llm.github.io/)

- (arXiv 2023.4) Visual **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2304.08485.pdf), [[Project]](https://llava-vl.github.io/)

- (arXiv 2023.4) MiniGPT-4: Enhancing **Vision-language** Understanding with Advanced Large Language Models, [[Paper]](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf), [[Project]](https://minigpt-4.github.io/)

- (arXiv 2023.4) RAFT: **Reward** rAnked FineTuning for **Generative** Foundation Model Alignment, [[Paper]](https://arxiv.org/pdf/2304.06767.pdf), [[Code]]()

- (arXiv 2023.4) **Multimodal** C4: An Open, Billion-scale Corpus of Images Interleaved With Text, [[Paper]](https://arxiv.org/pdf/2304.06939.pdf), [[Code]](https://github.com/allenai/mmc4)

- (arXiv 2023.4) ViewRefer: Grasp the Multi-view Knowledge for **3D Visual Grounding** with GPT and Prototype Guidance, [[Paper]](https://arxiv.org/pdf/2303.16894.pdf), [[Code]](https://github.com/ZiyuGuo99/ViewRefer3D)

- (arXiv 2023.4) **HuggingGPT**: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, [[Paper]](https://arxiv.org/pdf/2303.17580.pdf), [[Code]](https://github.com/microsoft/JARVIS)

- (arXiv 2023.4) ERRA: An **Embodied** Representation and Reasoning Architecture for **Long-horizon** Language-conditioned **Manipulation** Tasks, [[Paper]](https://arxiv.org/pdf/2304.02251.pdf), [[Code]](https://robotll.github.io/ERRA/)

- (arXiv 2023.4) Advancing **Medical Imaging** with Language Models: A Journey from N-grams to ChatGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.04920.pdf)

- (arXiv 2023.4) ChatGPT Empowered Long-Step **Robot Control** in Various Environments: A Case Application, [[Paper]](https://arxiv.org/pdf/2304.03893.pdf), [[Code]](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts)

- (arXiv 2023.4) OpenAGI: When LLM Meets Domain **Experts**, [[Paper]](https://arxiv.org/pdf/2304.04370.pdf), [[Code]](https://github.com/agiresearch/OpenAGI)

- (arXiv 2023.4) **Video** ChatCaptioner: Towards the Enriched Spatiotemporal **Descriptions**, [[Paper]](https://arxiv.org/pdf/2304.04227.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

### 2023.3

<!-- - (arXiv 2023.3) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.3) Open-World **Object Manipulation** using Pre-Trained Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2303.00905.pdf), [[Project]](https://robot-moo.github.io/)

- (arXiv 2023.3) Grounded Decoding: Guiding Text Generation with Grounded Models for **Robot Control**, [[Paper]](https://arxiv.org/pdf/2303.00855.pdf), [[Project]](https://grounded-decoding.github.io/) 

- (arXiv 2023.3) Task and Motion **Planning** with Large Language Models for **Object Rearrangement**, [[Paper]](https://arxiv.org/pdf/2303.06247.pdf), [[Project]](https://sites.google.com/view/llm-grop)

- (arXiv 2023.3) RE-MOVE: An Adaptive **Policy Design** Approach for Dynamic Environments via Language-Based Feedback, [[Paper]](https://arxiv.org/pdf/2303.07622.pdf), [[Project]](http://gamma.umd.edu/remove/)

- (arXiv 2023.3) Chat with the Environment: **Interactive Multimodal Perception** using Large Language Models, [[Paper]](https://arxiv.org/pdf/2303.08268.pdf)

- (arXiv 2023.3) MAtch, eXpand and Improve: Unsupervised Finetuning for **Zero-Shot Action Recognition** with Language Knowledge, [[Paper]](https://arxiv.org/pdf/2303.08914.pdf), [[Code]](https://github.com/wlin-at/MAXI)

- (arXiv 2023.3) DialogPaint: A Dialog-based **Image Editing** Model, [[Paper]](https://arxiv.org/pdf/2303.10073.pdf)

- (arXiv 2023.3) MM-REACT : Prompting ChatGPT for **Multimodal Reasoning** and **Action**, [[Paper]](https://arxiv.org/pdf/2303.11381.pdf), [[Project]](https://multimodal-react.github.io/)

- (arXiv 2023.3) eP-ALM: Efficient Perceptual **Augmentation** of Language Models, [[Paper]](https://arxiv.org/pdf/2303.11403.pdf), [[Code]](https://github.com/mshukor/eP-ALM)

- (arXiv 2023.3) Errors are Useful Prompts: Instruction Guided **Task Programming** with Verifier-Assisted Iterative Prompting, [[Paper]](https://arxiv.org/pdf/2303.14100.pdf), [[Project]](https://ac-rad.github.io/clairify/)

- (arXiv 2023.3) **LLaMA**-**Adapter**: Efficient Fine-tuning of Language Models with Zero-init Attention, [[Paper]](https://arxiv.org/pdf/2303.16199.pdf), [[Code]](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- (arXiv 2023.3) **MULTIMODAL** ANALOGICAL **REASONING** OVER KNOWLEDGE GRAPHS, [[Paper]](https://arxiv.org/pdf/2210.00312.pdf), [[Code]](https://github.com/zjunlp/MKG_Analogy)

- (arXiv 2023.3) CAN LARGE LANGUAGE MODELS **DESIGN** A **ROBOT**? [[Paper]](https://arxiv.org/pdf/2303.15324.pdf)

- (arXiv 2023.3) Learning **video** embedding space with Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2303.14584.pdf)

- (arXiv 2023.3) Audio Visual Language Maps for Robot **Navigation**, [[Paper]](https://arxiv.org/pdf/2303.07522.pdf), [[Project]](https://avlmaps.github.io/)

- (arXiv 2023.3) ViperGPT: Visual Inference via **Python** Execution for **Reasoning**, [[Paper]](https://arxiv.org/pdf/2303.08128.pdf)

- (arXiv 2023.3) **ChatGPT** Asks, **BLIP-2** Answers: Automatic Questioning Towards Enriched **Visual Descriptions**, [[Paper]](https://arxiv.org/pdf/2303.06594.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

- (arXiv 2023.3) Can an **Embodied** Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation, [[Paper]](https://arxiv.org/pdf/2303.03480.pdf), [[Project]](https://gamma.umd.edu/LGX/)

- (arXiv 2023.3) Visual **ChatGPT**: Talking, Drawing and Editing with Visual Foundation Models, [[Paper]](https://arxiv.org/abs/2303.04671), [[Code]](https://github.com/microsoft/visual-chatgpt)

- (arXiv 2023.3) PaLM-E: An **Embodied** Multimodal Language Model, [[Paper]](https://palm-e.github.io/assets/palm-e.pdf), [[Project]](https://palm-e.github.io/)

- (arXiv 2023.3) Language Is Not All You Need: **Aligning** Perception with Language Models, [[Paper]](https://arxiv.org/pdf/2302.14045.pdf), [[Code]](https://github.com/microsoft/unilm)

### 2023.2

- (arXiv 2023.2) ChatGPT for **Robotics**: Design Principles and Model Abilities, , [[Paper]](https://arxiv.org/pdf/2306.17582.pdf), [[Code]](https://github.com/microsoft/PromptCraft-Robotics)

- (arXiv 2023.2) Internet Explorer: Targeted Representation Learning on the Open Web, [[Paper]](https://arxiv.org/pdf/2302.14051.pdf), [[Project]](https://internet-explorer-ssl.github.io/)

### 2022.11

- (arXiv 2022.11) Visual Programming: Compositional **visual reasoning** without training, [[Paper]](https://arxiv.org/pdf/2211.11559.pdf), [[Project]](https://prior.allenai.org/projects/visprog)

### 2022.7

- (arXiv 2022.7) Language Models are General-Purpose **Interfaces**, [[Paper]](https://arxiv.org/pdf/2206.06336.pdf), [[Code]](https://github.com/microsoft/unilm)

- (arXiv 2022.7) LM-Nav: Robotic **Navigation** with Large Pre-Trained Models of Language, Vision, and Action, [[Paper]](https://arxiv.org/pdf/2207.04429.pdf), [[Project]](https://sites.google.com/view/lmnav)
