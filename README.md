# LLM-in-Vision
Recent LLM (Large Language Models)-based CV and multi-modal works. Welcome to comment/contribute!

### 2024.4

<!-- - (arXiv 2024.4) , [[Paper]](), [[Project]]()-->

- (arXiv 2024.4) Exploring the Potential of Large Foundation Models for Open-Vocabulary **HOI Detection**, [[Paper]](https://arxiv.org/pdf/2404.06194.pdf), [[Project]](https://github.com/ltttpku/CMD-SE-release)

### 2024.3

<!-- - (arXiv 2024.3) , [[Paper]](), [[Project]]()-->

- (arXiv 2024.3) LEGO: Learning **EGOcentric Action Frame Generation** via Visual Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2312.03849.pdf), [[Project]](https://bolinlai.github.io/Lego_EgoActGen/)

- (arXiv 2024.3) RAIL: **Robot** **Affordance** Imagination with Large Language Models, [[Paper]](https://arxiv.org/pdf/2403.19369.pdf)

- (arXiv 2024.3) Are We on the Right Way for Evaluating Large Vision-Language Models? [[Paper]](https://arxiv.org/pdf/2403.20330.pdf), [[Project]](https://mmstar-benchmark.github.io/)

- (arXiv 2024.3) FSMR: A Feature Swapping **Multi-modal Reasoning** Approach with Joint Textual and Visual Clues, [[Paper]](https://arxiv.org/pdf/2403.20026.pdf), [[Project]](https://github.com/THU-BPM/FSMR)

- (arXiv 2024.3) Unsolvable Problem Detection: Evaluating **Trustworthiness** of Vision Language Models, [[Paper]](https://arxiv.org/pdf/2403.20331.pdf), [[Project]](https://github.com/AtsuMiyai/UPD/)

- (arXiv 2024.3) OAKINK2 : A Dataset of Bimanual **Hands-Object Manipulation** in Complex Task Completion, [[Paper]](https://arxiv.org/pdf/2403.19417.pdf), [[Project]](https://oakink.net/v2)

- (arXiv 2024.3) InterDreamer: Zero-Shot Text to 3D Dynamic **Human-Object Interaction**, [[Paper]](https://arxiv.org/pdf/2403.19652.pdf), [[Project]](https://sirui-xu.github.io/InterDreamer/)

- (arXiv 2024.3) MM1: Methods, Analysis & Insights from Multimodal LLM **Pre-training**, [[Paper]](https://arxiv.org/pdf/2403.09611.pdf)

- (arXiv 2024.3) Mini-Gemini: Mining the Potential of Multi-modality **Vision Language** Models, [[Paper]](https://arxiv.org/pdf/2403.18814.pdf), [[Project]](https://github.com/dvlab-research/MiniGemini)

- (arXiv 2024.3) INSIGHT: End-to-End **Neuro-Symbolic** Visual Reinforcement Learning with Language Explanations, [[Paper]](https://arxiv.org/pdf/2403.12451.pdf)

- (arXiv 2024.3) DetToolChain: A New **Prompting** Paradigm to Unleash Detection Ability of MLLM, [[Paper]](https://arxiv.org/pdf/2403.12488.pdf)

- (arXiv 2024.3) Embodied LLM **Agents** Learn to Cooperate in Organized Teams, [[Paper]](https://arxiv.org/pdf/2403.12482.pdf)

- (arXiv 2024.3) To Help or Not to Help: LLM-based Attentive Support for **Human-Robot** Group **Interactions**, [[Paper]](https://arxiv.org/pdf/2403.12533.pdf), [[Project]](https://hri-eu.github.io/AttentiveSupport/)

- (arXiv 2024.3) BTGenBot: Behavior Tree Generation for **Robotic** Tasks with Lightweight LLMs, [[Paper]](https://arxiv.org/pdf/2403.12761.pdf)

- (arXiv 2024.3) Chain-of-Spot: Interactive Reasoning Improves Large **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2403.12966.pdf), [[Project]](https://sites.google.com/view/chain-of-spot/)

- (arXiv 2024.3) HYDRA: A Hyper Agent for Dynamic Compositional **Visual Reasoning**, [[Paper]](https://arxiv.org/pdf/2403.12884.pdf), [[Project]](https://hydra-vl4ai.github.io/)

- (arXiv 2024.3) RelationVLM: Making Large Vision-Language Models Understand **Visual Relations**, [[Paper]](https://arxiv.org/pdf/2403.12801.pdf)

- (arXiv 2024.3) Towards Multimodal **In-Context Learning** for Vision & Language Models, [[Paper]](https://arxiv.org/pdf/2403.12736.pdf)

- (arXiv 2024.3) Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for **Jailbreaking** **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2403.09792.pdf)

- (arXiv 2024.3) HawkEye: Training Video-Text LLMs for **Grounding** Text in **Videos**, [[Paper]](https://arxiv.org/pdf/2403.10228.pdf), [[Project]](https://github.com/yellow-binary-tree/HawkEye)

- (arXiv 2024.3) UniCode: Learning a Unified **Codebook** for **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2403.09072.pdf)

- (arXiv 2024.3) Lumen: Unleashing Versatile **Vision-Centric** Capabilities of Large Multimodal Models, [[Paper]](https://arxiv.org/pdf/2403.07304.pdf), [[Project]](https://github.com/SxJyJay/Lumen)

- (arXiv 2024.3) MoAI: Mixture of All Intelligence for Large **Language and Vision** Models, [[Paper]](https://arxiv.org/pdf/2403.07508.pdf), [[Project]](https://github.com/ByungKwanLee/MoAI)

- (arXiv 2024.3) **Multi-modal** Auto-regressive Modeling via Visual Words, [[Paper]](https://arxiv.org/pdf/2403.07720.pdf), [[Project]](https://github.com/pengts/VW-LMM)

- (arXiv 2024.3) DeepSeek-VL: Towards Real-World **Vision-Language** Understanding, [[Paper]](https://arxiv.org/pdf/2403.05525.pdf), [[Project]](https://github.com/deepseek-ai/DeepSeek-VL)

- (arXiv 2024.3) WILL GPT-4 RUN **DOOM**?, [[Paper]](https://arxiv.org/pdf/2403.05468.pdf), [[Project]](https://adewynter.github.io/Doom)

- (arXiv 2024.3) **Debiasing** Large Visual Language Models, [[Paper]](https://arxiv.org/pdf/2403.05262.pdf), [[Project]](https://github.com/yfzhang114/LLaVA-Align)


### 2024.2

<!-- - (arXiv 2024.2) , [[Paper]](), [[Project]]()-->

- (arXiv 2024.2) MOSAIC: A Modular System for **Assistive** and Interactive Cooking, [[Paper]](https://arxiv.org/pdf/2402.18796.pdf), [[Project]](https://portal-cornell.github.io/MOSAIC/)

- (arXiv 2024.2) Visual **Hallucinations** of **Multi-modal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2402.14683.pdf), [[Project]](https://github.com/wenhuang2000/VHTest)

- (arXiv 2024.2) DualFocus: Integrating Macro and Micro Perspectives in **Multi-modal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2402.14767.pdf), [[Project]](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/DualFocus)

- (arXiv 2024.2) RoboScript: Code Generation for Free-Form **Manipulation** Tasks across Real and Simulation, [[Paper]](https://arxiv.org/pdf/2402.14623.pdf)

- (arXiv 2024.2) TinyLLaVA: A Framework of **Small**-scale Large **Multimodal** Models, [[Paper]](https://arxiv.org/pdf/2402.14289.pdf), [[Project]](https://github.com/DLCV-BUAA/TinyLLaVABench)

- (arXiv 2024.2) Enhancing **Robotic** Manipulation with AI Feedback from Multimodal Large Language Models, [[Paper]](https://arxiv.org/pdf/2402.14245.pdf)

- (arXiv 2024.2) Uncertainty-Aware **Evaluation** for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2402.14418.pdf), [[Project]](https://github.com/EnSec-AI/VLM-Uncertainty-Bench)

- (arXiv 2024.2) RealDex: Towards Human-like **Grasping** for Robotic **Dexterous Hand**, [[Paper]](https://arxiv.org/pdf/2402.13853.pdf)

- (arXiv 2024.2) Aligning **Modalities** in Vision Large Language Models via Preference Fine-tuning, [[Paper]](https://arxiv.org/pdf/2402.11411.pdf), [[Project]](https://github.com/YiyangZhou/POVID)

- (arXiv 2024.2) Open3DSG: Open-Vocabulary **3D Scene Graphs** from Point Clouds with Queryable Objects and Open-Set Relationships, [[Paper]](https://arxiv.org/pdf/2402.12259.pdf)

- (arXiv 2024.2) ChartX & ChartVLM: A Versatile **Benchmark** and Foundation Model for Complicated **Chart Reasoning**, [[Paper]](https://arxiv.org/pdf/2402.12185.pdf), [[Project]](https://github.com/UniModal4Reasoning/ChartVLM)

- (arXiv 2024.2) LVCHAT: Facilitating **Long Video** Comprehension, [[Paper]](https://arxiv.org/pdf/2402.12079.pdf), [[Project]](https://github.com/wangyu-ustc/LVChat)

- (arXiv 2024.2) Scaffolding Coordinates to Promote **Vision-Language** Coordination in Large Multi-Modal Models, [[Paper]](https://arxiv.org/pdf/2402.12058.pdf), [[Project]](https://github.com/leixy20/Scaffold)

- (arXiv 2024.2) Using Left and Right Brains Together: Towards Vision and Language **Planning**, [[Paper]](https://arxiv.org/pdf/2402.10534.pdf)

- (arXiv 2024.2) Question-Instructed Visual Descriptions for Zero-Shot **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2402.10698.pdf)

- (arXiv 2024.2) PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong **Vision-language** Adapter, [[Paper]](https://arxiv.org/pdf/2402.10896.pdf)

- (arXiv 2024.2) Grounding LLMs For **Robot** Task Planning Using Closed-loop State Feedback, [[Paper]](https://arxiv.org/pdf/2402.08546.pdf)

- (arXiv 2024.2) BBSEA: An Exploration of Brain-Body Synchronization for **Embodied** Agents, [[Paper]](https://arxiv.org/pdf/2402.08212.pdf), [[Project]](https://bbsea-embodied-ai.github.io/)

- (arXiv 2024.2) Reasoning **Grasping** via Multimodal Large Language Model, [[Paper]](https://arxiv.org/pdf/2402.06798.pdf)

- (arXiv 2024.2) LOTA-BENCH: BENCHMARKING LANGUAGE-ORIENTED TASK PLANNERS FOR **EMBODIED** AGENTS, [[Paper]](https://arxiv.org/pdf/2402.08178.pdf), [[Project]](https://github.com/lbaa2022/LLMTaskPlanning)

- (arXiv 2024.2) OS-COPILOT: TOWARDS GENERALIST COMPUTER **AGENTS** WITH SELF-IMPROVEMENT, [[Paper]](https://arxiv.org/pdf/2402.07456.pdf), [[Project]](https://os-copilot.github.io/)

- (arXiv 2024.2) Doing Experiments and Revising Rules with Natural Language and Probabilistic **Reasoning**, [[Paper]](https://arxiv.org/pdf/2402.06025.pdf)

- (arXiv 2024.2) Preference-Conditioned Language-Guided **Abstraction**, [[Paper]](https://arxiv.org/pdf/2402.03081.pdf)

- (arXiv 2024.2) Affordable Generative **Agents**, [[Paper]](https://arxiv.org/pdf/2402.02053.pdf), [[Project]](https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents)

- (arXiv 2024.2) An Interactive **Agent** Foundation Model, [[Paper]](https://arxiv.org/pdf/2402.05929.pdf)

- (arXiv 2024.2) InCoRo: In-Context Learning for **Robotics Control** with Feedback Loops, [[Paper]](https://arxiv.org/pdf/2402.05188.pdf)

- (arXiv 2024.2) Real-World **Robot** Applications of Foundation Models: A **Review**, [[Paper]](https://arxiv.org/pdf/2402.05741.pdf)

- (arXiv 2024.2) Question Aware Vision Transformer for **Multimodal** Reasoning, [[Paper]](https://arxiv.org/pdf/2402.05472.pdf)

- (arXiv 2024.2) SPHINX-X: Scaling Data and Parameters for a Family of **Multi-modal** Large Language Models, [[Paper]](), [[Project]](https://github.com/Alpha-VLLM/LLaMA2-Accessory)

- (arXiv 2024.2) CREMA: **Multimodal** Compositional Video Reasoning via Efficient Modular Adaptation and Fusion, [[Paper]](https://arxiv.org/pdf/2402.05889.pdf), [[Project]](https://crema-videollm.github.io/)

- (arXiv 2024.2) S-AGENTS: SELF-ORGANIZING **AGENTS** IN OPENENDED ENVIRONMENT, [[Paper]](https://arxiv.org/pdf/2402.04578.pdf), [[Project]](https://github.com/fudan-zvg/S-Agents)

- (arXiv 2024.2) Code as Reward: Empowering **Reinforcement Learning** with VLMs, [[Paper]](https://arxiv.org/pdf/2402.04764.pdf)

- (arXiv 2024.2) Data-**efficient** Large Vision Models through Sequential Autoregression, [[Paper]](https://arxiv.org/pdf/2402.04841.pdf), [[Project]](https://github.com/ggjy/DeLVM)

- (arXiv 2024.2) MLLM-as-a-Judge: **Assessing** **Multimodal** LLM-as-a-Judge with Vision-Language Benchmark, [[Paper]](https://arxiv.org/pdf/2402.04788.pdf), [[Project]](https://github.com/Dongping-Chen/MLLM-as-a-Judge)

- (arXiv 2024.2) Beyond Lines and Circles: Unveiling the **Geometric Reasoning** Gap in Large Language Models, [[Paper]](https://arxiv.org/pdf/2402.03877.pdf), [[Project]](https://euclidea.fandom.com/wiki/Euclidea_Wiki)

- (arXiv 2024.2) “Task Success” is not Enough: Investigating the Use of **Video-Language** Models as Behavior Critics for Catching Undesirable **Agent** Behaviors, [[Paper]](https://arxiv.org/pdf/2402.04210.pdf), [[Project]](https://guansuns.github.io/pages/vlm-critic/)

- (arXiv 2024.2) RAP: Retrieval-Augmented **Planning** with Contextual Memory for Multimodal LLM **Agents**, [[Paper]](https://arxiv.org/pdf/2402.03610.pdf)

- (arXiv 2024.2) Uni3D-LLM: Unifying **Point Cloud** Perception, Generation and Editing with Large **Language** Models, [[Paper]](https://arxiv.org/pdf/2402.03327.pdf)

- (arXiv 2024.2) The Instinctive Bias: Spurious Images lead to **Hallucination** in MLLMs, [[Paper]](https://arxiv.org/pdf/2402.03757.pdf), [[Project]](https://github.com/MasaiahHan/CorrelationQA)

- (arXiv 2024.2) **Mobile**VLM V2: Faster and Stronger Baseline for **Vision Language** Model, [[Paper]](https://arxiv.org/pdf/2402.03766.pdf), [[Project]](https://github.com/Meituan-AutoML/MobileVLM)

- (arXiv 2024.2) CogCoM: Train Large **Vision-Language** Models Diving into Details through Chain of Manipulations, [[Paper]](https://arxiv.org/pdf/2402.04236.pdf), [[Project]](https://github.com/THUDM/CogCoM)

- (arXiv 2024.2) Compositional **Generative** Modeling: A Single Model is Not All You Need, [[Paper]](https://arxiv.org/pdf/2402.01103.pdf)

- (arXiv 2024.2) IMUGPT 2.0: Language-Based Cross Modality Transfer for **Sensor-Based** Human **Activity Recognition**, [[Paper]](https://arxiv.org/pdf/2402.01049.pdf)

- (arXiv 2024.2) SKIP \N: A SIMPLE METHOD TO REDUCE **HALLUCINATION** IN LARGE **VISION-LANGUAGE** MODELS, [[Paper]](https://arxiv.org/pdf/2402.01345.pdf), [[Project]](https://github.com/hanmenghan/Skip-n)

### 2024.1

<!-- - (arXiv 2024.1) , [[Paper]](), [[Project]]()-->

- (arXiv 2024.1) SWARMBRAIN: EMBODIED AGENT FOR REAL-TIME **STRATEGY GAME** STARCRAFT II VIA LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2401.17749.pdf)

- (arXiv 2024.1) YTCommentQA: **Video Question Answer**ability in Instructional Videos, [[Paper]](https://arxiv.org/pdf/2401.17343.pdf), [[Project]](https://github.com/lgresearch/YTCommentQA)

- (arXiv 2024.1) MouSi: Poly-Visual-Expert **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2401.17221.pdf), [[Project]](https://github.com/FudanNLPLAB/MouSi)

- (arXiv 2024.1) DORAEMONGPT: TOWARD UNDERSTANDING **DYNAMIC SCENES** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2401.08392.pdf), [[Project]](https://github.com/z-x-yang/DoraemonGPT)

- (arXiv 2024.1) KAM-CoT: Knowledge Augmented Multimodal **Chain-of-Thoughts** Reasoning, [[Paper]](https://arxiv.org/pdf/2401.12863.pdf)

- (arXiv 2024.1) Growing from Exploration: A self-exploring framework for **robots** based on foundation models, [[Paper]](https://arxiv.org/pdf/2401.13462.pdf), [[Project]](https://sites.google.com/view/gexp)

- (arXiv 2024.1) TRUE KNOWLEDGE COMES FROM PRACTICE: ALIGNING LLMS WITH **EMBODIED** ENVIRONMENTS VIA REINFORCEMENT LEARNING, [[Paper]](https://arxiv.org/pdf/2401.14151.pdf), [[Project]](https://github.com/WeihaoTan/TWOSOME)

- (arXiv 2024.1) **Red Teaming** Visual Language Models, [[Paper]](https://arxiv.org/pdf/2401.12915.pdf), [[Project]](https://huggingface.co/datasets/MMInstruction/RedTeamingVLM)

- (arXiv 2024.1) The Neglected Tails of **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2401.12425.pdf), [[Project]](https://shubhamprshr27.github.io/neglected-tails-of-vlms/)

- (arXiv 2024.1) Zero Shot Open-ended **Video** Inference, [[Paper]](https://arxiv.org/pdf/2401.12471.pdf)

- (arXiv 2024.1) **Small** **Language** Model Meets with Reinforced **Vision** Vocabulary, [[Paper]](https://arxiv.org/pdf/2401.12503.pdf), [[Project]](https://varytoy.github.io/)

- (arXiv 2024.1) HAZARD CHALLENGE: **EMBODIED** DECISION MAKING IN DYNAMICALLY CHANGING ENVIRONMENTS, [[Paper]](https://arxiv.org/pdf/2401.12975.pdf), [[Project]](https://vis-www.cs.umass.edu/hazard/)

- (arXiv 2024.1) VisualWebArena: **EVALUATING** MULTIMODAL **AGENTS** ON REALISTIC VISUAL WEB TASKS, [[Paper]](https://arxiv.org/pdf/2401.13649.pdf), [[Project]](https://jykoh.com/vwa)

- (arXiv 2024.1) ChatterBox: Multi-round Multimodal **Referring** and **Grounding**, [[Paper]](https://arxiv.org/pdf/2401.13307.pdf), [[Project]](https://github.com/sunsmarterjie/ChatterBox)

- (arXiv 2024.1) CONTEXTUAL: Evaluating Context-Sensitive Text-Rich **Visual Reasoning** in Large Multimodal Models, [[Paper]](https://arxiv.org/pdf/2401.13311.pdf), [[Project]](https://con-textual.github.io/#leaderboard)

- (arXiv 2024.1) UNIMO-G: Unified **Image Generation** through Multimodal Conditional Diffusion, [[Paper]](https://arxiv.org/pdf/2401.13388.pdf), [[Project]](https://unimo-ptm.github.io/)

- (arXiv 2024.1) DEMOCRATIZING FINE-GRAINED VISUAL **RECOGNITION** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2401.13837.pdf), [[Project]](https://projfiner.github.io/)

- (arXiv 2024.1) Benchmarking Large **Multimodal** Models against Common **Corruptions**, [[Paper]](https://arxiv.org/pdf/2401.11943.pdf), [[Project]](https://github.com/sail-sg/MMCBench)

- (arXiv 2024.1) CMMMU: A Chinese Massive Multi-discipline **Multimodal** Understanding **Benchmark**, [[Paper]](https://arxiv.org/pdf/2401.11944.pdf), [[Project]](https://cmmmu-benchmark.github.io/)

- (arXiv 2024.1) Prompting Large Vision-Language Models for **Compositional Reasoning**, [[Paper]](https://arxiv.org/pdf/2401.11337.pdf), [[Project]](https://github.com/tossowski/KeyComp)

- (arXiv 2024.1) Mastering **Text-to-Image** Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs, [[Paper]](https://arxiv.org/pdf/2401.11708.pdf), [[Project]](https://github.com/YangLing0818/RPG-DiffusionMaster)

- (arXiv 2024.1) SpatialVLM: Endowing Vision-Language Models with **Spatial Reasoning** Capabilities, [[Paper]](https://arxiv.org/pdf/2401.12168.pdf), [[Project]](https://spatial-vlm.github.io/)

- (arXiv 2024.1) Towards A Better **Metric** for **Text-to-Video** Generation, [[Paper]](https://arxiv.org/pdf/2401.07781.pdf), [[Project]](https://showlab.github.io/T2VScore/)

- (arXiv 2024.1) EXPLOITING GPT-4 VISION FOR ZERO-SHOT **POINT CLOUD** UNDERSTANDING, [[Paper]](https://arxiv.org/pdf/2401.07572.pdf)

- (arXiv 2024.1) MM-SAP: A Comprehensive **Benchmark** for Assessing Self-Awareness of Multimodal Large Language Models in Perception, [[Paper]](https://arxiv.org/pdf/2401.07529.pdf), [[Project]](https://github.com/YHWmz/MM-SAP)

- (arXiv 2024.1) GATS: Gather-Attend-Scatter, [[Paper]](https://arxiv.org/pdf/2401.08525.pdf)

- (arXiv 2024.1) DiffusionGPT: LLM-Driven **Text-to-Image** Generation System, [[Paper]](https://arxiv.org/pdf/2401.10061.pdf), [[Project]](https://diffusiongpt.github.io/)

- (arXiv 2024.1) TEMPORAL INSIGHT ENHANCEMENT: MITIGATING **TEMPORAL HALLUCINATION** IN MULTIMODAL LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2401.09861.pdf)

- (arXiv 2024.1) Advancing Large **Multi-modal** Models with Explicit Chain-of-Reasoning and Visual Question Generation, [[Paper]](https://arxiv.org/pdf/2401.10005.pdf)

- (arXiv 2024.1) GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot **Egocentric** **Action** Recognition, [[Paper]](https://arxiv.org/pdf/2401.10039.pdf)

- (arXiv 2024.1) SCENEVERSE: Scaling **3D Vision-Language** Learning for Grounded Scene Understanding, [[Paper]](https://arxiv.org/pdf/2401.09340.pdf), [[Project]](https://scene-verse.github.io/)

- (arXiv 2024.1) Vlogger: Make Your Dream A **Vlog**, [[Paper]](https://arxiv.org/pdf/2401.09414.pdf), [[Project]](https://github.com/zhuangshaobin/Vlogger)

- (arXiv 2024.1) CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of **Quadruped Robot**, [[Paper]](https://arxiv.org/pdf/2401.09388.pdf), [[Project]](https://huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset)

- (arXiv 2024.1) Consolidating Trees of **Robotic** Plans Generated Using Large Language Models to Improve Reliability, [[Paper]](https://arxiv.org/pdf/2401.07868.pdf)

- (arXiv 2024.1) Mementos: A Comprehensive **Benchmark** for Multimodal Large Language Model **Reasoning** over Image Sequences, [[Paper]](https://arxiv.org/pdf/2401.10529.pdf), [[Project]](https://github.com/umd-huang-lab/Mementos)

- (arXiv 2024.1) Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2401.10711.pdf)

- (arXiv 2024.1) Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for **VQA** requiring Diverse World Knowledge, [[Paper]](https://arxiv.org/pdf/2401.10712.pdf)

- (arXiv 2024.1) Tool-LMM: A Large Multi-Modal Model for Tool **Agent** Learning, [[Paper]](https://arxiv.org/pdf/2401.10727.pdf), [[Project]](https://github.com/Tool-LMM/Tool-LMM)

- (arXiv 2024.1) MMToM-QA: **Multimodal** Theory of Mind **Question Answering**, [[Paper]](https://arxiv.org/pdf/2401.08743.pdf), [[Project]](https://chuanyangjin.com/mmtom-qa)

- (arXiv 2024.1) EgoGen: An **Egocentric** Synthetic Data **Generator**, [[Paper]](https://arxiv.org/pdf/2401.08739.pdf), [[Project]](https://ego-gen.github.io/)

- (arXiv 2024.1) COCO IS “ALL” YOU NEED FOR **VISUAL INSTRUCTION FINE-TUNING**, [[Paper]](https://arxiv.org/pdf/2401.08968.pdf)

- (arXiv 2024.1) OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in **Mixed Reality**, [[Paper]](https://arxiv.org/pdf/2401.08973.pdf), [[Project]](https://octo-pearl.github.io/)

- (arXiv 2024.1) MultiPLY: A Multisensory Object-Centric **Embodied** Large Language Model in 3D World, [[Paper]](https://arxiv.org/pdf/2401.08577.pdf), [[Project]](https://vis-www.cs.umass.edu/multiply/)

- (arXiv 2024.1) SELF-IMAGINE: EFFECTIVE UNIMODAL **REASONING** WITH MULTIMODAL MODELS USING SELF-IMAGINATION, [[Paper]](https://arxiv.org/pdf/2401.08025.pdf), [[Project]](https://github.com/snat1505027/self-imagine)

- (arXiv 2024.1) Multi-Track Timeline Control for Text-Driven **3D** Human **Motion** Generation, [[Paper]](https://arxiv.org/pdf/2401.08559.pdf), [[Project]](https://mathis.petrovich.fr/stmc/)

- (arXiv 2024.1) Towards Language-Driven **Video Inpainting** via Multimodal Large Language Models, [[Paper]](https://arxiv.org/pdf/2401.10226.pdf), [[Project]](https://jianzongwu.github.io/projects/rovi/)

- (arXiv 2024.1) An Improved Baseline for Reasoning **Segmentation** with Large Language Model, [[Paper]](https://arxiv.org/pdf/2312.17240.pdf)

- (arXiv 2024.1) MultiPLY: A Multisensory Object-Centric **Embodied** Large Language Model in 3D World, [[Paper]](https://arxiv.org/pdf/2401.08577.pdf), [[Project]](https://vis-www.cs.umass.edu/multiply/)

- (arXiv 2024.1) 3D-PREMISE: CAN LARGE LANGUAGE MODELS GENERATE 3D SHAPES WITH SHARP FEATURES AND PARAMETRIC CONTROL? [[Paper]](https://arxiv.org/pdf/2401.06437.pdf)

- (arXiv 2024.1) 360DVD: Controllable Panorama Video Generation with 360-Degree **Video Diffusion** Model, [[Paper]](https://arxiv.org/pdf/2401.06578.pdf), [[Project]](https://akaneqwq.github.io/360DVD/)

- (arXiv 2024.1) Eyes Wide Shut? Exploring the Visual Shortcomings of **Multimodal** LLMs, [[Paper]](https://arxiv.org/pdf/2401.06209.pdf), [[Project]](https://tsb0601.github.io/mmvp_blog/)

- (arXiv 2024.1) AffordanceLLM: **Grounding** **Affordance** from Vision Language Models, [[Paper]](https://arxiv.org/pdf/2401.06341.pdf), [[Project]](https://jasonqsy.github.io/AffordanceLLM/)

- (arXiv 2024.1) ModaVerse: **Efficiently** Transforming **Modalities** with LLMs, [[Paper]](https://arxiv.org/pdf/2401.06395.pdf)

- (arXiv 2024.1) REPLAN: **ROBOTIC** REPLANNING WITH PERCEPTION AND LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2401.04157.pdf), [[Project]](https://replan-lm.github.io/replan.github.io/)

- (arXiv 2024.1) Language-Conditioned Robotic **Manipulation** with Fast and Slow Thinking, [[Paper]](https://arxiv.org/pdf/2401.04181.pdf), [[Project]](https://jlm-z.github.io/RSFT/)

- (arXiv 2024.1) FunnyNet-W: Multimodal Learning of Funny Moments in **Videos** in the Wild, [[Paper]](https://arxiv.org/pdf/2401.04210.pdf)

- (arXiv 2024.1) REBUS: A Robust Evaluation Benchmark of Understanding **Symbols**, [[Paper]](https://arxiv.org/pdf/2401.05604.pdf), [[Project]](https://github.com/cvndsh/rebus)

- (arXiv 2024.1) LEGO:Language Enhanced Multi-modal **Grounding** Model, [[Paper]](https://arxiv.org/pdf/2401.06071.pdf), [[Project]](https://lzw-lzw.github.io/LEGO.github.io/)

- (arXiv 2024.1) Distilling Vision-**Language** Models on Millions of **Videos**, [[Paper]](https://arxiv.org/pdf/2401.06129.pdf)

- (arXiv 2024.1) EXPLORING LARGE LANGUAGE MODEL BASED INTELLIGENT **AGENTS**: DEFINITIONS, METHODS, AND PROSPECTS, [[Paper]](https://arxiv.org/pdf/2401.03428.pdf)

- (arXiv 2024.1) **AGENT** AI: SURVEYING THE HORIZONS OF MULTIMODAL INTERACTION, [[Paper]](https://arxiv.org/pdf/2401.03568.pdf)

- (arXiv 2024.1) ExTraCT – Explainable **Trajectory Corrections** from language inputs using Textual description of features, [[Paper]](https://arxiv.org/pdf/2401.03701.pdf)

- (arXiv 2024.1) Incorporating Visual **Experts** to Resolve the Information Loss in **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2401.03105.pdf)

- (arXiv 2024.1) GPT-4V(ision) is a Human-Aligned Evaluator for **Text-to-3D** Generation, [[Paper]](https://arxiv.org/pdf/2401.04092.pdf), [[Project]](https://gpteval3d.github.io/)

- (arXiv 2024.1) Large Language Models as Visual **Cross-Domain** Learners, [[Paper]](https://arxiv.org/pdf/2401.03253.pdf), [[Project]](https://ll-a-vo.github.io/)

- (arXiv 2024.1) 3DMIT: **3D** MULTI-MODAL INSTRUCTION TUNING FOR SCENE UNDERSTANDING, [[Paper]](https://arxiv.org/pdf/2401.03201.pdf), [[Project]](https://github.com/staymylove/3DMIT)

- (arXiv 2024.1) CaMML: **Context**-Aware **Multimodal** Learner for Large Models, [[Paper]](https://arxiv.org/pdf/2401.03149.pdf)

- (arXiv 2024.1) Object-Centric Instruction Augmentation for **Robotic Manipulation**, [[Paper]](https://arxiv.org/pdf/2401.02814.pdf), [[Project]](https://oci-robotics.github.io/)

- (arXiv 2024.1) Towards Truly Zero-shot Compositional **Visual Reasoning** with LLMs as Programmers, [[Paper]](https://arxiv.org/pdf/2401.01974.pdf)

- (arXiv 2024.1) A Vision **Check-up** for Language Models, [[Paper]](https://arxiv.org/pdf/2401.01862.pdf), [[Project]](https://vision-checkup.csail.mit.edu/)

- (arXiv 2024.1) GPT-4V(ision) is a Generalist **Web** **Agent**, if Grounded, [[Paper]](https://arxiv.org/pdf/2401.01614.pdf), [[Project]](https://github.com/OSU-NLP-Group/SeeAct)

- (arXiv 2024.1) LLaVA-ϕ: Efficient Multi-Modal **Assistant** with Small Language Model, [[Paper]](https://arxiv.org/pdf/2401.02330.pdf), [[Project]](https://github.com/zhuyiche/llava-phi)

### 2023.12

<!-- - (arXiv 2023.12) , [[Paper]](), [[Project]]()-->

- (arXiv 2023.12) MOCHa: Multi-Objective Reinforcement Mitigating **Caption** **Hallucinations**, [[Paper]](https://arxiv.org/pdf/2312.03631.pdf), [[Project]](https://assafbk.github.io/mocha/)

- (arXiv 2023.12) Visual Program **Distillation**: Distilling Tools and Programmatic Reasoning into **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2312.03052.pdf), [[Project]](https://visual-program-distillation.github.io/)

- (arXiv 2023.12) Customization Assistant for **Text-to-image** Generation, [[Paper]](https://arxiv.org/pdf/2312.03045.pdf)

- (arXiv 2023.12) GPT4Point: A Unified Framework for **Point-Language** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2312.02980.pdf), [[Project]](https://gpt4point.github.io/)

- (arXiv 2023.12) LLaVA-Grounding: Grounded Visual Chat with Large **Multimodal** Models, [[Paper]](https://arxiv.org/pdf/2312.02949.pdf), [[Project]](https://llava-vl.github.io/llava-grounding/)

- (arXiv 2023.12) BenchLMM: Benchmarking **Cross-style** Visual Capability of Large Multimodal Models, [[Paper]](https://arxiv.org/pdf/2312.02896.pdf), [[Project]](https://github.com/AIFEG/BenchLMM)

- (arXiv 2023.12) Generating Fine-Grained Human **Motions** Using ChatGPT-Refined Descriptions, [[Paper]](https://arxiv.org/pdf/2312.02772.pdf), [[Project]](https://sx0207.github.io/fg-mdm/)

- (arXiv 2023.12) Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual **Robustness** via Denoising In-Context Learning, [[Paper]](https://arxiv.org/pdf/2312.02546.pdf), [[Project]](https://github.com/tmllab/Machine_Vision_Therapy)

- (arXiv 2023.12) ETC: Temporal Boundary Expand then Clarify for Weakly Supervised **Video Grounding** with Multimodal Large Language Model, [[Paper]](https://arxiv.org/pdf/2312.02483.pdf)

- (arXiv 2023.12) Lenna: Language Enhanced **Reasoning** **Detection** Assistant, [[Paper]](https://arxiv.org/pdf/2312.02433.pdf), [[Project]](https://github.com/Meituan-AutoML/Lenna)

- (arXiv 2023.12) VaQuitA: Enhancing Alignment in LLM-Assisted **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2312.02310.pdf)

- (arXiv 2023.12) StoryGPT-V: Large Language Models as Consistent **Story Visualizers**, [[Paper]](https://arxiv.org/pdf/2312.02252.pdf), [[Project]](https://xiaoqian-shen.github.io/StoryGPT-V)

- (arXiv 2023.12) Diversify, Don’t Fine-Tune: Scaling Up Visual **Recognition** **Training** with **Synthetic** Images, [[Paper]](https://arxiv.org/pdf/2312.02253.pdf)

- (arXiv 2023.12) Recursive **Visual Programming**, [[Paper]](https://arxiv.org/pdf/2312.02249.pdf)

- (arXiv 2023.12) PixelLM: **Pixel** Reasoning with Large Multimodal Model, [[Paper]](https://arxiv.org/pdf/2312.02228.pdf)

- (arXiv 2023.12) Generating Action-conditioned Prompts for Open-vocabulary **Video Action** Recognition, [[Paper]](https://arxiv.org/pdf/2312.02226.pdf)

- (arXiv 2023.12) Behind the Magic, MERLIM: Multi-modal Evaluation **Benchmark** for Large **Image-Language** Models, [[Paper]](https://arxiv.org/pdf/2312.02219.pdf), [[Project]](https://github.com/ojedaf/MERLIM)

- (arXiv 2023.12) **Video** Summarization: Towards Entity-Aware **Captions**, [[Paper]](https://arxiv.org/pdf/2312.02188.pdf)

- (arXiv 2023.12) VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2312.08367.pdf)

- (arXiv 2023.12) See, Say, and **Segment**: Teaching LMMs to Overcome False Premises, [[Paper]](https://arxiv.org/pdf/2312.08366.pdf), [[Project]](https://see-say-segment.github.io/)

- (arXiv 2023.12) Chat-3D v2: Bridging **3D Scene** and Large Language Models with Object Identifiers, [[Paper]](https://arxiv.org/pdf/2312.08168.pdf)

- (arXiv 2023.12) Interfacing Foundation Models’ **Embeddings**, [[Paper]](https://arxiv.org/pdf/2312.07532.pdf), [[Project]](https://github.com/UX-Decoder/FIND)

- (arXiv 2023.12) VILA: On Pre-training for **Visual Language** Models, [[Paper]](https://arxiv.org/pdf/2312.07533.pdf)

- (arXiv 2023.12) MP5: A Multi-modal Open-ended Embodied System in **Minecraft** via Active Perception, [[Paper]](https://arxiv.org/pdf/2312.07472.pdf), [[Project]](https://iranqin.github.io/MP5.github.io/)

- (arXiv 2023.12) Hallucination Augmented Contrastive Learning for **Multimodal** Large Language Model, [[Paper]](https://arxiv.org/pdf/2312.06968.pdf)

- (arXiv 2023.12) Honeybee: Locality-enhanced Projector for **Multimodal** LLM, [[Paper]](https://arxiv.org/pdf/2312.06742.pdf), [[Project]](https://github.com/kakaobrain/honeybee)

- (arXiv 2023.12) SmartEdit: Exploring Complex Instruction-based **Image Editing** with Multimodal Large Language Models, [[Paper]](https://arxiv.org/pdf/2312.06739.pdf), [[Project]](https://github.com/TencentARC/SmartEdit)

- (arXiv 2023.12) InstructAny2Pix: Flexible **Visual Editing** via Multimodal Instruction Following, [[Paper]](https://arxiv.org/pdf/2312.06738.pdf), [[Project]](https://github.com/jacklishufan/InstructAny2Pix)

- (arXiv 2023.12) EgoPlan-Bench: Benchmarking **Egocentric** **Embodied** Planning with Multimodal Large Language Models, [[Paper]](https://arxiv.org/pdf/2312.06722.pdf), [[Project]](https://github.com/ChenYi99/EgoPlan)

- (arXiv 2023.12) AM-RADIO: Agglomerative Model – Reduce All **Domains** Into One, [[Paper]](https://arxiv.org/pdf/2312.06709.pdf), [[Project]](https://github.com/NVlabs/RADIO)

- (arXiv 2023.12) Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in **Video-Language** Joint Learning, [[Paper]](https://arxiv.org/pdf/2312.06699.pdf)

- (arXiv 2023.12) How Well Does GPT-4V(ision) Adapt to **Distribution Shifts**? A Preliminary Investigation, [[Paper]](https://arxiv.org/pdf/2312.07424.pdf), [[Project]](https://github.com/jameszhou-gl/gpt-4v-distribution-shift)

- (arXiv 2023.12) **Audio-Visual** LLM for Video Understanding, [[Paper]](https://arxiv.org/pdf/2312.06720.pdf)

- (arXiv 2023.12) AnyHome: Open-Vocabulary **Generation** of Structured and Textured **3D Homes**, [[Paper]](https://arxiv.org/pdf/2312.06644.pdf), [[Project]](https://freddierao.github.io/AnyHome/)

- (arXiv 2023.12) Learning **Hierarchical Prompt** with Structured Linguistic Knowledge for Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2312.06323.pdf), [[Project]]( https://
github.com/Vill-Lab/2024-AAAI-HPT)

- (arXiv 2023.12) Vary: Scaling up the Vision Vocabulary for Large **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2312.06109.pdf), [[Project]](https://varybase.github.io/)

- (arXiv 2023.12) AllSpark: a **multimodal** **spatiotemporal** general model, [[Paper]](https://arxiv.org/pdf/2401.00546.pdf)

- (arXiv 2023.12) **Tracking** with Human-Intent Reasoning, [[Paper]](https://arxiv.org/pdf/2312.17448.pdf), [[Project]](https://github.com/jiawen-zhu/TrackGPT)

- (arXiv 2023.12) Retrieval-Augmented Egocentric **Video Captioning**, [[Paper]](https://arxiv.org/pdf/2401.00789.pdf)

- (arXiv 2023.12) COSMO: COntrastive Streamlined **MultimOdal** Model with Interleaved **Pre-Training**, [[Paper]](https://arxiv.org/pdf/2401.00849.pdf), [[Project]](https://fingerrec.github.io/cosmo/)

- (arXiv 2023.12) LARP: LANGUAGE-**AGENT** ROLE PLAY FOR OPEN-WORLD **GAMES**, [[Paper]](https://arxiv.org/pdf/2312.17653.pdf), [[Project]](https://arxiv.org/pdf/2312.17653.pdf)

- (arXiv 2023.12) CLOVA: A Closed-LOop Visual Assistant with **Tool** Usage and Update, [[Paper]](https://arxiv.org/pdf/2312.10908.pdf), [[Project]](https://clova-tool.github.io/)

- (arXiv 2023.12) DiffVL: Scaling Up **Soft Body Manipulation** using Vision-Language Driven Differentiable Physics, [[Paper]](https://arxiv.org/pdf/2312.06408.pdf), [[Project]](https://sites.google.com/view/diffvl/home)

- (arXiv 2023.12) VISTA-LLAMA: Reliable **Video Narrator** via Equal Distance to Visual Tokens, [[Paper]](https://arxiv.org/pdf/2312.08870.pdf), [[Project]](https://jinxxian.github.io/Vista-LLaMA/)

- (arXiv 2023.12) VL-GPT: A Generative Pre-trained Transformer for **Vision and Language** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2312.09251.pdf), [[Project]](https://github.com/AILab-CVC/VL-GPT)

- (arXiv 2023.12) **Pixel Aligned** Language Models, [[Paper]](https://arxiv.org/pdf/2312.09237.pdf), [[Project]](https://jerryxu.net/PixelLLM/)

- (arXiv 2023.12) Grounding-Prompter: Prompting LLM with Multimodal Information for **Temporal Sentence Grounding** in Long Videos, [[Paper]](https://arxiv.org/pdf/2312.17117.pdf)

- (arXiv 2023.12) Q-ALIGN: Teaching LMMs for **Visual Scoring** via Discrete Text-Defined Levels, [[Paper]](https://arxiv.org/pdf/2312.17090.pdf), [[Project]](https://github.com/Q-Future/Q-Align)

- (arXiv 2023.12) Osprey: Pixel Understanding with **Visual Instruction** Tuning, [[Paper]](https://arxiv.org/pdf/2312.10032.pdf), [[Project]](https://github.com/CircleRadon/Osprey)

- (arXiv 2023.12) Unified-IO 2: Scaling Autoregressive **Multimodal** Models with Vision, Language, Audio, and Action, [[Paper]](https://arxiv.org/pdf/2312.17172.pdf), [[Project]](https://unified-io-2.allenai.org/)

- (arXiv 2023.12) A Simple LLM Framework for Long-Range **Video Question-Answering**, [[Paper]](https://arxiv.org/pdf/2312.17235.pdf), [[Project]](https://github.com/CeeZh/LLoVi)

- (arXiv 2023.12) TinyGPT-V: **Efficient** **Multimodal** Large Language Model via Small Backbones, [[Paper]](https://arxiv.org/pdf/2312.16862.pdf), [[Project]](https://github.com/DLYuanGod/TinyGPT-V)

- (arXiv 2023.12) ManipLLM: Embodied Multimodal Large Language Model for Object-Centric **Robotic Manipulation**, [[Paper]](https://arxiv.org/pdf/2312.16217.pdf), [[Project]](https://sites.google.com/view/manipllm)

- (arXiv 2023.12) ChartBench: A Benchmark for Complex **Visual Reasoning** in **Charts**, [[Paper]](https://arxiv.org/pdf/2312.15915.pdf)

- (arXiv 2023.12) FoundationPose: Unified **6D Pose** Estimation and Tracking of Novel Objects, [[Paper]](https://arxiv.org/pdf/2312.08344.pdf), [[Project]](https://nvlabs.github.io/FoundationPose/)

- (arXiv 2023.12) Make-A-Character: High Quality **Text-to-3D** **Character** Generation within Minutes, [[Paper]](https://arxiv.org/pdf/2312.15430.pdf), [[Project]](https://human3daigc.github.io/MACH/)

- (arXiv 2023.12) Osprey: Pixel Understanding with **Visual Instruction** Tuning, [[Paper]](https://arxiv.org/pdf/2312.10032.pdf), [[Project]](https://github.com/CircleRadon/Osprey)

- (arXiv 2023.12) 3DAxiesPrompts: Unleashing the **3D Spatial** Task Capabilities of GPT-4V, [[Paper]](https://arxiv.org/pdf/2312.09738.pdf)

- (arXiv 2023.12) SMILE: Multimodal Dataset for Understanding **Laughter** in **Video** with Language Models, [[Paper]](https://arxiv.org/pdf/2312.09818.pdf), [[Project]](https://github.com/SMILE-data/SMILE)

- (arXiv 2023.12) VideoPoet: A Large Language Model for Zero-Shot **Video Generation**, [[Paper]](https://arxiv.org/pdf/2312.14125.pdf), [[Project]](https://sites.research.google/videopoet/)

- (arXiv 2023.12) V∗: Guided Visual Search as a Core Mechanism in Multimodal LLMs, [[Paper]](https://arxiv.org/pdf/2312.14135.pdf), [[Project]](https://vstar-seal.github.io/)

- (arXiv 2023.12) A Semantic Space is Worth 256 Language Descriptions: Make Stronger **Segmentation** Models with Descriptive Properties, [[Paper]](https://arxiv.org/pdf/2312.13764.pdf), [[Project]](https://github.com/lambert-x/ProLab)

- (arXiv 2023.12) AppAgent: **Multimodal** **Agents** as Smartphone Users, [[Paper]](https://arxiv.org/pdf/2312.13771.pdf), [[Project]](https://appagent-official.github.io/)

- (arXiv 2023.12) InfoVisDial: An Informative **Visual Dialogue** **Dataset** by Bridging Large Multimodal and Language Models, [[Paper]](https://arxiv.org/pdf/2312.13503.pdf), [[Project]]()

- (arXiv 2023.12) Not All Steps are Equal: Efficient Generation with Progressive **Diffusion** Models, [[Paper]](https://arxiv.org/pdf/2312.13307.pdf)

- (arXiv 2023.12) Generative **Multimodal** Models are In-Context Learners, [[Paper]](https://arxiv.org/pdf/2312.13286.pdf), [[Project]](https://baaivision.github.io/emu2/)

- (arXiv 2023.12) VCoder: Versatile Vision Encoders for **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2312.14233.pdf), [[Project]](https://github.com/SHI-Labs/VCoder)

- (arXiv 2023.12) LLM4VG: Large Language Models Evaluation for **Video Grounding**, [[Paper]](https://arxiv.org/pdf/2312.14206.pdf)

- (arXiv 2023.12) InternVL: Scaling up Vision Foundation Models and Aligning for Generic **Visual-Linguistic** Tasks, [[Paper]](https://arxiv.org/pdf/2312.14238.pdf), [[Project]](https://github.com/OpenGVLab/InternVL)

- (arXiv 2023.12) VIEScore: Towards Explainable **Metrics** for Conditional **Image Synthesis** Evaluation, [[Paper]](https://arxiv.org/pdf/2312.14867.pdf), [[Project]](https://tiger-ai-lab.github.io/VIEScore/)

- (arXiv 2023.12) Plan, Posture and Go: Towards Open-World **Text-to-Motion** Generation, [[Paper]](https://arxiv.org/pdf/2312.14828.pdf), [[Project]](https://moonsliu.github.io/Pro-Motion/)

- (arXiv 2023.12) MotionScript: Natural Language Descriptions for Expressive **3D Human Motions**, [[Paper]](https://arxiv.org/pdf/2312.12634.pdf), [[Project]](https://pjyazdian.github.io/MotionScript/)

- (arXiv 2023.12) Assessing GPT4-V on **Structured Reasoning** Tasks, [[Paper]](https://arxiv.org/pdf/2312.11524.pdf), [[Project]]()

- (arXiv 2023.12) Iterative **Motion Editing** with Natural Language, [[Paper]](https://arxiv.org/pdf/2312.11538.pdf), [[Project]]()

- (arXiv 2023.12) **Gemini**: A Family of Highly Capable **Multimodal** Models, [[Paper]](https://arxiv.org/pdf/2312.11805.pdf), [[Project]](https://deepmind.google/gemini)

- (arXiv 2023.12) StarVector: Generating **Scalable Vector Graphics** Code from Images, [[Paper]](https://arxiv.org/pdf/2312.11556.pdf), [[Project]](https://github.com/joanrod/star-vector)

- (arXiv 2023.12) Text-Conditioned Resampler For Long Form **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2312.11897.pdf)

- (arXiv 2023.12) Mixture of Cluster-conditional LoRA Experts for Vision-language **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2312.12379.pdf)

- (arXiv 2023.12) A Challenger to **GPT-4V**? Early Explorations of **Gemini** in Visual Expertise, [[Paper]](https://arxiv.org/pdf/2312.12436.pdf), [[Project]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

- (arXiv 2023.12) Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine **Vision-Language** Model, [[Paper]](https://arxiv.org/pdf/2312.12423.pdf), [[Project]](https://shramanpramanick.github.io/VistaLLM/)

- (arXiv 2023.12) M^2ConceptBase: A Fine-grained Aligned **Multi-modal** Conceptual Knowledge Base, [[Paper]](https://arxiv.org/pdf/2312.10417.pdf)

- (arXiv 2023.12) Language-conditioned Learning for **Robotic** **Manipulation**: A Survey, [[Paper]](https://arxiv.org/pdf/2312.10807.pdf)

- (arXiv 2023.12) TUNING LAYERNORM IN ATTENTION: TOWARDS **EFFICIENT** MULTI-MODAL LLM **FINETUNING**, [[Paper]](https://arxiv.org/pdf/2312.11420.pdf)

- (arXiv 2023.12) GSVA: Generalized **Segmentation** via Multimodal Large Language Models, [[Paper]](https://arxiv.org/pdf/2312.10103.pdf), [[Project]](https://github.com/LeapLabTHU/GSVA)

- (arXiv 2023.12) SILKIE: **PREFERENCE DISTILLATION** FOR LARGE VISUAL LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2312.10665.pdf), [[Project]](https://vlf-silkie.github.io/)

- (arXiv 2023.12) AN **EVALUATION** OF GPT-4V AND GEMINI IN ONLINE **VQA**, [[Paper]](https://arxiv.org/pdf/2312.10637.pdf)

- (arXiv 2023.12) CEIR: CONCEPT-BASED **EXPLAINABLE** **IMAGE** REPRESENTATION LEARNING, [[Paper]](https://arxiv.org/pdf/2312.10747.pdf), [[Project]](https://github.com/ShuhongLL/CEIR)

- (arXiv 2023.12) Language-Assisted **3D Scene** Understanding, [[Paper]](https://arxiv.org/pdf/2312.11451.pdf), [[Project]](https://github.com/yanmin-wu/LAST-PCL)

- (arXiv 2023.12) M3DBench: Let’s Instruct Large Models with Multi-modal **3D** Prompts, [[Paper]](https://arxiv.org/pdf/2312.10763.pdf), [[Project]](https://github.com/OpenM3D/M3DBench/)

- (arXiv 2023.12) Modality Plug-and-Play: Elastic Modality Adaptation in **Multimodal** LLMs for **Embodied AI**, [[Paper]](https://arxiv.org/pdf/2312.07886.pdf), [[Project]](https://github.com/pittisl/mPnP-LLM)

- (arXiv 2023.12) FROM TEXT TO MOTION: GROUNDING GPT-4 IN A HUMANOID **ROBOT** “ALTER3”, [[Paper]](https://arxiv.org/pdf/2312.06571.pdf), [[Project]](https://tnoinkwms.github.io/ALTER-LLM/)

- (arXiv 2023.12) Interactive Planning Using Large Language Models for Partially Observable **Robotics** Tasks, [[Paper]](https://arxiv.org/pdf/2312.06876.pdf)

- (arXiv 2023.12) LifelongMemory: Leveraging LLMs for Answering Queries in **Egocentric Videos**, [[Paper]](https://arxiv.org/pdf/2312.05269.pdf)

- (arXiv 2023.12) LEGO: Learning **EGOcentric Action** Frame **Generation** via Visual Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2312.03849.pdf)

- (arXiv 2023.12) Localized Symbolic Knowledge Distillation for **Visual Commonsense** Models, [[Paper]](https://arxiv.org/pdf/2312.04837.pdf), [[Project]](https://github.com/jamespark3922/lskd)

- (arXiv 2023.12) MoVQA: A Benchmark of Versatile **Question-Answering** for Long-Form **Movie** Understanding, [[Paper]](https://arxiv.org/pdf/2312.04817.pdf), [[Project]](https://movqa.github.io/)

- (arXiv 2023.12) Human Demonstrations are Generalizable Knowledge for **Robots**, [[Paper]](https://arxiv.org/pdf/2312.02419.pdf)

- (arXiv 2023.12) WonderJourney: Going from Anywhere to Everywhere, [[Paper]](https://arxiv.org/pdf/2312.03884.pdf), [[Project]](https://kovenyu.com/wonderjourney/)

- (arXiv 2023.12) VRPTEST: Evaluating Visual Referring **Prompting** in Large **Multimodal** Models, [[Paper]](https://arxiv.org/pdf/2312.04087.pdf), [[Code]](https://github.com/tszdanger/VisualReferPrompt)

- (arXiv 2023.12) Text as Image: Learning Transferable Adapter for **Multi-Label Classification**, [[Paper]](https://arxiv.org/pdf/2312.04160.pdf)

- (arXiv 2023.12) **Prompt** Highlighter: Interactive Control for **Multi-Modal** LLMs, [[Paper]](https://arxiv.org/pdf/2312.04302.pdf), [[Project]](https://julianjuaner.github.io/projects/PromptHighlighter/)

- (arXiv 2023.12) Digital Life Project: Autonomous **3D Characters** with Social Intelligence, [[Paper]](https://arxiv.org/pdf/2312.04547.pdf), [[Project]](https://digital-life-project.com/)

- (arXiv 2023.12) Generating Illustrated **Instructions**, [[Paper]](https://arxiv.org/pdf/2312.04552.pdf), [[Project]](https://facebookresearch.github.io/IllustratedInstructions/)

- (arXiv 2023.12) Aligning and Prompting Everything All at Once for Universal Visual **Perception**, [[Paper]](https://arxiv.org/pdf/2312.02153.pdf), [[Code]](https://github.com/shenyunhang/APE)

- (arXiv 2023.12) LEAP: LLM-Generation of **Egocentric Action** Programs, [[Paper]](https://arxiv.org/pdf/2312.00055.pdf)

- (arXiv 2023.12) OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General **Video** Recognition, [[Paper]](https://arxiv.org/pdf/2312.00096.pdf), [[Project]](https://tomchen-ctj.github.io/OST/)

- (arXiv 2023.12) Merlin: Empowering **Multimodal** LLMs with Foresight Minds, [[Paper]](https://arxiv.org/pdf/2312.00589.pdf), [[Project]](https://ahnsun.github.io/merlin/)

- (arXiv 2023.12) VIoTGPT: Learning to Schedule Vision **Tools** towards Intelligent **Video** Internet of Things, [[Paper]](https://arxiv.org/pdf/2312.00401.pdf), [[Code]]()

- (arXiv 2023.12) Making Large **Multimodal** Models Understand Arbitrary Visual **Prompts**, [[Paper]](https://arxiv.org/pdf/2312.00784.pdf), [[Project]](https://vip-llava.github.io/)

### 2023.11

<!-- - (arXiv 2023.11) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.11) Self-Chained Image-Language Model for **Video** Localization and Question Answering, [[Paper]](https://arxiv.org/pdf/2305.06988.pdf), [[Code]](https://github.com/Yui010206/SeViLA)

- (arXiv 2023.11) Look Before You Leap: Unveiling the Power of GPT-4V in **Robotic** Vision-Language **Planning**, [[Paper]](https://arxiv.org/pdf/2311.17842.pdf), [[Project]](https://robot-vila.github.io/)

- (arXiv 2023.11) LALM: Long-Term **Action Anticipation** with Language Models, [[Paper]](https://arxiv.org/pdf/2311.17944.pdf)

- (arXiv 2023.11) Contrastive **Vision-Language** Alignment Makes Efficient Instruction Learner, [[Paper]](https://arxiv.org/pdf/2311.17945.pdf), [[Code]](https://github.com/lizhaoliu-Lec/CG-VLM)

- (arXiv 2023.11) ChatIllusion: Efficient-Aligning Interleaved **Generation** ability with Visual Instruction Model, [[Paper]](https://arxiv.org/pdf/2311.17963.pdf), [[Code]](https://github.com/litwellchi/ChatIllusion)

- (arXiv 2023.11) MV-CLIP: Multi-View CLIP for Zero-shot **3D** Shape **Recognition**, [[Paper]](https://arxiv.org/pdf/2311.18402.pdf)

- (arXiv 2023.11) VTimeLLM: Empower LLM to Grasp **Video** Moments, [[Paper]](https://arxiv.org/pdf/2311.18445.pdf), [[Code]](https://arxiv.org/pdf/2311.18445.pdf)

- (arXiv 2023.11) Simple Semantic-Aided **Few-Shot** Learning, [[Paper]](https://arxiv.org/pdf/2311.18649.pdf)

- (arXiv 2023.11) LL3DA: Visual Interactive Instruction Tuning for Omni-**3D** Understanding, Reasoning, and Planning, [[Paper]](https://arxiv.org/pdf/2311.18651.pdf), [[Project]](https://ll3da.github.io/)

- (arXiv 2023.11) Detailed Human-Centric Text Description-Driven Large **Scene Synthesis**, [[Paper]](https://arxiv.org/pdf/2311.18654.pdf)

- (arXiv 2023.11) X-InstructBLIP: A Framework for aligning **X-Modal** instruction-aware representations to LLMs and Emergent Cross-modal Reasoning, [[Paper]](https://arxiv.org/pdf/2311.18799.pdf), [[Code]](https://github.com/artemisp/LAVIS-XInstructBLIP)

- (arXiv 2023.11) CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any **Generation**, [[Paper]](https://arxiv.org/pdf/2311.18775.pdf), [[Project]](https://codi-2.github.io/)

- (arXiv 2023.11) AvatarGPT: All-in-One Framework for **Motion** Understanding, Planning, Generation and Beyond, [[Paper]](https://arxiv.org/pdf/2311.16468.pdf), [[Project]](https://zixiangzhou916.github.io/AvatarGPT/)

- (arXiv 2023.11) InstructSeq: Unifying Vision Tasks with Instruction-conditioned **Multi-modal** Sequence **Generation**, [[Paper]](https://arxiv.org/pdf/2311.18835.pdf), [[Code]](https://github.com/rongyaofang/InstructSeq)

- (arXiv 2023.11) MLLMs-Augmented **Visual-Language** Representation Learning, [[Paper]](https://arxiv.org/pdf/2311.18765.pdf), [[Code]](https://github.com/lyq312318224/MLLMs-Augmented)

- (arXiv 2023.11) PoseGPT: Chatting about **3D** Human **Pose**, [[Paper]](https://arxiv.org/pdf/2311.18836.pdf), [[Project]](https://yfeng95.github.io/posegpt/)

- (arXiv 2023.11) LLM-State: Expandable State Representation for Long-horizon Task **Planning** in the Open World, [[Paper]](https://arxiv.org/pdf/2311.17406.pdf)

- (arXiv 2023.11) UniIR: Training and **Benchmarking** Universal **Multimodal** Information Retrievers, [[Paper]](https://arxiv.org/pdf/2311.17136.pdf), [[Project]](https://tiger-ai-lab.github.io/UniIR/)

- (arXiv 2023.11) VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of **Video**-Language Models, [[Paper]](https://arxiv.org/pdf/2311.17404.pdf), [[Code]](https://github.com/lscpku/VITATECS)

- (arXiv 2023.11) MM-Narrator: **Narrating** Long-form **Videos** with Multimodal In-Context Learning, [[Paper]](https://arxiv.org/pdf/2311.17435.pdf), [[Project]](https://mm-narrator.github.io/)

- (arXiv 2023.11) Knowledge Pursuit Prompting for Zero-Shot **Multimodal** **Synthesis**, [[Paper]](https://arxiv.org/pdf/2311.17898.pdf)

- (arXiv 2023.11) Evaluating VLMs for Score-Based, Multi-Probe **Annotation** of **3D** Objects, [[Paper]](https://arxiv.org/pdf/2311.17851.pdf)

- (arXiv 2023.11) OPERA: Alleviating **Hallucination** in **Multi-Modal** Large Language Models via Over-Trust Penalty and Retrospection-Allocation, [[Paper]](https://arxiv.org/pdf/2311.17911.pdf), [[Code]](https://github.com/shikiw/OPERA)

- (arXiv 2023.11) ShapeGPT: **3D Shape Generation** with A Unified Multi-modal Language Model, [[Paper]](https://arxiv.org/pdf/2311.17618.pdf), [[Project]](https://shapegpt.github.io/)

- (arXiv 2023.11) VIM: **Probing** **Multimodal** Large Language Models for Visual Embedded Instruction Following, [[Paper]](https://arxiv.org/pdf/2311.17647.pdf), [[Project]](https://vim-bench.github.io/)

- (arXiv 2023.11) Video-Bench: A Comprehensive **Benchmark** and **Toolkit** for Evaluating **Video**-based Large Language Models, [[Paper]](https://arxiv.org/pdf/2311.16103.pdf), [[Code]](https://github.com/PKU-YuanGroup/Video-Bench)

- (arXiv 2023.11) Self-correcting LLM-controlled **Diffusion** Models, [[Paper]](https://arxiv.org/pdf/2311.16090.pdf)

- (arXiv 2023.11) InterControl: **Generate** Human **Motion** Interactions by Controlling Every Joint, [[Paper]](https://arxiv.org/pdf/2311.15864.pdf), [[Code]](https://github.com/zhenzhiwang/intercontrol)

- (arXiv 2023.11) DRESS: **Instructing** Large **Vision-Language** Models to Align and Interact with Humans via Natural Language Feedback, [[Paper]](https://arxiv.org/pdf/2311.10081.pdf), [[Code]](https://huggingface.co/datasets/YangyiYY/LVLM_NLF)

- (arXiv 2023.11) GAIA: A **Benchmark** for General AI **Assistants**, [[Paper]](https://arxiv.org/pdf/2311.12983.pdf), [[Project]](https://huggingface.co/gaia-benchmark)

- (arXiv 2023.11) PG-Video-LLaVA: Pixel Grounding Large **Video-Language** Models, [[Paper]](https://arxiv.org/pdf/2311.13435.pdf), [[Code]](https://github.com/mbzuai-oryx/Video-LLaVA)

- (arXiv 2023.11) Enhancing **Scene Graph** Generation with Hierarchical Relationships and Commonsense Knowledge, [[Paper]](https://arxiv.org/pdf/2311.12889.pdf)

- (arXiv 2023.11) AN EMBODIED GENERALIST **AGENT** IN **3D** WORLD, [[Paper]](https://arxiv.org/pdf/2311.12871.pdf), [[Project]](https://embodied-generalist.github.io/)

- (arXiv 2023.11) ShareGPT4V: Improving Large **Multi-Modal** Models with Better **Captions**, [[Paper]](https://arxiv.org/pdf/2311.12793.pdf), [[Project]](https://sharegpt4v.github.io/)

- (arXiv 2023.11) KNVQA: A Benchmark for evaluation knowledge-based **VQA**, [[Paper]](https://arxiv.org/pdf/2311.12639.pdf)

- (arXiv 2023.11) GPT4Motion: Scripting **Physical** Motions in **Text-to-Video** Generation via Blender-Oriented GPT Planning, [[Paper]](https://arxiv.org/pdf/2311.12631.pdf), [[Project]](https://gpt4motion.github.io/)

- (arXiv 2023.11) Boosting **Audio-visual** Zero-shot Learning with Large Language Models, [[Paper]](https://arxiv.org/pdf/2311.12268.pdf), [[Code]](https://github.com/chenhaoxing/KDA)

- (arXiv 2023.11) Few-Shot **Classification** & **Segmentation** Using Large Language Models Agent, [[Paper]](https://arxiv.org/pdf/2311.12065.pdf)

- (arXiv 2023.11) Igniting Language Intelligence: The Hitchhiker’s Guide From **Chain-of-Thought** Reasoning to Language **Agents**, [[Paper]](https://arxiv.org/pdf/2311.11797.pdf), [[Code]](https://github.com/Zoeyyao27/CoT-Igniting-Agent)

- (arXiv 2023.11) VLM-Eval: A General Evaluation on **Video** Large Language Models, [[Paper]](https://arxiv.org/pdf/2311.11865.pdf)

- (arXiv 2023.11) LLMs as Visual Explainers: Advancing **Image Classification** with Evolving Visual Descriptions, [[Paper]](https://arxiv.org/pdf/2311.11904.pdf), [[Code]](https://github.com/zhuole1025/LLMs_as_Visual_Explainers)

- (arXiv 2023.11) LION : Empowering **Multimodal** Large Language Model with Dual-Level Visual Knowledge, [[Paper]](https://arxiv.org/pdf/2311.11860.pdf), [[Project]](https://rshaojimmy.github.io/Projects/JiuTian-LION)

- (arXiv 2023.11) Chat-UniVi: Unified Visual Representation Empowers Large Language Models with **Image** and **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2311.08046.pdf), [[Code]](https://github.com/PKU-YuanGroup/Chat-UniVi)

- (arXiv 2023.11) How to Bridge the Gap between Modalities: A Comprehensive **Survey** on **Multimodal** Large Language Model, [[Paper]](https://arxiv.org/pdf/2311.07594.pdf)

- (arXiv 2023.11) Unlock the Power: Competitive **Distillation** for Multi-Modal Large Language Models, [[Paper]](https://arxiv.org/pdf/2311.08213.pdf), [[Code]]()

- (arXiv 2023.11) Towards **Open**-Ended Visual **Recognition** with Large Language Model, [[Paper]](https://arxiv.org/pdf/2311.08400.pdf), [[Code]](https://github.com/bytedance/OmniScient-Model)

- (arXiv 2023.11) Monkey: Image Resolution and Text Label Are Important Things for Large **Multi-modal** Models, [[Paper]](https://arxiv.org/pdf/2311.06607.pdf), [[Code]](https://github.com/Yuliang-Liu/Monkey)

- (arXiv 2023.11) VILMA: A ZERO-SHOT **BENCHMARK** FOR LINGUISTIC AND TEMPORAL GROUNDING IN **VIDEO-LANGUAGE** MODELS, [[Paper]](https://arxiv.org/pdf/2311.07022.pdf), [[Project]](https://cyberiada.github.io/ViLMA/)

- (arXiv 2023.11) VOLCANO: Mitigating Multimodal **Hallucination** through Self-Feedback Guided Revision, [[Paper]](https://arxiv.org/pdf/2311.07362.pdf), [[Code]](https://github.com/kaistAI/Volcano)

- (arXiv 2023.11) AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs **Hallucination** Evaluation, [[Paper]](https://arxiv.org/pdf/2311.07397.pdf), [[Code]](https://github.com/junyangwang0410/AMBER)

- (arXiv 2023.11) Analyzing Modular Approaches for **Visual Question** Decomposition, [[Paper]](https://arxiv.org/pdf/2311.06411.pdf), [[Code]](https://github.com/brown-palm/visual-question-decomposition)

- (arXiv 2023.11) **Layout**Prompter: Awaken the Design Ability of Large Language Models, [[Paper]](https://arxiv.org/pdf/2311.06495.pdf), [[Code]](https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter)

- (arXiv 2023.11) PerceptionGPT: Effectively Fusing **Visual** Perception into LLM, [[Paper]](https://arxiv.org/pdf/2311.06612.pdf)

- (arXiv 2023.11) InfMLLM: A Unified Framework for **Visual-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2311.06791.pdf), [[Code]](https://github.com/mightyzau/InfMLLM)

- (arXiv 2023.11) WHAT LARGE LANGUAGE MODELS BRING TO TEXTRICH **VQA**?, [[Paper]](https://arxiv.org/pdf/2311.07306.pdf)

- (arXiv 2023.11) Story-to-**Motion**: Synthesizing Infinite and Controllable Character Animation from Long **Text**, [[Paper]](https://arxiv.org/pdf/2311.07446.pdf), [[Project]](https://story2motion.github.io/)

- (arXiv 2023.11) GPT-4V(ision) as A **Social Media Analysis** Engine, [[Paper]](https://arxiv.org/pdf/2311.07547.pdf), [[Code]](https://github.com/VIStA-H/GPT-4V_Social_Media)

- (arXiv 2023.11) GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot **Smartphone GUI Navigation**, [[Paper]](https://arxiv.org/pdf/2311.07562.pdf), [[Code]](https://github.com/zzxslp/MM-Navigator)

- (arXiv 2023.11) To See is to Believe: Prompting GPT-4V for Better **Visual Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2311.07574.pdf), [[Code]](https://github.com/X2FD/LVIS-INSTRUCT4V)

- (arXiv 2023.11) SPHINX: THE JOINT **MIXING** OF WEIGHTS, TASKS, AND VISUAL EMBEDDINGS FOR **MULTI-MODAL** LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2311.07575.pdf), [[Code]](https://github.com/Alpha-VLLM/LLaMA2-Accessory)

- (arXiv 2023.11) ADAPT: As-Needed Decomposition and **Planning** with Language Models, [[Paper]](https://arxiv.org/pdf/2311.05772.pdf), [[Project]](https://allenai.github.io/adaptllm/)

- (arXiv 2023.11) JARVIS-1: Open-world Multi-task **Agents** with Memory-Augmented Multimodal Language Models, [[Paper]](https://arxiv.org/pdf/2311.05997.pdf), [[Project]](https://craftjarvis-jarvis1.github.io/)

- (arXiv 2023.11) Florence-2: Advancing a Unified **Representation** for a Variety of **Vision** Tasks, [[Paper]](https://arxiv.org/pdf/2311.06242.pdf)

- (arXiv 2023.11) Multitask Multimodal Prompted Training for Interactive **Embodied** Task Completion, [[Paper]](https://arxiv.org/pdf/2311.04067.pdf), [[Code]](https://github.com/emma-heriot-watt/emma)

- (arXiv 2023.11) TEAL: TOKENIZE AND EMBED ALL FOR **MULTIMODAL** LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2311.04589.pdf)

- (arXiv 2023.11) u-LLaVA: Unifying **Multi-Modal** Tasks via Large Language Model, [[Paper]](https://arxiv.org/pdf/2311.05348.pdf)

- (arXiv 2023.11) LLAVA-PLUS: LEARNING TO **USE TOOLS** FOR CREATING MULTIMODAL **AGENTS**, [[Paper]](https://arxiv.org/pdf/2311.05437.pdf), [[Project]](https://llava-vl.github.io/llava-plus/)

- (arXiv 2023.11) Detecting Any **Human-Object Interaction** Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models, [[Paper]](https://arxiv.org/pdf/2311.03799.pdf), [[Code]](https://github.com/Caoyichao/UniHOI)

- (arXiv 2023.11) OtterHD: A High-Resolution **Multi-modality** Model, [[Paper]](https://arxiv.org/pdf/2311.04219.pdf), [[Code]](https://github.com/Luodian/Otter)

- (arXiv 2023.11) NExT-Chat: An LMM for **Chat**, **Detection** and **Segmentation**, [[Paper]](https://arxiv.org/pdf/2311.04498.pdf), [[Project]](https://next-chatv.github.io/)

- (arXiv 2023.11) GENOME: GENERATIVE NEURO-SYMBOLIC VISUAL **REASONING** BY GROWING AND REUSING MODULES, [[Paper]](https://arxiv.org/pdf/2311.04901.pdf), [[Project]](https://vis-www.cs.umass.edu/genome/)

- (arXiv 2023.11) MAKE A DONUT: LANGUAGE-GUIDED HIERARCHICAL EMD-SPACE PLANNING FOR ZERO-SHOT **DEFORMABLE** OBJECT **MANIPULATION**, [[Paper]](https://arxiv.org/pdf/2311.02787.pdf)

- (arXiv 2023.11) Kinematic-aware **Prompting** for Generalizable Articulated Object **Manipulation** with LLMs, [[Paper]](https://arxiv.org/pdf/2311.02847.pdf), [[Code]](https://github.com/xwinks/LLM_articulated_object_manipulation)

- (arXiv 2023.11) Accelerating **Reinforcement Learning** of Robotic **Manipulations** via Feedback from Large Language Models, [[Paper]](https://arxiv.org/pdf/2311.02379.pdf)

- (arXiv 2023.11) ROBOGEN: TOWARDS UNLEASHING INFINITE DATA FOR AUTOMATED **ROBOT** LEARNING VIA **GENERATIVE SIMULATION**, [[Paper]](https://arxiv.org/pdf/2311.01455.pdf)


### 2023.10

<!-- - (arXiv 2023.10) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.10) MINIGPT-5: INTERLEAVED **VISION-AND-LANGUAGE** GENERATION VIA GENERATIVE VOKENS, [[Paper]](https://arxiv.org/pdf/2310.02239.pdf), [[Code]](https://github.com/eric-ai-lab/MiniGPT-5)

- (arXiv 2023.10) What’s “up” with vision-language models? Investigating their struggle with spatial reasoning, [[Paper]](https://arxiv.org/pdf/2310.19785.pdf), [[Code]](https://github.com/amitakamath/whatsup_vlms)

- (arXiv 2023.10) APOLLO: ZERO-SHOT **MULTIMODAL REASONING** WITH MULTIPLE EXPERTS, [[Paper]](https://arxiv.org/pdf/2310.18369.pdf), [[Code]](https://github.com/danielabd/Apollo-Cap)

- (arXiv 2023.10) ROME: **Evaluating** Pre-trained Vision-Language Models on **Reasoning** beyond Visual Common Sense, [[Paper]](https://arxiv.org/pdf/2310.19301.pdf)

- (arXiv 2023.10) Gen2Sim: Scaling up **Robot** Learning in **Simulation** with Generative Models, [[Paper]](https://arxiv.org/pdf/2310.18308.pdf), [[Project]](https://gen2sim.github.io/)

- (arXiv 2023.10) LARGE LANGUAGE MODELS AS GENERALIZABLE **POLICIES** FOR **EMBODIED** TASKS, [[Paper]](https://arxiv.org/pdf/2310.17722.pdf), [[Project]](https://llm-rl.github.io/)

- (arXiv 2023.10) Humanoid **Agents**: Platform for Simulating Human-like Generative Agents, [[Paper]](https://arxiv.org/pdf/2310.05418.pdf), [[Project]](https://www.humanoidagents.com/)

- (arXiv 2023.10) REVO-LION: EVALUATING AND REFINING VISION-LANGUAGE **INSTRUCTION TUNING** DATASETS, [[Paper]](https://arxiv.org/pdf/2310.06594.pdf), [[Code]](https://github.com/liaoning97/REVO-LION)

- (arXiv 2023.10) How (not) to **ensemble** LVLMs for VQA, [[Paper]](https://arxiv.org/pdf/2310.06641.pdf)

- (arXiv 2023.10) What If the TV Was Off? Examining **Counterfactual Reasoning** Abilities of Multi-modal Language Models, [[Paper]](https://arxiv.org/pdf/2310.06627.pdf), [[Code]](https://github.com/Letian2003/C-VQA)

- (arXiv 2023.10) Words into Action: Learning Diverse **Humanoid Robot** Behaviors using Language Guided Iterative **Motion** Refinement, [[Paper]](https://arxiv.org/pdf/2310.06226.pdf), [[Code]](https://www.kniranjankumar.com/words_into_action/)

- (arXiv 2023.10) GameGPT: Multi-**agent** Collaborative Framework for **Game** Development, [[Paper]](https://arxiv.org/pdf/2310.08067.pdf)

- (arXiv 2023.10) STEVE-EYE: EQUIPPING LLM-BASED EMBODIED **AGENTS** WITH VISUAL PERCEPTION IN OPEN WORLDS, [[Paper]](https://arxiv.org/pdf/2310.13255.pdf)

- (arXiv 2023.10) **BENCHMARKING** SEQUENTIAL VISUAL INPUT **REASONING** AND PREDICTION IN **MULTIMODAL** LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.13473.pdf), [[Code]](https://github.com/CoderJ-ONE/Giraffe-Bench)

- (arXiv 2023.10) A Simple Baseline for Knowledge-Based **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2310.13570.pdf), [[Code]](https://github.com/alexandrosXe/A-Simple-Baseline-For-Knowledge-Based-VQA)

- (arXiv 2023.10) Interactive **Robot** Learning from **Verbal Correction**, [[Paper]](https://arxiv.org/pdf/2310.17555.pdf), [[Project]](https://ut-austin-rpl.github.io/olaf/)

- (arXiv 2023.10) Exploring Question Decomposition for **Zero-Shot VQA**, [[Paper]](https://arxiv.org/pdf/2310.17050.pdf), [[Project]](https://zaidkhan.me/decomposition-0shot-vqa/)

- (arXiv 2023.10) RIO: A Benchmark for Reasoning **Intention**-Oriented **Objects** in Open Environments, [[Paper]](https://arxiv.org/pdf/2310.17290.pdf), [[Project]](https://reasonio.github.io/)

- (arXiv 2023.10) An Early **Evaluation** of **GPT-4V**(ision), [[Paper]](https://arxiv.org/pdf/2310.16534.pdf), [[Code]](https://github.com/albertwy/GPT-4V-Evaluation)

- (arXiv 2023.10) DDCoT: Duty-Distinct Chain-of-Thought **Prompting** for Multimodal Reasoning in Language Models, [[Paper]](https://arxiv.org/pdf/2310.16436.pdf), [[Project]](https://toneyaya.github.io/ddcot/)

- (arXiv 2023.10) CommonCanvas: An Open **Diffusion** Model Trained with Creative-Commons Images, [[Paper]](https://arxiv.org/pdf/2310.16825.pdf), [[Code]](https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md)

- (arXiv 2023.10) VIDEOPROMPTER: AN ENSEMBLE OF FOUNDATIONAL MODELS FOR **ZERO-SHOT VIDEO UNDERSTANDING**, [[Paper]](https://arxiv.org/pdf/2310.15324.pdf)

- (arXiv 2023.10) Inject Semantic Concepts into Image Tagging for **Open-Set Recognition**, [[Paper]](https://arxiv.org/pdf/2310.15200.pdf), [[Code]](https://github.com/xinyu1205/recognize-anything)

- (arXiv 2023.10) Woodpecker: **Hallucination Correction** for **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.16045.pdf), [[Code]](https://github.com/BradyFU/Woodpecker)

- (arXiv 2023.10) Visual Cropping Improves **Zero-Shot Question Answering** of **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.16033.pdf), [[Code]](https://github.com/saccharomycetes/visual_crop_zsvqa)

- (arXiv 2023.10) Large Language Models are Temporal and Causal Reasoners for **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2310.15747.pdf), [[Code]](https://github.com/mlvlab/Flipped-VQA)

- (arXiv 2023.10) What’s Left? Concept **Grounding** with Logic-Enhanced Foundation Models, [[Paper]](https://arxiv.org/pdf/2310.16035.pdf)

- (arXiv 2023.10) Evaluating **Spatial Understanding** of Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.14540.pdf)

- (arXiv 2023.10) Learning **Reward** for **Physical Skills** using Large Language Model, [[Paper]](https://arxiv.org/pdf/2310.14092.pdf)

- (arXiv 2023.10) CREATIVE **ROBOT TOOL USE** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.13065.pdf), [[Project]](https://creative-robotool.github.io/)

- (arXiv 2023.10) Open-Ended Instructable **Embodied Agents** with Memory-Augmented Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.15127.pdf), [[Project]](https://helper-agent-llm.github.io/)

- (arXiv 2023.10) **Robot** Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World **Reinforcement Learning**, [[Paper]](https://arxiv.org/pdf/2310.15145.pdf), [[Project]](https://robofume.github.io/)

- (arXiv 2023.10) LARGE LANGUAGE MODELS CAN **Share IMAGES**, TOO! [[Paper]](https://arxiv.org/pdf/2310.14804.pdf), [[Code]](https://github.com/passing2961/LLM-Share-Image)

- (arXiv 2023.10) Dataset **Bias** Mitigation in Multiple-Choice **Visual Question Answering** and Beyond, [[Paper]](https://arxiv.org/pdf/2310.14670.pdf)

- (arXiv 2023.10) HALLUSIONBENCH: You See What You Think? Or You Think What You See? An **Image-Context Reasoning Benchmark** Challenging for GPT-4V(vision), LLaVA-1.5, and Other Multi-modality Models, [[Paper]](https://arxiv.org/pdf/2310.14566.pdf), [[Code]](https://github.com/tianyi-lab/HallusionBench)

- (arXiv 2023.10) Can Language Models Laugh at YouTube **Short-form Videos**? [[Paper]](https://arxiv.org/pdf/2310.14159.pdf), [[Code]](https://github.com/dayoon-ko/ExFunTube)

- (arXiv 2023.10) Large Language Models are **Visual Reasoning** Coordinators, [[Paper]](https://arxiv.org/pdf/2310.15166.pdf), [[Code]](https://github.com/cliangyu/Cola)

- (arXiv 2023.10) Language Models as Zero-Shot **Trajectory Generators**, [[Paper]](https://arxiv.org/pdf/2310.11604.pdf), [[Project]](https://www.robot-learning.uk/language-models-trajectory-generators)

- (arXiv 2023.10) **Localizing** Active **Objects** from **Egocentric** Vision with Symbolic World Knowledge, [[Paper]](https://arxiv.org/pdf/2310.15066.pdf), [[Code]](https://github.com/PlusLabNLP/ENVISION)

- (arXiv 2023.10) Multimodal Large Language Model for Visual **Navigation**, [[Paper]](https://arxiv.org/pdf/2310.08669.pdf)

- (arXiv 2023.10) MAKING **MULTIMODAL GENERATION** EASIER: WHEN DIFFUSION MODELS MEET LLMS, [[Paper]](https://arxiv.org/pdf/2310.08949.pdf), [[Code]](https://github.com/zxy556677/EasyGen)

- (arXiv 2023.10) Open X-Embodiment: **Robotic Learning** Datasets and RT-X Models, [[Paper]](https://arxiv.org/pdf/2310.08864.pdf), [[Project]](https://robotics-transformer-x.github.io/)

- (arXiv 2023.10) Large Language Models Meet **Open-World Intent** Discovery and Recognition: An Evaluation of ChatGPT, [[Paper]](https://arxiv.org/pdf/2310.10176.pdf), [[Code]](https://github.com/songxiaoshuai/OOD-Evaluation)

- (arXiv 2023.10) Lost in Translation: When GPT-4V(ision) Can’t See Eye to Eye with Text A **Vision-Language-Consistency** Analysis of VLLMs and Beyond, [[Paper]](https://arxiv.org/pdf/2310.12520.pdf)

- (arXiv 2023.10) Interactive **Navigation** in Environments with Traversable Obstacles Using Large Language and Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2310.08873.pdf)

- (arXiv 2023.10) VLIS: Unimodal Language Models Guide **Multimodal** **Language Generation**, [[Paper]](https://arxiv.org/pdf/2310.09767.pdf), [[Code]](https://github.com/JiwanChung/vlis)

- (arXiv 2023.10) CLIN: A CONTINUALLY LEARNING LANGUAGE **AGENT** FOR RAPID TASK ADAPTATION AND GENERALIZATION, [[Paper]](https://arxiv.org/pdf/2310.10134.pdf), [[Project]](https://allenai.github.io/clin/)

- (arXiv 2023.10) **Navigation** with Large Language Models: Semantic Guesswork as a Heuristic for Planning, [[Paper]](https://arxiv.org/pdf/2310.10103.pdf)

- (arXiv 2023.10) Lost in Translation: When GPT-4V(ision) Can’t See Eye to Eye with Text A **Vision-Language-Consistency** Analysis of VLLMs and Beyond, [[Paper]](https://arxiv.org/pdf/2310.12520.pdf)

- (arXiv 2023.10) FROZEN TRANSFORMERS IN **LANGUAGE** MODELS ARE EFFECTIVE **VISUAL** ENCODER LAYERS, [[Paper]](https://arxiv.org/pdf/2310.12973.pdf), [[Code]](https://github.com/ziqipang/LM4VisualEncoding)

- (arXiv 2023.10) CLAIR: Evaluating **Image Captions** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.12971.pdf), [[Project]](https://davidmchan.github.io/clair/)

- (arXiv 2023.10) 3D-GPT: PROCEDURAL **3D MODELING** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.12945.pdf), [[Project]](https://chuny1.github.io/3DGPT/3dgpt.html)

- (arXiv 2023.10) Automated Natural Language **Explanation** of Deep Visual Neurons with Large Models, [[Paper]](https://arxiv.org/pdf/2310.10708.pdf)

- (arXiv 2023.10) Set-of-Mark Prompting Unleashes Extraordinary **Visual Grounding** in GPT-4V, [[Paper]](https://arxiv.org/pdf/2310.11441.pdf), [[Project]](https://som-gpt4v.github.io/)

- (arXiv 2023.10) EvalCrafter: **Benchmarking** and Evaluating Large **Video Generation** Models, [[Paper]](https://arxiv.org/pdf/2310.11440.pdf), [[Project]](https://evalcrafter.github.io/)

- (arXiv 2023.10) MISAR: A **MULTIMODAL** INSTRUCTIONAL SYSTEM WITH **AUGMENTED REALITY**, [[Paper]](https://arxiv.org/pdf/2310.11699.pdf), [[Code]](https://github.com/nguyennm1024/misar)

- (arXiv 2023.10) NON-INTRUSIVE ADAPTATION: INPUT-CENTRIC PARAMETER-**EFFICIENT** **FINE-TUNING** FOR VERSATILE **MULTIMODAL** MODELING, [[Paper]](https://arxiv.org/pdf/2310.12100.pdf)

- (arXiv 2023.10) LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for **Robotic** Tabletop **Manipulation**, [[Paper]](https://arxiv.org/pdf/2310.12020.pdf), [[Project]](https://shengqiang-zhang.github.io/lohoravens-webpage/)

- (arXiv 2023.10) ChatGPT-guided Semantics for **Zero-shot** Learning, [[Paper]](https://arxiv.org/pdf/2310.11657.pdf)

- (arXiv 2023.10) On the Benefit of Generative Foundation Models for Human **Activity Recognition**, [[Paper]](https://arxiv.org/pdf/2310.12085.pdf)

- (arXiv 2023.10) DiagrammerGPT: **Generating** Open-Domain, Open-Platform **Diagrams** via LLM Planning, [[Paper]](https://arxiv.org/pdf/2310.12128.pdf), [[Project]](https://diagrammergpt.github.io/)

- (arXiv 2023.10) Interactive Task **Planning** with Language Models, [[Paper]](https://arxiv.org/pdf/2310.10645.pdf), [[Project]](https://wuphilipp.github.io/itp_site/)

- (arXiv 2023.10) Bootstrap Your Own **Skills**: Learning to Solve New Tasks with Large Language Model Guidance, [[Paper]](https://arxiv.org/pdf/2310.10021.pdf), [[Project]](https://clvrai.github.io/boss/)

- (arXiv 2023.10) Penetrative AI: Making LLMs Comprehend the **Physical** World, [[Paper]](https://arxiv.org/pdf/2310.09605.pdf)

- (arXiv 2023.10) **BONGARD**-OPENWORLD: FEW-SHOT REASONING FOR FREE-FORM VISUAL CONCEPTS IN THE REAL WORLD, [[Paper]](https://arxiv.org/pdf/2310.10207.pdf), [[Project]](https://joyjayng.github.io/Bongard-OpenWorld.github.io/)

- (arXiv 2023.10) ViPE: **Visualise** Pretty-much Everything, [[Paper]](https://arxiv.org/pdf/2310.10543.pdf)

- (arXiv 2023.10) MINIGPT-V2: LARGE LANGUAGE MODEL AS A UNIFIED INTERFACE FOR **VISION-LANGUAGE** MULTITASK LEARNING, [[Paper]](https://arxiv.org/pdf/2310.09478.pdf), [[Project]](https://minigpt-v2.github.io/)

- (arXiv 2023.10) MoConVQ: Unified Physics-Based **Motion Control** via Scalable Discrete Representations, [[Paper]](https://arxiv.org/pdf/2310.10198.pdf)

- (arXiv 2023.10) LLM BLUEPRINT: ENABLING **TEXT-TO-IMAGE** GENERATION WITH COMPLEX AND DETAILED PROMPTS, [[Paper]](https://arxiv.org/pdf/2310.10640.pdf)

- (arXiv 2023.10) VIDEO LANGUAGE **PLANNING**, [[Paper]](https://arxiv.org/pdf/2310.10625.pdf), [[Project]](https://video-language-planning.github.io/)

- (arXiv 2023.10) Dobby: A Conversational Service **Robot** Driven by GPT-4, [[Paper]](https://arxiv.org/pdf/2310.06303.pdf)

- (arXiv 2023.10) CoPAL: Corrective **Planning** of **Robot** Actions with Large Language Models, [[Paper]](https://arxiv.org/pdf/2310.07263.pdf)

- (arXiv 2023.10) Forgetful Large Language Models: Lessons Learned from Using LLMs in **Robot Programming**, [[Paper]](https://arxiv.org/pdf/2310.06646.pdf)

- (arXiv 2023.10) TREE-PLANNER: EFFICIENT CLOSE-LOOP TASK **PLANNING** WITH LARGE LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.08582.pdf), [[Project]](https://tree-planner.github.io/)

- (arXiv 2023.10) TOWARDS ROBUST **MULTI-MODAL** REASONING VIA MODEL SELECTION, [[Paper]](https://arxiv.org/pdf/2310.08446.pdf), [[Code]](https://github.com/LINs-lab/M3)

- (arXiv 2023.10) FERRET: REFER AND **GROUND** ANYTHING ANYWHERE AT ANY GRANULARITY, [[Paper]](https://arxiv.org/pdf/2310.07704.pdf), [[Code]](https://github.com/apple/ml-ferret)

- (arXiv 2023.10) FROM SCARCITY TO EFFICIENCY: IMPROVING **CLIP TRAINING** VIA VISUAL-ENRICHED **CAPTIONS**, [[Paper]](https://arxiv.org/pdf/2310.07699.pdf)

- (arXiv 2023.10) OPENLEAF: OPEN-DOMAIN INTERLEAVED **IMAGE-TEXT** GENERATION AND EVALUATION, [[Paper]](https://arxiv.org/pdf/2310.07749.pdf)

- (arXiv 2023.10) Can We **Edit** **Multimodal** Large Language Models? [[Paper]](https://arxiv.org/pdf/2310.08475.pdf), [[Code]](https://github.com/zjunlp/EasyEdit)

- (arXiv 2023.10) VISUAL **DATA-TYPE** UNDERSTANDING DOES NOT EMERGE FROM SCALING VISION-LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2310.08577.pdf), [[Code]](https://github.com/bethgelab/DataTypeIdentification)

- (arXiv 2023.10) Idea2Img: Iterative Self-Refinement with GPT-4V(vision) for Automatic **Image Design and Generation**, [[Paper]](https://arxiv.org/pdf/2310.08541.pdf), [[Project]](https://idea2img.github.io/)

- (arXiv 2023.10) OCTOPUS: **EMBODIED** VISION-LANGUAGE PROGRAMMER FROM ENVIRONMENTAL FEEDBACK, [[Paper]](https://arxiv.org/pdf/2310.08588.pdf), [[Project]](https://choiszt.github.io/Octopus/)

### 2023.9

<!-- - (arXiv 2023.9) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.9) LMEye: An **Interactive** Perception Network for Large Language Models, [[Paper]](https://arxiv.org/pdf/2305.03701.pdf), [[Code]](https://github.com/YunxinLi/LingCloud)

- (arXiv 2023.9) DynaCon: Dynamic **Robot Planner** with Contextual Awareness via LLMs, [[Paper]](https://arxiv.org/pdf/2309.16031.pdf), [[Project]](https://sites.google.com/view/dynacon)

- (arXiv 2023.9) AnyMAL: An Efficient and Scalable **Any-Modality** Augmented Language Model, [[Paper]](https://arxiv.org/pdf/2309.16058.pdf)

- (arXiv 2023.9) ConceptGraphs: Open-Vocabulary **3D Scene Graphs** for Perception and **Planning**, [[Paper]](https://arxiv.org/pdf/2309.16650.pdf), [[Project]](https://concept-graphs.github.io/)

- (arXiv 2023.9) LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic **Object Rearrangement**, [[Paper]](https://arxiv.org/pdf/2309.15821.pdf), [[Code]](https://github.com/changhaonan/LG-MCTS)

- (arXiv 2023.9) ONE FOR ALL: **VIDEO CONVERSATION** IS FEASIBLE WITHOUT VIDEO INSTRUCTION TUNING, [[Paper]](https://arxiv.org/pdf/2309.15785.pdf)

- (arXiv 2023.9) Verifiable Learned Behaviors via **Motion Primitive Composition**: Applications to Scooping of Granular Media, [[Paper]](https://arxiv.org/pdf/2309.14894.pdf)

- (arXiv 2023.9) Human-Assisted Continual **Robot** Learning with Foundation Models, [[Paper]](https://arxiv.org/pdf/2309.14321.pdf), [[Project]](https://sites.google.com/mit.edu/halp-robot-learning)

- (arXiv 2023.9) InternLM-XComposer: A **Vision-Language** Large Model for Advanced Text-image Comprehension and Composition, [[Paper]](https://arxiv.org/pdf/2309.15112.pdf), [[Code]](https://github.com/InternLM/InternLM-XComposer)

- (arXiv 2023.9) VIDEODIRECTORGPT: CONSISTENT MULTI-SCENE **VIDEO GENERATION** VIA LLM-GUIDED PLANNING, [[Paper]](https://arxiv.org/pdf/2309.15091.pdf), [[Project]](https://videodirectorgpt.github.io/)

- (arXiv 2023.9) **Text-to-Image** Generation for Abstract Concepts, [[Paper]](https://arxiv.org/pdf/2309.14623.pdf)

- (arXiv 2023.9) Free-Bloom: Zero-Shot **Text-to-Video** Generator with LLM Director and LDM Animator, [[Paper]](https://arxiv.org/pdf/2309.14494.pdf), [[Code]](https://github.com/SooLab/Free-Bloom)

- (arXiv 2023.9) ALIGNING LARGE **MULTIMODAL** MODELS WITH FACTUALLY AUGMENTED **RLHF**, [[Paper]](https://arxiv.org/pdf/2309.14525.pdf), [[Project]](https://llava-rlhf.github.io/)

- (arXiv 2023.9) Self-Recovery **Prompting**: Promptable General Purpose Service **Robot** System with Foundation Models and Self-Recovery, [[Paper]](https://arxiv.org/pdf/2309.14425.pdf), [[Project]](https://sites.google.com/view/srgpsr)

- (arXiv 2023.9) Q-BENCH: A BENCHMARK FOR GENERAL-PURPOSE FOUNDATION MODELS ON **LOW-LEVEL VISION**, [[Paper]](https://arxiv.org/pdf/2309.14181.pdf)

- (arXiv 2023.9) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave **Chat** via **Multi-Modal** Causal Attention, [[Paper]](https://arxiv.org/pdf/2309.14327.pdf), [[Code]](https://github.com/microsoft/DeepSpeedExamples)

- (arXiv 2023.9) LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set **Object Recognition**, [[Paper]](https://arxiv.org/pdf/2309.12780.pdf), [[Code]](https://github.com/Harryqu123/LMC)

- (arXiv 2023.9) LLM-Grounder: Open-Vocabulary **3D** Visual **Grounding** with Large Language Model as an Agent, [[Paper]](https://arxiv.org/pdf/2309.12311.pdf), [[Project]](https://chat-with-nerf.github.io/)

- (arXiv 2023.9) Video-ChatGPT: Towards Detailed **Video Understanding** via Large Vision and Language Models, [[Paper]](https://arxiv.org/pdf/2306.05424.pdf), [[Code]](https://github.com/mbzuai-oryx/Video-ChatGPT)

- (arXiv 2023.9) STRUCTCHART: PERCEPTION, STRUCTURING, REASONING FOR **VISUAL CHART** UNDERSTANDING, [[Paper]](https://arxiv.org/pdf/2309.11268.pdf)

- (arXiv 2023.9) DREAMLLM: SYNERGISTIC **MULTIMODAL** COMPREHENSION AND CREATION, [[Paper]](https://arxiv.org/pdf/2309.11499.pdf), [[Project]](https://dreamllm.github.io/)

- (arXiv 2023.9) A LARGE-SCALE DATASET FOR **AUDIO-LANGUAGE** REPRESENTATION LEARNING, [[Paper]](https://arxiv.org/pdf/2309.11500.pdf), [[Project]](https://auto-acd.github.io/)

- (arXiv 2023.9) YOU ONLY LOOK AT SCREENS: **MULTIMODAL** CHAIN-OF-ACTION **AGENTS**, [[Paper]](https://arxiv.org/pdf/2309.11436.pdf), [[Code]](https://github.com/cooelf/Auto-UI)

- (arXiv 2023.9) SMART-LLM: Smart Multi-Agent Robot Task **Planning** using Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.10062.pdf), [[Project]](https://sites.google.com/view/smart-llm/)

- (arXiv 2023.9) Conformal Temporal Logic **Planning** using Large Language Models: Knowing When to Do What and When to Ask for Help, [[Paper]](https://arxiv.org/pdf/2309.10092.pdf), [[Project]](https://ltl-llm.github.io/)

- (arXiv 2023.9) Investigating the **Catastrophic Forgetting** in **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.10313.pdf)

- (arXiv 2023.9) Specification-Driven **Video Search** via Foundation Models and Formal Verification, [[Paper]](https://arxiv.org/pdf/2309.10171.pdf)

- (arXiv 2023.9) Language as the Medium: Multimodal **Video Classification** through text only, [[Paper]](https://arxiv.org/pdf/2309.10783.pdf)

- (arXiv 2023.9) **Multimodal** Foundation Models: From Specialists to General-Purpose Assistants, [[Paper]](https://arxiv.org/pdf/2309.10020.pdf)

- (arXiv 2023.9) TEXTBIND: Multi-turn Interleaved **Multimodal** **Instruction**-following, [[Paper]](https://arxiv.org/pdf/2309.08637.pdf), [[Project]](https://textbind.github.io/)

- (arXiv 2023.9) Prompt a **Robot** to **Walk** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.09969.pdf), [[Project]](https://prompt2walk.github.io/)

- (arXiv 2023.9) Grasp-Anything: Large-scale **Grasp** Dataset from Foundation Models, [[Paper]](https://arxiv.org/pdf/2309.09818.pdf), [[Project]](https://grasp-anything-2023.github.io/)

- (arXiv 2023.9) MMICL: EMPOWERING **VISION-LANGUAGE** MODEL WITH MULTI-MODAL IN-CONTEXT LEARNING, [[Paper]](https://arxiv.org/pdf/2309.07915.pdf), [[Code]](https://github.com/HaozheZhao/MIC)

- (arXiv 2023.9) SwitchGPT: Adapting Large Language Models for **Non-Text Outputs**, [[Paper]](https://arxiv.org/pdf/2309.07623.pdf), [[Code]](https://github.com/xinke-wang/SwitchGPT)

- (arXiv 2023.9) UNIFIED **HUMAN-SCENE INTERACTION** VIA PROMPTED CHAIN-OF-CONTACTS, [[Paper]](https://arxiv.org/pdf/2309.07918.pdf), [[Code]](https://github.com/OpenRobotLab/UniHSI)

- (arXiv 2023.9) Incremental Learning of **Humanoid Robot Behavior** from Natural Interaction and Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.04316.pdf)

- (arXiv 2023.9) NExT-GPT: Any-to-Any **Multimodal** LLM, [[Paper]](https://arxiv.org/pdf/2309.05519.pdf), [[Project]](https://next-gpt.github.io/)

- (arXiv 2023.9) Multi3DRefer: **Grounding** Text Description to Multiple **3D** Objects, [[Paper]](https://arxiv.org/pdf/2309.05251.pdf), [[Project]](https://3dlg-hcvc.github.io/multi3drefer/#/)

- (arXiv 2023.9) Language Models as Black-Box **Optimizers** for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2309.05950.pdf)

- (arXiv 2023.9) Evaluation and Mitigation of Agnosia in **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.04041.pdf)

- (arXiv 2023.9) Measuring and Improving **Chain-of-Thought** Reasoning in Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2309.04461.pdf), [[Code]](https://github.com/Yangyi-Chen/CoTConsistency)

- (arXiv 2023.9) Context-Aware **Prompt Tuning** for Vision-Language Model with Dual-Alignment, [[Paper]](https://arxiv.org/pdf/2309.04158.pdf)

- (arXiv 2023.9) ImageBind-LLM: **Multi-modality Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2309.03905.pdf), [[Code]](https://github.com/OpenGVLab/LLaMA-Adapter)

- (arXiv 2023.9) Developmental **Scaffolding** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2309.00904.pdf)

- (arXiv 2023.9) Gesture-Informed **Robot Assistance** via Foundation Models, [[Paper]](https://arxiv.org/pdf/2309.02721.pdf), [[Project]](https://sites.google.com/view/giraf23)

- (arXiv 2023.9) Zero-Shot **Recommendations** with Pre-Trained Large Language Models for Multimodal Nudging, [[Paper]](https://arxiv.org/pdf/2309.01026.pdf)

- (arXiv 2023.9) Large AI Model Empowered Multimodal Semantic Communications, [[Paper]](https://arxiv.org/pdf/2309.01249.pdf)

- (arXiv 2023.9) CoTDet: **Affordance** Knowledge Prompting for Task Driven Object **Detection**, [[Paper]](https://arxiv.org/pdf/2309.01093.pdf), [[Project]](https://toneyaya.github.io/cotdet/)

- (arXiv 2023.9) Scaling Autoregressive **Multi-Modal** Models: Pretraining and Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2309.02591.pdf)

- (arXiv 2023.9) CIEM: Contrastive Instruction Evaluation Method for Better **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2309.02301.pdf)

- (arXiv 2023.9) Point-Bind & Point-LLM: Aligning **Point Cloud** with **Multi-modality** for 3D Understanding, Generation, and Instruction Following, [[Paper]](https://arxiv.org/pdf/2309.00615.pdf), [[Code]](https://github.com/ZiyuGuo99/Point-Bind_Point-LLM)

### 2023.8

<!-- - (arXiv 2023.8) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.8) EVE: Efficient **Vision-Language** Pre-training with Masked Prediction and Modality-Aware MoE, [[Paper]](https://arxiv.org/pdf/2308.11971.pdf)

- (arXiv 2023.8) Breaking Common Sense: WHOOPS! A **Vision-and-Language Benchmark** of Synthetic and Compositional Images, [[Paper]](https://arxiv.org/pdf/2303.07274.pdf), [[Project]](https://whoops-benchmark.github.io/)

- (arXiv 2023.8) Improving Knowledge Extraction from LLMs for Task Learning through **Agent** Analysis, [[Paper]](https://arxiv.org/pdf/2306.06770.pdf)

- (arXiv 2023.8) Sparkles: Unlocking Chats Across Multiple Images for **Multimodal Instruction**-Following Models, [[Paper]](https://arxiv.org/pdf/2308.16463.pdf), [[Code]](https://github.com/HYPJUDY/Sparkles)

- (arXiv 2023.8) PointLLM: Empowering Large Language Models to Understand **Point Clouds**, [[Paper]](https://arxiv.org/pdf/2308.16911.pdf), [[Project]](https://runsenxu.com/projects/PointLLM/)

- (arXiv 2023.8) TouchStone: **Evaluating Vision-Language** Models by Language Models, [[Paper]](https://arxiv.org/pdf/2308.16890.pdf), [[Code]](https://github.com/OFA-Sys/TouchStone)

- (arXiv 2023.8) WALL-E: Embodied **Robotic** WAiter Load Lifting with Large Language Model, [[Paper]](https://arxiv.org/pdf/2308.15962.pdf)

- (arXiv 2023.8) ISR-LLM: Iterative Self-Refined Large Language Model for **Long-Horizon Sequential Task Planning**, [[Paper]](https://arxiv.org/pdf/2308.13724.pdf), [[Code]](https://github.com/zhehuazhou/ISR-LLM)

- (arXiv 2023.8) LLM-Based **Human-Robot Collaboration** Framework for Manipulation Tasks, [[Paper]](https://arxiv.org/pdf/2308.14972.pdf)

- (arXiv 2023.8) Evaluation and Analysis of **Hallucination** in Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2308.15126.pdf)

- (arXiv 2023.8) MLLM-**DataEngine**: An Iterative Refinement Approach for MLLM, [[Paper]](https://arxiv.org/pdf/2308.13566.pdf)

- (arXiv 2023.8) Position-Enhanced **Visual Instruction** Tuning for Multimodal Large Language Models, [[Paper]](https://arxiv.org/pdf/2308.13437.pdf)

- (arXiv 2023.8) Can Linguistic Knowledge Improve Multimodal Alignment in **Vision-Language** Pretraining? [[Paper]](https://arxiv.org/pdf/2308.12898.pdf), [[Code]](https://github.com/WangFei-2019/SNARE/)

- (arXiv 2023.8) VIGC: **Visual Instruction** Generation and Correction, [[Paper]](https://arxiv.org/pdf/2308.12714.pdf)

- (arXiv 2023.8) Towards Realistic **Zero-Shot** Classification via Self Structural Semantic Alignment, [[Paper]](https://arxiv.org/pdf/2308.12960.pdf)

- (arXiv 2023.8) Qwen-VL: A Frontier Large **Vision-Language** Model with Versatile Abilities, [[Paper]](https://arxiv.org/pdf/2308.12966.pdf), [[Code]](https://github.com/QwenLM/Qwen-VL)

- (arXiv 2023.8) **DIFFUSION** **LANGUAGE MODELS** CAN PERFORM MANY TASKS WITH SCALING AND INSTRUCTION-FINETUNING, [[Paper]](https://arxiv.org/pdf/2308.12219.pdf), [[Code]](https://github.com/yegcjs/DiffusionLLM)

- (arXiv 2023.8) CHORUS: Learning Canonicalized **3D Human-Object** Spatial **Relations** from Unbounded Synthesized Images, [[Paper]](https://arxiv.org/pdf/2308.12288.pdf), [[Project]](https://jellyheadandrew.github.io/projects/chorus)

- (arXiv 2023.8) Pro**Agent**: Building Proactive Cooperative AI with Large Language Models, [[Paper]](https://arxiv.org/pdf/2308.11339.pdf), [[Project]](https://pku-proagent.github.io/)

- (arXiv 2023.8) ROSGPT_Vision: Commanding **Robots** Using Only Language Models’ Prompts, [[Paper]](https://arxiv.org/pdf/2308.11236.pdf), [[Code]](https://github.com/bilel-bj/ROSGPT_Vision)

- (arXiv 2023.8) StoryBench: A Multifaceted Benchmark for Continuous **Story Visualization**, [[Paper]](https://arxiv.org/pdf/2308.11606.pdf), [[Code]](https://github.com/google/storybench)

- (arXiv 2023.8) Tackling Vision Language Tasks Through Learning **Inner Monologues**, [[Paper]](https://arxiv.org/pdf/2308.09970.pdf)

- (arXiv 2023.8) ExpeL: LLM Agents Are **Experiential Learners**, [[Paper]](https://arxiv.org/pdf/2308.10144.pdf)

- (arXiv 2023.8) On the Adversarial **Robustness** of **Multi-Modal** Foundation Models, [[Paper]](https://arxiv.org/pdf/2308.10741.pdf)

- (arXiv 2023.8) WanJuan: A Comprehensive **Multimodal** **Dataset** for Advancing English and Chinese Large Models, [[Paper]](https://arxiv.org/pdf/2308.10755.pdf), [[Project]](https://opendatalab.org.cn/WanJuan1.0)

- (arXiv 2023.8) March in Chat: Interactive **Prompting** for Remote **Embodied** Referring Expression, [[Paper]](https://arxiv.org/pdf/2308.10141.pdf), [[Code]](https://github.com/YanyuanQiao/MiC)

- (arXiv 2023.8) BLIVA: A Simple **Multimodal** LLM for Better Handling of Text-Rich Visual Questions, [[Paper]](https://arxiv.org/pdf/2308.09936.pdf), [[Code]](https://github.com/mlpc-ucsd/BLIVA.git)

- (arXiv 2023.8) VIT-LENS: Towards **Omni-modal** Representations, [[Paper]](https://arxiv.org/pdf/2308.10185.pdf), [[Code]](https://github.com/TencentARC/ViT-Lens)

- (arXiv 2023.8) StableLLaVA: Enhanced **Visual Instruction Tuning** with Synthesized Image-Dialogue Data, [[Paper]](https://arxiv.org/pdf/2308.10253.pdf), [[Project]](https://github.com/icoz69/StableLLAVA)

- (arXiv 2023.8) PUMGPT: A Large Vision-Language Model for **Product Understanding**, [[Paper]](https://arxiv.org/pdf/2308.09568.pdf)

- (arXiv 2023.8) Link-Context Learning for **Multimodal** LLMs, [[Paper]](https://arxiv.org/pdf/2308.07891.pdf), [[Code]](https://github.com/isekai-portal/Link-Context-Learning)

- (arXiv 2023.8) Detecting and Preventing **Hallucinations** in Large Vision Language Models, [[Paper]](https://arxiv.org/pdf/2308.06394.pdf)

- (arXiv 2023.8) VisIT-Bench: A **Benchmark** for **Vision-Language Instruction** Following Inspired by Real-World Use, [[Paper]](https://arxiv.org/pdf/2308.06595.pdf), [[Project]](https://visit-bench.github.io/)

- (arXiv 2023.8) Foundation Model based Open Vocabulary Task Planning and Executive System for General Purpose **Service Robots**, [[Paper]](https://arxiv.org/pdf/2308.03357.pdf)

- (arXiv 2023.8) LayoutLLM-T2I: Eliciting **Layout** Guidance from LLM for **Text-to-Image** Generation, [[Paper]](https://arxiv.org/pdf/2308.05095.pdf), [[Project]](https://layoutllm-t2i.github.io/)

- (arXiv 2023.8) OmniDataComposer: A Unified Data Structure for Multimodal **Data Fusion** and Infinite Data **Generation**, [[Paper]](https://arxiv.org/pdf/2308.04126.pdf)

- (arXiv 2023.8) EMPOWERING VISION-LANGUAGE MODELS TO FOLLOW INTERLEAVED **VISION-LANGUAGE** INSTRUCTIONS, [[Paper]](https://arxiv.org/pdf/2308.04152.pdf), [[Code]](https://github.com/DCDmllm/Cheetah)

- (arXiv 2023.8) 3D-VisTA: Pre-trained Transformer for **3D** Vision and **Text** Alignment, [[Paper]](https://arxiv.org/pdf/2308.04352.pdf), [[Project]](https://3d-vista.github.io/)

- (arXiv 2023.8) Gentopia.AI: A **Collaborative Platform** for **Tool**-Augmented LLMs, [[Paper]](https://arxiv.org/pdf/2308.04030.pdf), [[Project]](https://github.com/Gentopia-AI)

- (arXiv 2023.8) AgentBench: Evaluating LLMs as **Agents**, [[Paper]](https://arxiv.org/pdf/2308.03688.pdf), [[Project]](https://llmbench.ai/)

- (arXiv 2023.8) Learning Concise and Descriptive **Attributes** for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2308.03685.pdf)

- (arXiv 2023.8) Tiny LVLM-eHub: Early Multimodal Experiments with **Bard**, [[Paper]](https://arxiv.org/pdf/2308.03729.pdf), [[Project]](http://lvlm-ehub.opengvlab.com/)

- (arXiv 2023.8) MM-Vet: **Evaluating** Large **Multimodal** Models for Integrated Capabilities, [[Paper]](https://arxiv.org/pdf/2308.02490.pdf), [[Code]](https://github.com/yuweihao/MM-Vet)

- (arXiv 2023.8) RegionBLIP: A Unified **Multi-modal Pre-training** Framework for Holistic and Regional Comprehension, [[Paper]](https://arxiv.org/pdf/2308.02299.pdf), [[Code]](https://github.com/mightyzau/RegionBLIP)

- (arXiv 2023.8) Learning to **Model** the **World** with Language, [[Paper]](https://arxiv.org/pdf/2308.01399.pdf), [[Project]](https://dynalang.github.io/)

- (arXiv 2023.8) The All-Seeing Project: Towards Panoptic **Visual Recognition** and Understanding of the Open World, [[Paper]](https://arxiv.org/pdf/2308.01907.pdf), [[Code]](https://github.com/OpenGVLab/all-seeing)

- (arXiv 2023.8) Multimodal **Neurons** in Pretrained Text-Only Transformers, [[Paper]](https://arxiv.org/pdf/2308.01544.pdf), [[Project]](https://mmns.csail.mit.edu/)

- (arXiv 2023.8) LISA: REASONING **SEGMENTATION** VIA LARGE LANGUAGE MODEL, [[Paper]](https://arxiv.org/pdf/2308.00692.pdf), [[Code]](https://github.com/dvlab-research/LISA)

### 2023.7

<!-- - (arXiv 2023.7) , [[Paper]](), [[Code]]()-->

- (arXiv 2023.7) **Caption** Anything: Interactive Image Description with Diverse Multimodal Controls, [[Paper]](https://arxiv.org/pdf/2305.02677.pdf), [[Code]](https://github.com/ttengwang/Caption-Anything)

- (arXiv 2023.7) DesCo: Learning Object **Recognition** with Rich Language Descriptions, [[Paper]](https://arxiv.org/pdf/2306.14060.pdf)

- (arXiv 2023.7) KOSMOS-2: **Grounding** **Multimodal** Large Language Models to the World, [[Paper]](https://arxiv.org/pdf/2306.14824.pdf), [[Project]](https://thegenerality.com/agi/)

- (arXiv 2023.7) MME: A Comprehensive Evaluation **Benchmark** for **Multimodal** Large Language Models, [[Paper]](https://arxiv.org/pdf/2306.13394.pdf), [[Code]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)

- (arXiv 2023.7) Evaluating ChatGPT and GPT-4 for **Visual Programming**, [[Paper]](https://arxiv.org/pdf/2308.02522.pdf)

- (arXiv 2023.7) SEED-Bench: **Benchmarking** **Multimodal** LLMs with Generative Comprehension, [[Paper]](https://arxiv.org/pdf/2307.16125.pdf), [[Code]](https://github.com/AILab-CVC/SEED-Bench)

- (arXiv 2023.7) AntGPT: Can Large Language Models Help **Long-term Action Anticipation** from Videos? [[Paper]](https://arxiv.org/pdf/2307.16368.pdf), [[Project]](https://brown-palm.github.io/AntGPT/)

- (arXiv 2023.7) Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex **Visual Reasoning** Tasks, [[Paper]](https://arxiv.org/pdf/2307.16395.pdf)

- (arXiv 2023.7) MovieChat: From Dense Token to Sparse Memory for **Long Video Understanding**, [[Paper]](https://arxiv.org/pdf/2307.16449.pdf), [[Project]](https://rese1f.github.io/MovieChat/)

- (arXiv 2023.7) Large Language Models as **General Pattern Machines**, [[Paper]](https://arxiv.org/pdf/2307.04721.pdf), [[Project]](https://general-pattern-machines.github.io/)

- (arXiv 2023.7) How Good is Google **Bard**’s **Visual Understanding**? An Empirical Study on Open Challenges, [[Paper]](https://arxiv.org/pdf/2307.15016.pdf), [[Project]](https://github.com/htqin/GoogleBard-VisUnderstand)

- (arXiv 2023.7) RT-2: **Vision-Language-Action** Models Transfer Web Knowledge to **Robotic Control**, [[Paper]](https://robotics-transformer2.github.io/assets/rt2.pdf), [[Project]](https://robotics-transformer2.github.io/)

- (arXiv 2023.7) Scaling Up and Distilling Down: Language-Guided **Robot Skill Acquisition**, [[Paper]](https://arxiv.org/pdf/2307.14535.pdf), [[Project]](https://www.cs.columbia.edu/~huy/scalingup/)

- (arXiv 2023.7) GraspGPT: Leveraging Semantic Knowledge from a Large Language Model for Task-Oriented **Grasping**, [[Paper]](https://arxiv.org/pdf/2307.13204.pdf), [[Project]](https://sites.google.com/view/graspgpt/)

- (arXiv 2023.7) CARTIER: Cartographic lAnguage Reasoning Targeted at **Instruction Execution** for **Robots**, [[Paper]](https://arxiv.org/pdf/2307.11865.pdf)

- (arXiv 2023.7) **3D-LLM**: Injecting the 3D World into Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.12981.pdf), [[Project]](https://vis-www.cs.umass.edu/3dllm/)

- (arXiv 2023.7) Generative **Pretraining** in **Multimodality**, [[Paper]](https://arxiv.org/pdf/2307.05222.pdf), [[Code]](https://github.com/baaivision/Emu)

- (arXiv 2023.7) VoxPoser: Composable 3D Value Maps for **Robotic Manipulation** with Language Models, [[Paper]](https://arxiv.org/pdf/2307.05973.pdf), [[Project]](https://voxposer.github.io/)

- (arXiv 2023.7) VELMA: Verbalization Embodiment of LLM Agents for Vision and Language **Navigation** in Street View, [[Paper]](https://arxiv.org/pdf/2307.06082.pdf)

- (arXiv 2023.7) SayPlan: Grounding Large Language Models using **3D Scene Graphs** for Scalable **Task Planning**, [[Paper]](https://arxiv.org/pdf/2307.06135.pdf), [[Project]](https://sayplan.github.io/)

- (arXiv 2023.7) Enhancing **CLIP** with GPT-4: Harnessing Visual Descriptions as **Prompts**, [[Paper]](https://arxiv.org/pdf/2307.11661.pdf)

- (arXiv 2023.7) InternVid: A Large-scale **Video-Text Dataset** for Multimodal Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2307.06942.pdf), [[Data]](https://arxiv.org/pdf/2307.06942.pdf)

- (arXiv 2023.7) MBLIP: EFFICIENT BOOTSTRAPPING OF **MULTILINGUAL VISION-LLMS**, [[Paper]](https://arxiv.org/pdf/2307.06930.pdf), [[Code]](https://github.com/gregor-ge/mBLIP)

- (arXiv 2023.7) Bootstrapping **Vision-Language** Learning with Decoupled Language Pre-training, [[Paper]](https://arxiv.org/pdf/2307.07063.pdf)

- (arXiv 2023.7) BuboGPT: Enabling **Visual Grounding** in Multi-Modal LLMs, [[Paper]](https://arxiv.org/pdf/2307.08581.pdf), [[Project]](https://bubo-gpt.github.io/)

- (arXiv 2023.7) ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2307.09474.pdf), [[Project]](https://chatspot.streamlit.app/)

- (arXiv 2023.7) TOWARDS A **UNIFIED AGENT** WITH FOUNDATION MODELS, [[Paper]](https://arxiv.org/pdf/2307.09668.pdf)

- (arXiv 2023.7) Robots That Ask For Help: **Uncertainty** Alignment for Large Language Model **Planners**, [[Paper]](https://arxiv.org/pdf/2307.01928.pdf), [[Project]](https://robot-help.github.io/)

- (arXiv 2023.7) Building Cooperative **Embodied Agents** Modularly with Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.02485.pdf), [[Project]](https://vis-www.cs.umass.edu/Co-LLM-Agents/)

- (arXiv 2023.7) **Embodied Task Planning** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2307.01848.pdf), [[Project]](https://gary3410.github.io/TaPA/)

- (arXiv 2023.7) What Matters in **Training** a GPT4-Style Language Model with **Multimodal** Inputs?, [[Paper]](https://arxiv.org/pdf/2307.02469.pdf), [[Project]](https://lynx-llm.github.io/)

- (arXiv 2023.7) GPT4RoI: **Instruction Tuning** Large Language Model on **Region-of-Interest**, [[Paper]](https://arxiv.org/pdf/2307.03601.pdf), [[Code]](https://github.com/jshilong/GPT4RoI)

- (arXiv 2023.7) JourneyDB: A Benchmark for **Generative Image Understanding**, [[Paper]](https://arxiv.org/pdf/2307.00716.pdf), [[Code]](https://journeydb.github.io/)

- (arXiv 2023.7) DoReMi: Grounding Language Model by Detecting and Recovering from **Plan-Execution Misalignment**, [[Paper]](https://arxiv.org/pdf/2307.00329.pdf), [[Project]](https://sites.google.com/view/doremi-paper)
  
- (arXiv 2023.7) Motion-X: A Large-scale 3D Expressive Whole-body Human **Motion Dataset**, [[Paper]](https://arxiv.org/pdf/2307.00818.pdf), [[Code]](https://github.com/IDEA-Research/Motion-X)

- (arXiv 2023.7) Visual **Instruction Tuning** with Polite Flamingo, [[Paper]](https://arxiv.org/pdf/2307.01003.pdf), [[Code]](https://github.com/ChenDelong1999/polite_flamingo)

- (arXiv 2023.7) Statler: State-Maintaining Language Models for **Embodied Reasoning**, [[Paper]](https://arxiv.org/pdf/2306.17840.pdf), [[Project]](https://statler-lm.github.io/)

- (arXiv 2023.7) SCITUNE: Aligning Large Language Models with **Scientific Multimodal Instructions**, [[Paper]](https://arxiv.org/pdf/2307.01139.pdf)

- (arXiv 2023.7) SPAE: Semantic **Pyramid** AutoEncoder for **Multimodal Generation** with Frozen LLMs, [[Paper]](https://arxiv.org/pdf/2306.17842.pdf), [[Code]](https://github.com/google-research/magvit/)

- (arXiv 2023.7) KITE: Keypoint-Conditioned **Policies** for **Semantic Manipulation**, [[Paper]](https://arxiv.org/pdf/2306.16605.pdf), [[Project]](https://sites.google.com/view/kite-website/home)

### 2023.6

<!-- - (arXiv 2023.6) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.6) MultiModal-GPT: A **Vision** and **Language** Model for Dialogue with Humans, [[Paper]](https://arxiv.org/pdf/2305.04790.pdf), [[Code]](https://github.com/open-mmlab/Multimodal-GPT)

- (arXiv 2023.6) InternGPT: Solving **Vision**-Centric Tasks by Interacting with ChatGPT Beyond **Language**, [[Paper]](https://arxiv.org/pdf/2305.05662.pdf), [[Code]](https://github.com/OpenGVLab/InternGPT)

- (arXiv 2023.6) InstructBLIP: Towards General-purpose **Vision-Language** Models with Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2305.06500.pdf), [[Code]](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)

- (arXiv 2023.6) LAMM: Language-Assisted **Multi-Modal Instruction-Tuning** Dataset, Framework, and Benchmark, [[Paper]](https://arxiv.org/pdf/2306.06687.pdf), [[Code]](https://github.com/OpenLAMM/LAMM)

- (arXiv 2023.6) Scalable **3D Captioning** with Pretrained Models, [[Paper]](https://arxiv.org/pdf/2306.07279.pdf), [[Code]](https://huggingface.co/datasets/tiange/Cap3D)

- (arXiv 2023.6) AutoTAMP: Autoregressive Task and Motion **Planning** with LLMs as Translators and Checkers, [[Paper]](https://arxiv.org/pdf/2306.06531.pdf), [[Code]](https://github.com/yongchao98/AutoTAMP)

- (arXiv 2023.6) VALLEY: **VIDEO ASSISTANT** WITH LARGE LANGUAGE MODEL ENHANCED ABILITY, [[Paper]](https://arxiv.org/pdf/2306.07207.pdf), [[Code]](https://github.com/RupertLuo/Valley)

- (arXiv 2023.6) Pave the Way to Grasp Anything: Transferring Foundation Models for Universal **Pick-Place Robots**, [[Paper]](https://arxiv.org/pdf/2306.05716.pdf)

- (arXiv 2023.6) LVLM-eHub: A Comprehensive Evaluation **Benchmark** for Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2306.09265.pdf)

- (arXiv 2023.6) AssistGPT: A General **Multi-modal Assistant** that can Plan, Execute, Inspect, and Learn, [[Paper]](https://arxiv.org/pdf/2306.08640.pdf), [[Project]](https://showlab.github.io/assistgpt/)

- (arXiv 2023.6) Towards AGI in Computer Vision: **Lessons** Learned from GPT and Large Language Models, [[Paper]](https://arxiv.org/pdf/2306.08641.pdf)

- (arXiv 2023.6) MACAW-LLM: **MULTI-MODAL** LANGUAGE MODELING WITH IMAGE, AUDIO, VIDEO, AND TEXT INTEGRATION, [[Paper]](https://arxiv.org/pdf/2306.09093.pdf), [[Code]](https://github.com/lyuchenyang/Macaw-LLM)

- (arXiv 2023.6) Investigating Prompting Techniques for Zero- and Few-Shot **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2306.09996.pdf)

- (arXiv 2023.6) Language to Rewards for **Robotic** Skill Synthesis, [[Paper]](https://arxiv.org/pdf/2306.08647.pdf), [[Project]](https://language-to-reward.github.io/)

- (arXiv 2023.6) Toward Grounded **Social** **Reasoning**, [[Paper]](https://arxiv.org/pdf/2306.08651.pdf), [[Code]](https://minaek.github.io/groundedsocialreasoning/)

- (arXiv 2023.6) Improving Image **Captioning** Descriptiveness by Ranking and LLM-based Fusion, [[Paper]](https://arxiv.org/pdf/2306.11593.pdf), [[Code]](https://drive.google.com/drive/folders/1tXbJyvwrEF-a-t-NZbt2xjoKaiGGeaz1?usp=sharing)

- (arXiv 2023.6) RM-PRT: Realistic **Robotic** Manipulation Simulator and **Benchmark** with Progressive Reasoning Tasks, [[Paper]](https://arxiv.org/pdf/2306.11335.pdf), [[Code]](https://necolizer.github.io/RM-PRT/)

- (arXiv 2023.6) Mitigating Hallucination in Large Multi-Modal Models via Robust **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2306.14565.pdf), [[Project]](https://fuxiaoliu.github.io/LRV/)

- (arXiv 2023.6) Towards Language Models That Can See: Computer **Vision** Through the LENS of Natural **Language**, [[Paper]](https://arxiv.org/pdf/2306.16410.pdf), [[Code]](https://arxiv.org/pdf/2306.16410.pdf)

- (arXiv 2023.6) LLaVAR: Enhanced **Visual Instruction Tuning** for Text-Rich Image Understanding, [[Paper]](https://arxiv.org/pdf/2306.17107.pdf), [[Project]](https://llavar.github.io/)

- (arXiv 2023.6) OpenShape: Scaling Up **3D Shape Representation** Towards **Open-World** Understanding, [[Paper]](https://arxiv.org/pdf/2305.10764.pdf), [[Project]](https://colin97.github.io/OpenShape/)

- (arXiv 2023.6) Statler: State-Maintaining Language Models for **Embodied Reasoning**, [[Paper]](https://arxiv.org/pdf/2306.17840.pdf), [[Project]](https://statler-lm.github.io/)

- (arXiv 2023.6) CLARA: Classifying and Disambiguating User Commands for Reliable **Interactive Robotic** Agents, [[Paper]](https://arxiv.org/pdf/2306.10376.pdf)

- (arXiv 2023.6) **Mass-Producing Failures** of **Multimodal** Systems with Language Models, [[Paper]](https://arxiv.org/pdf/2306.12105.pdf), [[Code]](https://github.com/tsb0601/MultiMon)

- (arXiv 2023.6) SoftGPT: Learn Goal-oriented **Soft Object Manipulation** Skills by Generative Pre-trained Heterogeneous Graph Transformer, [[Paper]](https://arxiv.org/pdf/2306.12677.pdf)

- (arXiv 2023.6) SPRINT: SCALABLE **POLICY PRE-TRAINING** VIA LANGUAGE INSTRUCTION RELABELING, [[Paper]](https://arxiv.org/pdf/2306.11886.pdf), [[Project]](https://clvrai.github.io/sprint/)

- (arXiv 2023.6) MotionGPT: Finetuned LLMs are General-Purpose **Motion Generators**, [[Paper]](https://arxiv.org/pdf/2306.10900.pdf), [[Project]](https://qiqiapink.github.io/MotionGPT/)

- (arXiv 2023.6) MIMIC-IT: **Multi-Modal** **In-Context** Instruction Tuning, [[Paper]](https://arxiv.org/pdf/2306.05425.pdf), [[Code]](https://github.com/Luodian/Otter)

- (arXiv 2023.6) Dense and Aligned **Captions** (DAC) Promote **Compositional Reasoning** in VL Models, [[Paper]](https://arxiv.org/pdf/2305.19595.pdf)

### 2023.5

<!-- - (arXiv 2023.5) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.5) Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-**AIGCs** Collaboration, [[Paper]](https://arxiv.org/pdf/2305.12799.pdf), [[Code]](https://github.com/Yuqifan1117/Labal-Anything-Pipeline)

- (arXiv 2023.5) X-LLM: Bootstrapping Advanced Large Language Models by Treating **Multi-Modalities** as Foreign Languages, [[Paper]](https://arxiv.org/pdf/2305.04160.pdf), [[Project]](https://x-llm.github.io/)

- (arXiv 2023.5) Otter: A **Multi-Modal** Model with In-Context **Instruction** Tuning, [[Paper]](https://arxiv.org/pdf/2305.03726.pdf), [[Code]](https://github.com/Luodian/Otter)

- (arXiv 2023.5) VideoChat: **Chat**-Centric **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2305.06355.pdf), [[Code]](https://github.com/OpenGVLab/Ask-Anything)

- (arXiv 2023.5) Prompting Large Language Models with Answer Heuristics for Knowledge-based **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2303.01903.pdf), [[Code]](https://github.com/MILVLG/prophet)

- (arXiv 2023.5) VIMA: General **Robot Manipulation** with Multimodal Prompts, [[Paper]](https://arxiv.org/pdf/2210.03094.pdf), [[Project]](https://vimalabs.github.io/)

- (arXiv 2023.5) TidyBot: Personalized **Robot Assistance** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2305.05658.pdf), [[Project]](https://tidybot.cs.princeton.edu/)

- (arXiv 2023.5) Training **Diffusion** Models with **Reinforcement Learning**, [[Paper]](https://arxiv.org/pdf/2305.13301.pdf), [[Project]](https://rl-diffusion.github.io/)

- (arXiv 2023.5) EmbodiedGPT: Vision-Language Pre-Training via **Embodied** Chain of Thought, [[Paper]](https://arxiv.org/pdf/2305.15021.pdf), [[Project]](https://embodiedgpt.github.io/)

- (arXiv 2023.5) ArtGPT-4: **Artistic Vision-Language** Understanding with Adapter-enhanced MiniGPT-4, [[Paper]](https://arxiv.org/pdf/2305.07490.pdf), [[Code]](https://huggingface.co/Tyrannosaurus/ArtGPT-4)

- (arXiv 2023.5) Evaluating **Object Hallucination** in Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2305.10355.pdf), [[Code]](https://github.com/RUCAIBox/POPE)

- (arXiv 2023.5) LLMScore: Unveiling the Power of Large Language Models in **Text-to-Image Synthesis Evaluation**, [[Paper]](https://arxiv.org/pdf/2305.11116.pdf), [[Code]](https://github.com/YujieLu10/LLMScore)

- (arXiv 2023.5) VisionLLM: Large Language Model is also an Open-Ended **Decoder** for **Vision**-Centric Tasks, [[Paper]](https://arxiv.org/pdf/2305.11175.pdf), [[Code]](https://github.com/OpenGVLab/VisionLLM)

- (arXiv 2023.5) OpenShape: Scaling Up **3D Shape Representation** Towards **Open-World** Understanding, [[Paper]](https://arxiv.org/pdf/2305.10764.pdf), [[Project]](https://colin97.github.io/OpenShape/)

- (arXiv 2023.5) Towards A Foundation Model for Generalist **Robots**: Diverse **Skill Learning** at Scale via Automated Task and Scene Generation, [[Paper]](https://arxiv.org/pdf/2305.10455.pdf)

- (arXiv 2023.5) An Android **Robot Head** as Embodied **Conversational** Agent, [[Paper]](https://arxiv.org/pdf/2305.10945.pdf)

- (arXiv 2023.5) Instruct2Act: Mapping Multi-modality Instructions to **Robotic Actions** with Large Language Model, [[Paper]](https://arxiv.org/pdf/2305.11176.pdf), [[Code]](https://github.com/OpenGVLab/Instruct2Act)

- (arXiv 2023.5) **Principle**-Driven Self-Alignment of Language Models from Scratch with **Minimal Human Supervision**, [[Paper]](https://arxiv.org/pdf/2305.03047.pdf), [[Project]](https://mitibmdemos.draco.res.ibm.com/dromedary)

- (arXiv 2023.5) Multimodal **Procedural Planning** via Dual Text-Image Prompting, [[Paper]](https://arxiv.org/pdf/2305.01795.pdf), [[Code]](https://github.com/YujieLu10/TIP)

- (arXiv 2023.5) ArK: **Augmented Reality** with **Knowledge** Interactive Emergent Ability, [[Paper]](https://arxiv.org/pdf/2305.00970.pdf)

### 2023.4

<!-- - (arXiv 2023.4) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.4) LLaMA-Adapter V2: Parameter-**Efficient** **Visual Instruction** Model, [[Paper]](https://arxiv.org/pdf/2304.15010.pdf), [[Code]](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- (arXiv 2023.4) Multimodal Grounding for **Embodied AI** via **Augmented Reality** Headsets for Natural Language Driven Task Planning, [[Paper]](https://arxiv.org/pdf/2304.13676.pdf)

- (arXiv 2023.4) mPLUG-Owl : **Modularization** Empowers Large Language Models with **Multimodality**, [[Paper]](https://arxiv.org/pdf/2304.14178.pdf), [[Code]](https://github.com/X-PLUG/mPLUG-Owl)

- (arXiv 2023.4) ChatVideo: A Tracklet-centric Multimodal and Versatile **Video Understanding** System, [[Paper]](https://arxiv.org/pdf/2304.14407.pdf), [[Project]](https://www.wangjunke.info/ChatVideo/)

- (arXiv 2023.4) ChatABL: **Abductive Learning** via Natural Language Interaction with ChatGPT, [[Paper]](https://arxiv.org/pdf/2304.11107.pdf)

- (arXiv 2023.4) **Robot**-Enabled Construction **Assembly** with Automated Sequence Planning based on ChatGPT: RoboGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.11018.pdf)

- (arXiv 2023.4) Graph-ToolFormer: To Empower LLMs with **Graph Reasoning** Ability via Prompt Augmented by ChatGPT, [[Paper]](https://arxiv.org/pdf/2304.11116.pdf), [[Code]](https://github.com/jwzhanggy/Graph_Toolformer)

- (arXiv 2023.4) Can GPT-4 Perform **Neural Architecture Search**?, [[Paper]](https://arxiv.org/pdf/2304.10970.pdf), [[Code]](https://github.com/mingkai-zheng/GENIUS)

- (arXiv 2023.4) MiniGPT-4: Enhancing **Vision-Language** Understanding with Advanced Large Language Models, [[Paper]](https://arxiv.org/pdf/2304.10592.pdf), [[Project]](https://minigpt-4.github.io/)

- (arXiv 2023.4) SINC: Spatial Composition of **3D** Human **Motions** for Simultaneous Action Generation, [[Paper]](https://arxiv.org/pdf/2304.10417.pdf), [[Project]](https://sinc.is.tue.mpg.de/)

- (arXiv 2023.4) LLM as A **Robotic Brain**: Unifying **Egocentric** Memory and Control, [[Paper]](https://arxiv.org/pdf/2304.09349.pdf)

- (arXiv 2023.4) Chameleon: **Plug-and-Play** Compositional **Reasoning** with Large Language Models, [[Paper]](https://arxiv.org/pdf/2304.09842.pdf), [[Project]](https://chameleon-llm.github.io/)

- (arXiv 2023.4) Visual **Instruction Tuning**, [[Paper]](https://arxiv.org/pdf/2304.08485.pdf), [[Project]](https://llava-vl.github.io/)

- (arXiv 2023.4) MiniGPT-4: Enhancing **Vision-language** Understanding with Advanced Large Language Models, [[Paper]](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf), [[Project]](https://minigpt-4.github.io/)

- (arXiv 2023.4) RAFT: **Reward** rAnked FineTuning for **Generative** Foundation Model Alignment, [[Paper]](https://arxiv.org/pdf/2304.06767.pdf), [[Code]]()

- (arXiv 2023.4) **Multimodal** C4: An Open, Billion-scale Corpus of Images Interleaved With Text, [[Paper]](https://arxiv.org/pdf/2304.06939.pdf), [[Code]](https://github.com/allenai/mmc4)

- (arXiv 2023.4) ViewRefer: Grasp the Multi-view Knowledge for **3D Visual Grounding** with GPT and Prototype Guidance, [[Paper]](https://arxiv.org/pdf/2303.16894.pdf), [[Code]](https://github.com/ZiyuGuo99/ViewRefer3D)

- (arXiv 2023.4) **HuggingGPT**: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, [[Paper]](https://arxiv.org/pdf/2303.17580.pdf), [[Code]](https://github.com/microsoft/JARVIS)

- (arXiv 2023.4) ERRA: An **Embodied** Representation and Reasoning Architecture for **Long-horizon** Language-conditioned **Manipulation** Tasks, [[Paper]](https://arxiv.org/pdf/2304.02251.pdf), [[Code]](https://robotll.github.io/ERRA/)

- (arXiv 2023.4) Advancing **Medical Imaging** with Language Models: A Journey from N-grams to ChatGPT, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2304/2304.04920.pdf)

- (arXiv 2023.4) ChatGPT Empowered Long-Step **Robot Control** in Various Environments: A Case Application, [[Paper]](https://arxiv.org/pdf/2304.03893.pdf), [[Code]](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts)

- (arXiv 2023.4) OpenAGI: When LLM Meets Domain **Experts**, [[Paper]](https://arxiv.org/pdf/2304.04370.pdf), [[Code]](https://github.com/agiresearch/OpenAGI)

- (arXiv 2023.4) **Video** ChatCaptioner: Towards the Enriched Spatiotemporal **Descriptions**, [[Paper]](https://arxiv.org/pdf/2304.04227.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

### 2023.3

<!-- - (arXiv 2023.3) , [[Paper]](), [[Code]]() -->

- (arXiv 2023.3) Open-World **Object Manipulation** using Pre-Trained Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2303.00905.pdf), [[Project]](https://robot-moo.github.io/)

- (arXiv 2023.3) Grounded Decoding: Guiding Text Generation with Grounded Models for **Robot Control**, [[Paper]](https://arxiv.org/pdf/2303.00855.pdf), [[Project]](https://grounded-decoding.github.io/) 

- (arXiv 2023.3) Task and Motion **Planning** with Large Language Models for **Object Rearrangement**, [[Paper]](https://arxiv.org/pdf/2303.06247.pdf), [[Project]](https://sites.google.com/view/llm-grop)

- (arXiv 2023.3) RE-MOVE: An Adaptive **Policy Design** Approach for Dynamic Environments via Language-Based Feedback, [[Paper]](https://arxiv.org/pdf/2303.07622.pdf), [[Project]](http://gamma.umd.edu/remove/)

- (arXiv 2023.3) Chat with the Environment: **Interactive Multimodal Perception** using Large Language Models, [[Paper]](https://arxiv.org/pdf/2303.08268.pdf)

- (arXiv 2023.3) MAtch, eXpand and Improve: Unsupervised Finetuning for **Zero-Shot Action Recognition** with Language Knowledge, [[Paper]](https://arxiv.org/pdf/2303.08914.pdf), [[Code]](https://github.com/wlin-at/MAXI)

- (arXiv 2023.3) DialogPaint: A Dialog-based **Image Editing** Model, [[Paper]](https://arxiv.org/pdf/2303.10073.pdf)

- (arXiv 2023.3) MM-REACT : Prompting ChatGPT for **Multimodal Reasoning** and **Action**, [[Paper]](https://arxiv.org/pdf/2303.11381.pdf), [[Project]](https://multimodal-react.github.io/)

- (arXiv 2023.3) eP-ALM: Efficient Perceptual **Augmentation** of Language Models, [[Paper]](https://arxiv.org/pdf/2303.11403.pdf), [[Code]](https://github.com/mshukor/eP-ALM)

- (arXiv 2023.3) Errors are Useful Prompts: Instruction Guided **Task Programming** with Verifier-Assisted Iterative Prompting, [[Paper]](https://arxiv.org/pdf/2303.14100.pdf), [[Project]](https://ac-rad.github.io/clairify/)

- (arXiv 2023.3) **LLaMA**-**Adapter**: Efficient Fine-tuning of Language Models with Zero-init Attention, [[Paper]](https://arxiv.org/pdf/2303.16199.pdf), [[Code]](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- (arXiv 2023.3) **MULTIMODAL** ANALOGICAL **REASONING** OVER KNOWLEDGE GRAPHS, [[Paper]](https://arxiv.org/pdf/2210.00312.pdf), [[Code]](https://github.com/zjunlp/MKG_Analogy)

- (arXiv 2023.3) CAN LARGE LANGUAGE MODELS **DESIGN** A **ROBOT**? [[Paper]](https://arxiv.org/pdf/2303.15324.pdf)

- (arXiv 2023.3) Learning **video** embedding space with Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2303.14584.pdf)

- (arXiv 2023.3) Audio Visual Language Maps for Robot **Navigation**, [[Paper]](https://arxiv.org/pdf/2303.07522.pdf), [[Project]](https://avlmaps.github.io/)

- (arXiv 2023.3) ViperGPT: Visual Inference via **Python** Execution for **Reasoning**, [[Paper]](https://arxiv.org/pdf/2303.08128.pdf)

- (arXiv 2023.3) **ChatGPT** Asks, **BLIP-2** Answers: Automatic Questioning Towards Enriched **Visual Descriptions**, [[Paper]](https://arxiv.org/pdf/2303.06594.pdf), [[Code]](https://github.com/Vision-CAIR/ChatCaptioner)

- (arXiv 2023.3) Can an **Embodied** Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation, [[Paper]](https://arxiv.org/pdf/2303.03480.pdf), [[Project]](https://gamma.umd.edu/LGX/)

- (arXiv 2023.3) Visual **ChatGPT**: Talking, Drawing and Editing with Visual Foundation Models, [[Paper]](https://arxiv.org/abs/2303.04671), [[Code]](https://github.com/microsoft/visual-chatgpt)

- (arXiv 2023.3) PaLM-E: An **Embodied** Multimodal Language Model, [[Paper]](https://palm-e.github.io/assets/palm-e.pdf), [[Project]](https://palm-e.github.io/)

- (arXiv 2023.3) Language Is Not All You Need: **Aligning** Perception with Language Models, [[Paper]](https://arxiv.org/pdf/2302.14045.pdf), [[Code]](https://github.com/microsoft/unilm)

### 2023.2

- (arXiv 2023.2) ChatGPT for **Robotics**: Design Principles and Model Abilities, , [[Paper]](https://arxiv.org/pdf/2306.17582.pdf), [[Code]](https://github.com/microsoft/PromptCraft-Robotics)

- (arXiv 2023.2) Internet Explorer: Targeted Representation Learning on the Open Web, [[Paper]](https://arxiv.org/pdf/2302.14051.pdf), [[Project]](https://internet-explorer-ssl.github.io/)

### 2022.11

- (arXiv 2022.11) Visual Programming: Compositional **visual reasoning** without training, [[Paper]](https://arxiv.org/pdf/2211.11559.pdf), [[Project]](https://prior.allenai.org/projects/visprog)

### 2022.7

- (arXiv 2022.7) Language Models are General-Purpose **Interfaces**, [[Paper]](https://arxiv.org/pdf/2206.06336.pdf), [[Code]](https://github.com/microsoft/unilm)

- (arXiv 2022.7) LM-Nav: Robotic **Navigation** with Large Pre-Trained Models of Language, Vision, and Action, [[Paper]](https://arxiv.org/pdf/2207.04429.pdf), [[Project]](https://sites.google.com/view/lmnav)
